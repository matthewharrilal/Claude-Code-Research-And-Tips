# Context Reader Synthesis: Middle-Tier Experiment in Project Arc

**Date:** 2026-02-16
**Role:** context-reader (retro-analysis team)
**Purpose:** Connect the Middle-Tier Experiment to the broader KortAI project arc, tier model, and strategic trajectory

---

## I. The Project Arc: From Showcase to Pipeline to Validation

### The Journey So Far

The KortAI design system project began with a fundamental ambition: transform flat documentation into "places you want to spend time in." Before the design system existed, the user said: "I didn't feel like I wanted to read this."

**Stage 1: Exploration (DD → OD → AD → CD, 30 validated pages)**
- Four exploration dimensions, 6 pages each
- DD (Density) established soul constraints and tokens
- OD (Organization) discovered "Organization IS Density"
- AD (Axis) created 4 density patterns (CRESCENDO, F-PATTERN, BENTO, PULSE)
- CD (Combination) achieved the crown jewel: CD-006 scored 39/40

The showcase pages were magnificent. They were also iterative, multi-pass, 4-6 hour explorations. Each constraint, each technique, each decision was discovered through adversarial audit cycles with 4-17 agents per page.

**Stage 2: Extraction (Phase C, 19 agents, 42 files)**
- All showcase knowledge → 6-layer compositional-core ontology
- Identity (22 prohibitions) → Vocabulary (65 tokens) → Grammar (18 mechanisms) → Components → Case Studies → Guidelines
- 94.7% anti-gravity compliance
- All 11 extraction criteria MET or EXCEEDED

The question: can we CAPTURE the richness in a reusable system?

**Stage 3: Validation (Phase D, 14 agents, 5 variants)**
- Tested whether the extracted system could produce pages comparable to showcases
- Variant B (weak permission) = best result: 18/19 compliance, 4/5 novelty, genuine "scientific calibration laboratory" metaphor
- But: only 5/44 CSS techniques deployed (vs showcase pages' 23-44/44)
- Container width violations on 4/5 pages (THE failure mode)
- "Always-load" protocol failure caused Track 1 instant fail

The question: can the pipeline REPRODUCE showcase quality?

**Stage 4: Research (Richness + Rigidity, 11 agents, 11 reports)**
- Richness investigation (5 agents): What IS the gap?
- Rigidity investigation (6 agents): Can we close it without limiting rigidity?
- Unanimous finding: "Sample 2-4 mechanisms" = the most limiting specification
- The tier model emerged: Floor / Middle / Ceiling / Flagship
- The modification recommendations: per-category minimums (M1) to replace "sample 2-4"

The question: what prevents the pipeline from reaching showcase richness?

**Stage 5: Skill Enrichment Wave 1 (M2/M3/M5/M8 applied)**
- M2 (fractal gate): binary enforcement, 2 scales minimum for Middle
- M3 (container width): 940-960px non-negotiable, THE Phase D failure mode fix
- M5 (tier routing): content-type → pattern mapping
- M8 (tier framing): clarify Middle = lookup, Ceiling = derivation
- M1 (per-category minimums) intentionally sequenced to Wave 2 AFTER Middle experiment

The question: should we validate M1 before encoding it?

**Stage 6: Middle-Tier Experiment (8 agents, 35 minutes, SUCCESS)**

The experiment asked: **Can an agent produce "designed" (not just "formatted") output by deploying 8-10+ mechanisms across 5 categories WITHOUT metaphor derivation?**

The answer: **YES.** PA-05 DESIGNED (4/4), novelty 3/3 STRONGLY NOVEL, better than Variant B in 3 perceptual dimensions. Container width 960px (PASS). Soul compliance 7/7 (PASS).

The mechanism: Per-category minimums (M1 manual override in the prompt) produced 12 mechanisms across all 5 categories. The planner selected mechanisms based on content structure. The builder implemented without seeing the mechanism catalog. The result: novel border-weight gradient for security criticality encoding, dark header with editorial authority, zone backgrounds for section differentiation.

The caveat: Known defects (CPL +2, token compliance 66.5%, missing footer). Perceptual audit verdict: "specifications applied correctly, not composition felt through." The page crossed the DESIGNED threshold but landed at "professionally stiff."

---

## II. The Tier Model: Four Levels of Richness

The research converged on a 4-tier system grounded in actual CSS analysis of existing pages:

| Tier | Mechanisms | Build Time | CSS Lines | Pages | What It's For |
|------|-----------|------------|-----------|-------|---------------|
| **Floor** | 5 | 30-45 min | 150-250 | 10-20% | API refs, changelogs, config docs |
| **Middle** | 8-10 | 70-100 min | 350-500 | 40-50% | Tutorials, guides, overviews |
| **Ceiling** | 12-15 | 150-220 min | 700-1,000 | 20-30% | Conceptual docs, deep explanations |
| **Flagship** | 16-18 | 240-400 min | 1,000-1,500 | 5-10% | Anchor content, crown jewels |

**CRITICAL DISTINCTION: These mechanism counts are DESCRIPTIVE (backward-engineered from existing pages), not PRESCRIPTIVE (targets to hit).**

Provenance:
- **Floor (5 mechanisms):** Variant B, built under "sample 2-4" constraint, 30-45 min
- **Middle (8-10 mechanisms):** Predicted range based on per-category minimums (1+ each of 5 categories). Experiment deployed 12.
- **Ceiling (12-15 mechanisms):** OD-004 geological confidence, 150-220 min iterative build
- **Flagship (16-18 mechanisms):** CD-006 combination pilot (39/40 score), DD-006 fractal (20+ refs), 240-400 min multi-pass

**Why Middle is the recommended universal floor:** Floor-to-Middle transition has HIGHEST ROI (3-4x richness for ~45 extra minutes). Every page should be at least Middle.

### The Two Dimensions of "Designed"

The Middle-Tier Experiment revealed that DESIGNED has two distinct dimensions:

**1. Vocabulary Fluency (Middle tier achieves this)**
- Deploy mechanisms from all 5 categories (Spatial, Hierarchy, Component, Depth/Emphasis, Structure/Navigation)
- Each mechanism serves a DIFFERENT content need independently
- Selection logic: content feature → mechanism capability (direct mapping)
- Result: pages that feel "specified correctly" -- professional, structured, readable
- Perceptual ceiling: "professionally stiff"

**2. Compositional Fluency (Ceiling tier adds this)**
- Derive a metaphor that gives the page a coherent REASON for its choices
- Select mechanisms that REINFORCE each other (multi-channel encoding of same dimension)
- Example: border-weight + zone backgrounds + spacing ALL encode "depth" together
- Selection logic: metaphor → shared semantic dimension → multiple mechanisms encoding that dimension simultaneously
- Result: pages that feel "composed through" -- atmosphere, spatial presence, "a place you want to be"

Middle tier: lookup ("This has code blocks → I need mechanism #17").
Ceiling tier: creative derivation ("Geological strata have pressure gradients → spacing compression, border-weight, AND zone backgrounds all encode depth").

---

## III. The M1 Decision: Why Wave 2, and Why That Was Arguably a Mistake

### The Original Rationale

M1 (replace "sample 2-4 mechanisms" with per-category minimums) was grouped with M6 (semantic justification) and M7 (documented combinations) as "vocabulary expansion" modifications. The sequencing logic:

- Wave 1: enabling constraints (M2 fractal gate, M3 container width, M5 tier routing, M8 framing)
- Wave 2: vocabulary expansion (M1 per-category, M6 justification, M7 combinations)

The idea: validate M1's specific parameters through experiment feedback before permanent encoding. M6 and M7 genuinely need experiment feedback (M6 might be busywork, M7 might over-prescribe). So group M1 with them.

### The Problem Identified

This creates a circular dependency. The experiment tests whether 8-10 mechanisms across 5 categories creates a "designed" feeling. But without M1, the skill still says "sample 2-4 mechanisms." The builder deploys 4. The experiment can't test the hypothesis it was designed to test.

Three possible approaches:
1. **Apply M1 before experiment** -- clean test of full Middle concept, but can't isolate M1's contribution
2. **Manual override in builder prompt** -- skill unchanged, but experiment tests the concept not the skill
3. **Two builds** -- one with "2-4", one with per-category -- provides comparison data but costs double

### What "Isolated Effect" Means Concretely

- If we change M1+M2+M3+M5 and the result is good, we can't attribute improvement to any single change
- If we change only M2+M3+M5 and the result is mediocre, then add M1 and the result improves, we know M1 was the critical piece
- **BUT:** We already have 11 agents' worth of evidence that M1 is the critical piece. The isolated test would confirm what we already know at the cost of a wasted build.

### The Consensus Position

**M1 sequencing was arguably a mistake.** M1 is categorically different from M6/M7. M1 is the foundational enabler that unlocks breadth across 5 categories. M6/M7 are refinements that need experiment feedback.

The Middle-Tier Experiment handled this via **manual override**: the master execution prompt explicitly told the planner to "IGNORE 'sample 2-4' wherever you see it. Deploy AT LEAST 1 mechanism from EACH of 5 categories." This tested the M1 CONCEPT without modifying the skill.

Result: Validated. The planner deployed 12 mechanisms (S:1, H:3, C:4, D:2, N:3). PA-05 4/4 DESIGNED. Novelty 3/3. Better than Variant B YES.

### The Lesson for Process Design

**Per-category minimums are the single highest-leverage change to the tension-composition skill.** They should have been Wave 1, not Wave 2. The "sample 2-4" instruction was identified by ALL 11 research agents as THE most limiting specification. When something has unanimous multi-agent convergence and clear theoretical grounding (breadth across categories drives perceptual richness), sequencing it for "validation" is over-caution.

---

## IV. The "Why" Documentation Gap

A meta-pattern emerged across the 8 clarifications from user conversation (documented in `ephemeral/session-insights/17-conversation-clarifications.md`):

**The existing documentation states WHAT without explaining WHY.**

Examples:
- Tier model lists mechanism counts (8-10 for Middle) without explaining they're BACKWARD-ENGINEERED from existing pages, not invented as targets
- "Sample 2-4 mechanisms" appears in the skill without noting it's the IDENTIFIED PROBLEM, not a design choice
- Density patterns (CRESCENDO/F-PATTERN/BENTO/PULSE) are mentioned without explaining they're SPATIAL ORGANIZATION STRATEGIES (skeleton), not mechanisms (flesh)
- Scales (Page/Section/Component/Character) are referenced without concrete examples of what "2-scale coherence" looks like

### Why This Matters

When documentation omits rationale:
- **Builders** follow instructions correctly but don't understand the reasoning
- **Planners** making adjustments don't know which parameters are load-bearing vs arbitrary
- **New instances** can't distinguish "this number is empirically grounded" from "this number is a rough guess"

### The Enrichment Opportunity

Every specification should include its provenance:
- **WHERE** did this number/decision come from? (backward-engineered from X page)
- **WHAT** would happen if we changed it? (increasing from 8-10 to 12-15 crosses into Ceiling territory)
- **IS** this validated or hypothetical? (counts are observed, Middle tier was untested hypothesis)
- **WHAT** was the alternative considered? (raw count target rejected because of Goodhart's Law)

The master execution prompt for the Middle-Tier Experiment was the FIRST document to systematically include provenance for every specification. 1,760 lines, ~25K tokens. Each section answered: what, why, where from, what if changed.

---

## V. The Middle-Tier Experiment: What Happened

### Topology: Flat File-Bus

8 agents, 1-layer hierarchy, 100% file-based communication, zero SendMessage calls:
1. **content-selector** (Phase 0): extract 800-1,200 words mixed technical content
2. **planner** (Phase 1): pattern + mechanism deployment plan
3. **builder** (Phase 2): implement HTML page
4. **programmatic-auditor** (Phase 3a): automated CSS/HTML verification at 1440px and 768px
5. **perceptual-auditor** (Phase 3b): fresh-eyes zero-context perceptual evaluation
6. **comparative-auditor** (Phase 4a): side-by-side comparison with Variant B
7. **novelty-evaluator** (Phase 4b): structural fingerprint, CSS overlap, mechanism combination tests
8. **verdict-synthesizer** (Phase 5): apply decision matrix, state SUCCESS / PARTIAL / FAILURE

**Parallelization:** Phases 3 and 4 ran 2 agents concurrently (programmatic + perceptual, comparative + novelty). Sequential Playwright scheduling prevented contention.

**Timing:** ~35 minutes wall-clock. The tier model predicted 70-100 minutes for Middle tier. Early planning documents estimated 4.5-6.5 hours. The 8-11x discrepancy reveals a systematic model error: planning used sequential human time (one person doing all phases back-to-back), but agent teams execute in PARALLEL with separate context windows.

### Content: SYSTEM Remote Mac Control

**Source:** `extractions/infrastructure/004-system-remote-mac.md`

**Structure:**
- What it is: Core concept + feature table + key differentiators
- Architecture: Brain/Tunnel/Body pattern + architecture diagram
- Setup: CLI installation steps + config commands
- Security: Threat/mitigation table + security code examples

**Mix:** 956 words, 3 prose sections, 4 code blocks, 3 callouts, 2 tables, 1 step sequence, 1 architecture diagram. Meets all A2.1-A2.10 content criteria.

**Why this content:** Different domain from Variant B (scientific calibration), mixed structural types (not pure prose), natural mechanism triggers (code blocks → #17, callouts → #2, table → #18), maps to F-PATTERN (reference/lookup content).

### Mechanisms Deployed: 12 (Across All 5 Categories)

| Category | Mechanisms | Count |
|----------|-----------|-------|
| Spatial (S) | #5 Dense/Sparse Alternation | 1 |
| Hierarchy (H) | #1 Border-Weight Gradient, #4 Spacing Compression, #11 Typography Scale | 3 |
| Component (C) | #2 2-Zone DNA, #9 Color Encoding, #10 Border-Left Signal, #17 Code Blocks | 4 |
| Depth/Emphasis (D) | #7 Zone Backgrounds, #16 Drop Cap | 2 |
| Structure/Navigation (N) | #13 Dark Header, #14 Footer Mirror, #18 Data Table | 3 |

**Design highlight (per novelty evaluator):** Border-weight gradient (4px → 3px → 3px → 1px) for security criticality encoding. Critical setup (Brain/Tunnel/Body) = 4px. Security mitigations = 3px. General configuration = 1px. Novel, functional, elegant.

### Results

**PA-05 (DESIGNED):**
- PA-05a: 3 non-default layout elements (>= 2 required) -- PASS
- PA-05b: 2.0x padding range ratio (>= 2.0x required) -- PASS (exactly at threshold, zero margin)
- PA-05c: Visual hierarchy (blur test) -- PASS
- PA-05d: 17% compositional CSS (>= 15% required) -- PASS (estimated, not measured)

**Verdict: 4/4 DESIGNED**

**Novelty (D3):**
- D3.1 Structural fingerprint: distinct sectioning pattern from DD-006 and CD-006 -- NOVEL
- D3.2 CSS value overlap: < 30% overlap with references -- NOVEL
- D3.3 Mechanism combination: different reinforcing pairs -- NOVEL

**Verdict: 3/3 STRONGLY NOVEL**

**Better than Variant B: YES**
- Perceptual improvement 1: Dark header creates editorial authority (Variant B had plain start)
- Perceptual improvement 2: Architecture diagram focal point creates visual anchor (Variant B had no visual moments)
- Perceptual improvement 3: Border-weight gradient encodes security criticality (Variant B used uniform 4px borders)

**Container width:** 960px (PASS)
**Soul compliance:** 7/7 (PASS)

**Known defects:**
1. CPL 82 chars (+2 over 45-80 spec)
2. Token compliance 66.5% (13.5 points below 80% threshold)
3. Missing footer (planned but not implemented -- WOULD-NOT-SHIP severity)

**Ship decision:** YES WITH RESERVATIONS (footer bug must be fixed)

**Final verdict:** **SUCCESS**

---

## VI. What the Experiment Proved (and What It Didn't)

### Validated with HIGH Confidence

1. **Per-category minimums produce DESIGNED output.** 12 mechanisms across 5 categories, PA-05 4/4, novelty 3/3. M1 works as predicted.
2. **Vocabulary fluency (breadth) creates perceptual richness.** Heavy constraints + high vocabulary = richer output. Constraints did NOT prevent richness.
3. **Flat file-bus topology works for 8 agents.** Zero contention, ~35 minutes, no failures. Per-file ownership is the key mechanism.
4. **Fresh-eyes perceptual auditing finds what loaded agents miss.** Zero-context auditor identified missing footer, heading spacing violations, metronomic rhythm, top-heavy weight distribution.
5. **Two-instance pattern prevents template copying.** Builder never saw mechanism catalog or case studies. Result: novel border-weight gradient, not showcase page copying.

### Validated with MEDIUM Confidence

6. **Middle-tier natural landing zone is 8-12 mechanisms (not 8-10).** One experiment deployed 12. Need 5+ builds to establish the actual distribution.
7. **Container width 940-960px is non-negotiable.** 960px passed. Guardrail held. (Confidence is MEDIUM not HIGH because only 1 data point post-fix.)

### NOT Validated (Requires Further Testing)

8. **Is per-category the driver, or just higher count?** The experiment can't isolate which factor drove improvement. An ablation study (8-10 total vs 1+ per category on same content) would resolve this.
9. **Does compositional fluency (Ceiling tier) close the "felt through" gap?** Middle achieved "professionally stiff." Ceiling adds metaphor derivation. Does metaphor create genuine compositional purpose, or just derivative pattern-matching?
10. **Is the 8-11x timing discrepancy systematic?** Single data point. Ceiling's metaphor derivation phases may not parallelize as cleanly as Middle's lookup phases.

---

## VII. Critical Evidence: Agent Communication Matters for Quality

The Middle-Tier Experiment achieved SUCCESS but had known defects (missing footer, CPL +2, token compliance 66.5%). The retrospective team surfaced a pattern:

**Previous experiments that used inter-agent messaging produced higher-quality output:**

- **CD-006 (crown jewel):** 39/40 score, built with agent communication and iteration
- **Phase C Extraction:** 19 agents with hierarchies and messaging, ALL 11 criteria MET
- **Phase D Variant B:** agents communicated via SendMessage, 18/19 compliance, 4/5 novelty

**The Middle-Tier Experiment used flat file-bus with ZERO SendMessage calls:**
- Result: technically compliant, structurally novel, but "professionally stiff" with B+ quality
- Missing footer = implementation bug that messaging might have caught
- Token compliance 66.5% = systemic builder capability gap

**The hypothesis:** The quality gap is PARTIALLY explained by agents being unable to collaborate, catch mistakes, or iterate. Flat file-bus optimizes for speed and simplicity. Hierarchical messaging topologies optimize for quality and error correction.

**The trade-off:**
- Flat file-bus: 35 minutes, B+ quality, zero coordination overhead
- Hierarchical messaging: 60-90 minutes, A quality, coordination complexity

For Ceiling tier (where quality matters more than speed), consider hybrid topology: flat file-bus for Phases 0-2 (content, planning, build), then hierarchical messaging for Phases 3-5 (validation, evaluation, iteration).

---

## VIII. What Comes Next: The F1 Sequence

From the master retrospective and verdict synthesis, the recommended next steps:

### F1.1: Apply M1 Permanently

**What:** Replace "sample 2-4 mechanisms" with per-category minimums in tension-composition skill. Three locations: Phase 0D Tier Classification, Phase 4.0 Mechanism Extraction, and any remaining "sample 2-4" language.

**Why:** Validated by 11 research agents + Middle experiment. THE highest-leverage change to the skill.

**When:** NOW, before any next experiment.

### F1.2: Proceed to Wave 2 (M6, M7)

**What:**
- M6: Semantic justification -- mechanism selections must cite specific content features, not catalog descriptions
- M7: Documented combinations -- for 3+ mechanism pairs, document how they reinforce each other

**Why:** Refinements that build on M1. M6 prevents mechanical "I see code → use #17" application. M7 pushes toward compositional fluency (mechanism interaction).

**When:** After M1 is applied, before Ceiling experiment.

### F1.3: Plan Ceiling Experiment with SAME Content

**What:** Run Ceiling-tier build using SYSTEM documentation (same source as Middle experiment).

**Why:** Eliminates content variables. Allows direct tier comparison. Tests whether metaphor derivation creates genuine compositional divergence or derivative pattern-matching.

**When:** After Wave 2 modifications are applied.

**Key design changes for Ceiling:**
- Add metaphor derivation agent (Phase 1, NEW)
- Raise per-category minimums: S:2+, H:2+, C:3+, D:2+, N:2+ (natural landing 11-15)
- Add mechanism INTERACTION requirement: for 3+ pairs, document how they reinforce each other
- Add landmark completeness gate at Phase 2->3 boundary
- Add builder self-checks: CPL formula, token compliance scan
- Consider dual perceptual auditors with reconciliation
- Use blind novelty evaluation (evaluator receives 3 unlabeled pages)

### F1.4: Document Strongest Content-Mechanism Fits

**What:** Border-weight gradient for security criticality encoding was the Middle-tier design highlight. Document this as an exemplar of content-mechanism fit.

**Why:** Mechanism catalog currently describes techniques abstractly. Adding content affinity documentation (which content structures favor which mechanisms) helps builders select mechanisms for desired perceptual balance.

**When:** Can be done anytime, not blocking.

### F1.5: Document Gaps for Wave 2

**What:** Known defects reveal skill gaps:
- No landmark completeness gate (caused footer bug)
- No CPL verification formula (caused CPL +2)
- No token compliance self-check (caused 66.5%)
- No rhythm variation requirement (caused metronomic spacing)
- No heading spacing ratio as binary rule (caused 1.5:1 violation)

**Why:** Wave 2 should address these process gaps, not just vocabulary expansion.

**When:** As part of Wave 2 planning.

---

## IX. The Broader Strategic Context

### Why This Experiment Matters Beyond One Page

This is the first PRACTICAL test of the vocabulary-vs-library distinction. The Name Test and Transfer Test passed in THEORY (mechanisms can be described without metaphor names, mechanisms work with different metaphors). The Middle-Tier Experiment tested them in PRACTICE.

Result: **Vocabulary fluency (breadth across 5 categories) produces DESIGNED output without metaphor derivation.**

This validates the tier model's foundation. Not every page needs 4-6 hours of iterative metaphor-driven composition. 40-50% of pages (tutorials, guides, overviews) can be built in 70-100 minutes using pattern-based mechanism deployment.

### The Open Question

Middle tier achieves vocabulary fluency. It crosses the DESIGNED threshold. But it lands at "professionally stiff" rather than "compositionally confident."

**Is "felt through" the Ceiling/Flagship differentiator?**

The perceptual auditor distinguished "specifications applied correctly" (Middle) from "composition felt through" (what Ceiling should achieve). This maps to the two dimensions of DESIGNED:
- Vocabulary fluency (Middle) = knowing many words, deploying them correctly
- Compositional fluency (Ceiling) = using words to EXPRESS a coherent concept

The Ceiling experiment will test whether metaphor -- giving the page a REASON for its choices -- is the ingredient that transforms vocabulary fluency into compositional fluency.

---

## X. How This Connects to Your Immediate Next Steps

You're reading this because you're planning THE NEXT THING. Here's what you need to know:

**The Middle-Tier Experiment is COMPLETE. SUCCESS verdict. Known defects documented.**

**The skill enrichment project has ONE critical remaining task: Apply M1 permanently.**

Change three lines in `~/.claude/skills/tension-composition/SKILL.md`:
1. Phase 0D Tier Classification (lines 68-119): replace "sample 2-4" with "1+ per each of 5 categories"
2. Phase 4.0 Mechanism Extraction (lines 770-890): replace "sample 2-4" with per-category table
3. Any remaining "sample 2-4" language: replace with per-category minimums

**After M1 is applied, proceed to Wave 2 (M6 semantic justification, M7 documented combinations).**

**After Wave 2 is applied, plan the Ceiling experiment using SYSTEM documentation (same content as Middle).**

**DO NOT skip M1 and jump to Ceiling.** The Ceiling experiment tests whether metaphor adds compositional fluency ON TOP OF vocabulary fluency. You can't test that without first establishing vocabulary fluency as the baseline -- and M1 is the mechanism that establishes it.

---

*This synthesis connects the Middle-Tier Experiment (documented in `ephemeral/middle-tier-experiment/09-MASTER-RETROSPECTIVE.md`) to the broader project arc (documented in `ephemeral/continuity-docs/HANDOFF.md`), tier model, M1 sequencing decision (documented in `ephemeral/session-insights/17-conversation-clarifications.md`), and strategic trajectory. It answers: where we've been, where we are, what we learned, what comes next.*
