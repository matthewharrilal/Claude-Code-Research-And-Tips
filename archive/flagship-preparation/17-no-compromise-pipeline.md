# No-Compromise Flagship Pipeline Design

**Author:** NO-COMPROMISE-PIPELINE (Opus 4.6)
**Date:** 2026-02-16
**Task:** #17
**Evidence Base:**
- 05-flagship-architecture.md (3-pass architecture, 625 lines)
- 01-process-retrospective.md (pipeline failures, 453 lines)
- 04-metacognitive-analysis.md (preparation paradox, 384 lines)
- 03-diminishing-returns.md (economics of scale depth, 758 lines)
- CEILING-PREPARATION-BRIEF.md (unified synthesis, 505 lines)
- 04-showcase-archaeology.md (intentionality gap, 267 lines)
- 06-adversarial-anti-scale.md (restraint > saturation, 431 lines)
- AUDIT-REPORT.md (Mode 4 PA, 9 auditors)

**Design Brief:** "I don't care about expense. I don't care about cost. I don't care how granular we get. I want the richest, most beautiful, most nuanced, most scaled, most metaphorical, most mechanistic -- the best possible."

---

## TABLE OF CONTENTS

1. The Maximum Pipeline
2. Competitive Build
3. Iterative Refinement
4. Embedded Auditing
5. The Crown Jewel Pipeline
6. Calibration Run
7. The Ideal Flagship Specification
8. The Recommended Version

---

## 1. THE MAXIMUM PIPELINE

### How Many Passes?

**Answer: 5 passes, not 3.**

The current flagship architecture proposes 3 passes (Spatial Skeleton, Compositional Elaboration, Intentionality Layer). This is a good compression, but with no cost constraint, we should decompose further. Each pass should have a SINGLE cognitive focus, because the evidence shows that the ceiling experiment's builder failed when asked to manage multiple concerns simultaneously (metaphor vocabulary AND spatial proportion AND mechanism deployment).

The ideal decomposition:

| Pass | Focus | Builder Model | Input | Output | Gate |
|------|-------|---------------|-------|--------|------|
| **Pass 0** | Content Architecture | Opus 4.6 | Raw content + metaphor | Content zones, section plan, spatial budget | Spatial Budget Gate |
| **Pass 1** | Spatial Skeleton | Sonnet 4.5 | Content architecture plan | HTML/CSS skeleton (containers, flow, rhythm) | Spatial Confidence Gate (7 checks) |
| **Pass 2** | Mechanism Deployment | Sonnet 4.5 | Verified skeleton + mechanism plan | HTML/CSS with 12-14 mechanisms deployed | Composition Verification Gate |
| **Pass 3** | Metaphor Integration | Opus 4.6 | Verified composition + metaphor derivation | Vocabulary, zone naming, channel encoding | Metaphor Coherence Gate |
| **Pass 4** | Intentionality Layer | Opus 4.6 | Verified metaphor integration | Bookending, self-reference, cognitive transitions, meta-annotation | Ship-Ready Gate |

**Why 5 not 3:** The 3-pass architecture combines Content Architecture with Spatial Skeleton (Pass 1) and Mechanism Deployment with Metaphor Integration (Pass 2). These combinations recreate the ceiling experiment's failure mode: asking one builder to manage two concerns. By separating:

- **Content Architecture from Skeleton Build:** The content architecture is a THINKING task (what goes where, how much space each zone gets). The skeleton build is a BUILDING task (writing HTML/CSS). The ceiling experiment combined both and the builder never calculated the spatial budget.

- **Mechanism Deployment from Metaphor Integration:** Mechanisms can be deployed as CSS techniques first (border-weight gradient, spacing compression, zone backgrounds), then metaphor vocabulary layered on top (zone naming, semantic labels, channel encoding). The ceiling experiment tried to do both simultaneously and produced mechanisms that existed in CSS but were invisible under metaphor vocabulary labels.

### How Many Iterations Per Pass?

**Maximum 2 fix cycles per pass.** Each fix cycle involves:
1. Gate failure identification (what specific check failed)
2. Team-lead diagnosis (is this a CSS fix, HTML restructure, or architectural redesign?)
3. Builder fix (targeted, not wholesale rewrite)
4. Gate re-run

If a gate fails after 2 fix cycles, the pass DOWNGRADES: reduce scope rather than continue failing. This prevents the ceiling experiment's pattern of applying 40 CSS patches that don't address the root cause.

**Total possible fix cycles across pipeline: 10** (2 per pass x 5 passes). Expected: 2-3 total.

### How Many Auditors at Each Gate?

| Gate | Auditor Count | Type | Rationale |
|------|---------------|------|-----------|
| Spatial Budget Gate (Pass 0) | 1 programmatic | Binary calculation | Pure arithmetic -- content volume vs spatial allocation. No judgment needed. |
| Spatial Confidence Gate (Pass 1) | 1 programmatic + 1 cold-look | Binary + squint test | SC-01 through SC-07 are binary. SC-04 (squint test) requires fresh-eyes. |
| Composition Verification Gate (Pass 2) | 1 programmatic + 1 mechanism-perception | Binary + perceptual | Programmatic verifies mechanism presence. Perception check asks: "How many mechanisms are VISIBLE?" (Ceiling had 14 present, 1 visible.) |
| Metaphor Coherence Gate (Pass 3) | 2 fresh-eyes | Perceptual only | Two zero-context evaluators independently answer: "Is there a metaphor? What is it? Does it feel structural or announced?" If both identify the metaphor without labels, PASS. If label-dependent, FLAG. |
| Ship-Ready Gate (Pass 4) | Full Mode 4 | 9 independent auditors | The final gate deploys the full Mode 4 methodology: 9 auditors, 48 PA questions, 3 viewports, cold looks first. This is the methodology that caught what lighter audits missed. |

**Total auditors across pipeline: 15** (1+2+2+2+9, with some agents reused across early gates).

### How Many Builders?

**Answer: 4 builders (one per build pass), plus 1 competing builder for Pass 1.**

| Builder | Pass | Model | Why |
|---------|------|-------|-----|
| Content Architect | 0 | Opus 4.6 | THINKING pass. Requires creative judgment about content-zone mapping and spatial budgeting. |
| Skeleton Builder A | 1 | Sonnet 4.5 | PRIMARY build. Constructs the spatial skeleton from the content architecture plan. |
| Skeleton Builder B | 1 | Sonnet 4.5 | COMPETITIVE build. Same inputs, same constraints, different execution. Team-lead picks the better skeleton. (See Section 2.) |
| Mechanism Builder | 2 | Sonnet 4.5 | Deploys mechanisms into the verified skeleton. Does NOT add metaphor vocabulary. Pure CSS technique application. |
| Metaphor Builder | 3 | Opus 4.6 | Layers metaphor vocabulary, zone naming, channel encoding onto the mechanism-deployed page. Opus because this requires creative judgment about how metaphor vocabulary maps to CSS channels. |
| Intentionality Builder | 4 | Opus 4.6 | Adds bookending, self-reference, cognitive transitions, meta-annotation. Opus because intentionality requires the highest level of design intelligence. |

**Why separate Mechanism and Metaphor builders:** The ceiling experiment proved that a single builder managing both mechanism deployment AND metaphor vocabulary loses spatial coherence. By separating these, the Mechanism Builder focuses entirely on "does this CSS technique work in this location?" while the Metaphor Builder focuses on "does this vocabulary express the right meaning through this channel?"

### How Many Metacognitive Reviewers?

**3 metacognitive reviewers, deployed at different pipeline stages:**

| Reviewer | When | What They Check |
|----------|------|-----------------|
| Pre-Mortem Reviewer | Before Pass 0 begins | "Assume this experiment fails. What is the most likely cause?" Applied to: the content, the metaphor, the plan, the team structure. |
| Mid-Pipeline Reviewer | After Pass 2 (Composition verified) | "We are halfway through. What has been lost from the original vision? What emergent problems do the gates not check for? Is the metaphor still serving the page or has the page been deformed to serve the metaphor?" |
| Post-Build Reviewer | After Pass 4, before Mode 4 audit | "Look at this page as a READER, not a builder. In 10 seconds, what is your emotional response? Does this feel like a crown jewel or a well-executed technical document? What is the single thing that would most improve it?" |

These reviewers are Opus agents with ZERO prior context from the build. They receive only the HTML file (or screenshots) and answer their questions cold. This prevents continuation bias.

### Total Agent Count

| Role | Count | Model |
|------|-------|-------|
| Team Lead | 1 | Opus 4.6 |
| Metaphor Derivation Agent | 1 | Opus 4.6 |
| Content Architect | 1 | Opus 4.6 |
| Planner (Mechanism + Restraint) | 1 | Opus 4.6 |
| Skeleton Builder A | 1 | Sonnet 4.5 |
| Skeleton Builder B (competitive) | 1 | Sonnet 4.5 |
| Mechanism Builder | 1 | Sonnet 4.5 |
| Metaphor Builder | 1 | Opus 4.6 |
| Intentionality Builder | 1 | Opus 4.6 |
| Spatial Budget Auditor | 1 | Sonnet 4.5 |
| Spatial Confidence Auditor | 1 | Sonnet 4.5 |
| Cold-Look Auditor (Pass 1) | 1 | Sonnet 4.5 |
| Mechanism Perception Auditor | 1 | Sonnet 4.5 |
| Metaphor Fresh-Eyes A | 1 | Sonnet 4.5 |
| Metaphor Fresh-Eyes B | 1 | Sonnet 4.5 |
| Pre-Mortem Reviewer | 1 | Opus 4.6 |
| Mid-Pipeline Reviewer | 1 | Opus 4.6 |
| Post-Build Reviewer | 1 | Opus 4.6 |
| Mode 4 Auditors (9) | 9 | Sonnet 4.5 |
| Programmatic Auditor (final) | 1 | Sonnet 4.5 |
| Comparative Auditor | 1 | Sonnet 4.5 |
| Novelty Evaluator | 1 | Sonnet 4.5 |
| Verdict Synthesizer | 1 | Opus 4.6 |
| **TOTAL** | **30** | **10 Opus + 20 Sonnet** |

### Total Time Estimate

| Phase | Duration | Parallel? |
|-------|----------|-----------|
| Metaphor Derivation | 25-35 min | Parallel with content prep |
| Pass 0: Content Architecture | 20-30 min | Sequential |
| Pre-Mortem Review | 10-15 min | After Pass 0, before Pass 1 |
| Pass 1: Competitive Skeleton Build | 35-50 min | 2 builders parallel |
| Spatial Confidence Gate | 15-20 min | Sequential |
| Pass 2: Mechanism Deployment | 30-45 min | Sequential |
| Composition Verification Gate | 15-20 min | Sequential |
| Mid-Pipeline Review | 10-15 min | Sequential |
| Pass 3: Metaphor Integration | 25-40 min | Sequential |
| Metaphor Coherence Gate | 15-20 min | Sequential |
| Pass 4: Intentionality Layer | 20-30 min | Sequential |
| Post-Build Review | 10-15 min | Sequential |
| Mode 4 Audit | 35-50 min | 9 auditors parallel |
| Fix Cycles (expected 2-3) | 30-60 min | Sequential |
| Post-Fix Audit (if needed) | 20-30 min | Parallel |
| Verdict Synthesis | 15-20 min | Sequential |
| **TOTAL** | **310-460 min (5.2-7.7 hrs)** | |

**Expected: ~380 minutes (6.3 hours).**

---

## 2. COMPETITIVE BUILD

### The Concept

What if 2 builders execute Pass 1 in parallel with DIFFERENT approaches to the same content architecture plan? Then auditors evaluate both and the BETTER one proceeds.

### How It Works

Both Skeleton Builder A and Skeleton Builder B receive:
- The same content architecture plan (from Pass 0)
- The same metaphor summary (from metaphor derivation)
- The same soul constraints
- The same spatial confidence criteria

They execute independently, producing two different spatial skeletons: `flagship-page-A.html` and `flagship-page-B.html`.

**Selection criteria (team-lead evaluates):**

1. **Spatial Confidence Gate:** Both must pass all 7 SC checks. If one fails and the other passes, the passing version wins automatically. If both fail, both enter fix cycles.

2. **Squint Test Comparison:** Team-lead takes screenshots of both at 1440px and 768px. Which one has better visual mass distribution? Which one feels more designed at 25% zoom?

3. **Content-Container Fit:** Which version makes the content feel most natural in its containers? (The ceiling experiment's skeleton made content feel squeezed into oversized containers.)

4. **Mechanism Receptivity:** Which skeleton provides better "hooks" for mechanism deployment in Pass 2? (Some skeletons create natural locations for border-weight gradients, zone backgrounds, and spacing compression; others create uniform containers that resist mechanism deployment.)

### Cost/Benefit Analysis

**Cost:**
- 1 additional Sonnet agent for ~40 min = marginal cost of one builder
- 15 min additional evaluation time for comparison
- Total additional cost: ~55 min of Sonnet time + 15 min team-lead time

**Benefit:**
- Eliminates the single-builder bottleneck at Pass 1, which is THE highest-risk pass
- The ceiling experiment's spatial failure was a Pass-1-level problem (skeleton proportions). Having TWO attempts doubles the probability of getting a good skeleton.
- If the base probability of a good skeleton is 70%, two independent attempts give P(at least one good) = 1 - (0.3)^2 = 91%.

**Verdict: WORTH IT.**

The competitive build eliminates skeleton selection risk for the cost of one additional Sonnet agent. The skeleton is the foundation of everything -- if it fails, 4 subsequent passes are built on a broken foundation. Doubling the investment at the foundation is the highest-ROI spend in the pipeline.

### Variant: Competitive Metaphor Derivation

An even more ambitious version: two metaphor derivation agents working in parallel with the same content, producing two candidate metaphors. The planner evaluates both and selects the stronger one.

**Cost:** 1 additional Opus agent for ~30 min.

**Benefit:** The ceiling experiment's metaphor was excellent (12/12) but created spatial obligations (sparse exterior zone) that produced the void. A second metaphor might have produced a spatially cheaper metaphor with equal conceptual quality. The comparison would surface: "Metaphor A is conceptually richer but demands more spatial void. Metaphor B is conceptually simpler but fits the content volume."

**Verdict: MAYBE WORTH IT.** The benefit depends on whether metaphor-spatial conflict is common. The evidence suggests it IS common (the ceiling metaphor's spatial obligations were the root cause of the void). But two metaphor derivations at Opus cost is expensive. Recommendation: include in the MAXIMUM pipeline, defer in the RECOMMENDED pipeline.

### Variant: Competitive Intentionality Layer

Two intentionality builders (both Opus) independently adding bookending, self-reference, and cognitive transitions. The better version is selected.

**Verdict: NOT WORTH IT.** Intentionality is the lowest-risk pass (HTML content addition, not CSS complexity). The probability of failure is already ~90% success. Doubling the investment at a 90% success rate yields 99% -- a marginal improvement for a significant cost.

---

## 3. ITERATIVE REFINEMENT

### "Build at Ceiling, Iterate Twice, Ship" vs The No-Compromise Approach

The diminishing returns research recommends "build at ceiling, iterate twice, ship." This is the PRACTICAL path. The no-compromise path asks: what if we iterate MORE?

### The Iteration Model

Instead of 3 PASSES (sequential layers), the no-compromise pipeline uses 5 PASSES (sequential layers) plus up to 3 ITERATION CYCLES (full rebuild of a specific pass based on audit findings).

| Cycle | What Gets Rebuilt | Trigger | What It Fixes |
|-------|-------------------|---------|---------------|
| **Iteration 1** | Pass 1 (Spatial Skeleton) | Mode 4 audit finds spatial proportion issues | Container proportions, section density, void budget violations. This is the most common failure mode. |
| **Iteration 2** | Pass 2-3 (Mechanism + Metaphor) | Mode 4 audit finds "mechanisms present but not perceivable" or "metaphor announced not structural" | Mechanism-metaphor alignment, channel encoding strength, restraint ratio. Addresses the ceiling experiment's "14 mechanisms, 1 visible" failure. |
| **Iteration 3** | Pass 4 (Intentionality) | Post-iteration audit finds "competent but not exceptional" | Self-reference depth, bookending sophistication, cognitive transition quality. This is the polish iteration -- turning B+ into A. |

### What Each Iteration Specifically Fixes

**Iteration 1: Spatial Problems**
- Content-to-void ratio below 60:40 -- add content or remove spatial containers
- Section density imbalance -- redistribute content or merge sparse sections
- Header proportions -- reduce header height, increase content start point
- Footer reachability -- reduce pre-footer void

This is the iteration the ceiling experiment needed and never got (the "fix" was CSS patches, not spatial restructuring). In the no-compromise pipeline, Iteration 1 is a FULL PASS 1 REBUILD with adjusted content architecture, not a CSS patch.

**Iteration 2: Compositional Interactions**
- Mechanism saturation -- too many mechanisms competing for attention, need to REMOVE mechanisms and add silence zones
- Channel encoding weakness -- mechanisms deployed through single channels, need multi-channel reinforcing pairs
- Metaphor texture -- metaphor labels doing the work instead of structural expression, need to remove labels and strengthen structural encoding

This addresses the adversarial thesis: "14 mechanisms deployed, only 1 perceivable." The fix is not deploying more mechanisms -- it is deploying fewer with more compositional intention.

**Iteration 3: Beauty and Intentionality**
- Structural bookending -- if opening and closing don't echo, redesign the closing to mirror the opening
- Self-reference opportunities -- find locations where the content demonstrates its own form
- Cognitive mode transitions -- ensure bridge text between sections shifts the reader's mental state
- Meta-annotation -- add labels where they illuminate the architecture, remove labels where they substitute for structural expression

This is the iteration that converts a COMPETENT page into a CROWN JEWEL. It is the difference between CD-006 scoring 35/40 (competent) and 39/40 (crown jewel). The showcase archaeology shows this difference is 15-20% of perceived quality and is entirely in the intentionality dimension.

### Should Flagship Have 3 ITERATIONS or 3 PASSES?

**Both.** The no-compromise pipeline has 5 PASSES (sequential layers building complexity) AND up to 3 ITERATIONS (cycles that rebuild specific passes based on comprehensive audit). Passes build the page up. Iterations refine what was built based on holistic observation.

The distinction matters because:
- PASSES are CONSTRUCTIVE: each adds a new dimension (skeleton, mechanisms, metaphor, intentionality)
- ITERATIONS are CORRECTIVE: each rebuilds a dimension that the holistic audit revealed as inadequate

The ceiling experiment had 1 pass and 1 fix cycle (40 CSS patches). The no-compromise pipeline has 5 passes and up to 3 iteration cycles, each of which is a substantive rebuild of a specific pass.

### Iteration Diminishing Returns

| Iteration | Expected Quality Gain | Time Cost | ROI |
|-----------|----------------------|-----------|-----|
| Iteration 1 (spatial) | +15-25% (catastrophic void to adequate proportion) | 50-80 min | EXTREME (eliminates blocking defect) |
| Iteration 2 (composition) | +10-15% (mechanisms visible, metaphor structural) | 40-60 min | HIGH (quality threshold crossed) |
| Iteration 3 (intentionality) | +5-8% (competent to crown jewel) | 30-40 min | MODERATE (polish, not structure) |
| Iteration 4 (hypothetical) | +1-3% | 30-40 min | LOW (below perception threshold) |

**Stop at 3 iterations.** The evidence from cross-domain analysis (audio, image, typography) shows perception thresholds at approximately 85-90% of maximum. 3 iterations plus 5 passes targets ~95% of maximum achievable quality, which is at or beyond the perception ceiling.

---

## 4. EMBEDDED AUDITING

### The Concept

Instead of build-then-audit, a dedicated auditor agent watches the build IN REAL TIME. The auditor takes screenshots after every major CSS change and catches spatial problems as they emerge.

### How It Works

**The Embedded Auditor (Sonnet 4.5)** runs in parallel with each builder. Its protocol:

1. **File watch:** Every 60 seconds, the auditor reads the current state of the HTML file.
2. **Snapshot protocol:** After each checkpoint (defined below), the auditor serves the page and takes a screenshot at 1440px.
3. **Binary checks:** After each snapshot, the auditor runs SC-01 (container width) and SC-02 (void budget) on the current partial page.
4. **Alert on deviation:** If any binary check fails, the auditor sends a SendMessage to the builder: "SPATIAL ALERT: [check name] failing. Current void ratio: X%. Current container width: Xpx. Please verify this is intentional."
5. **Log:** The auditor maintains a running log of all snapshots and check results, enabling post-hoc analysis of WHERE in the build process spatial problems emerged.

**Checkpoints (triggers for snapshot):**

- After CSS custom properties / token definition block
- After header CSS complete
- After first section CSS complete
- After every 100 lines of CSS added
- After major architectural element (grid, zone, checkpoint divider)
- After final CSS write (pre-audit snapshot)

### Cost/Benefit Analysis

**Cost:**
- 1 Sonnet agent running for the full build duration (~35-50 min per pass)
- Playwright contention: the auditor needs browser access, which conflicts with any builder that also uses Playwright
- Messaging overhead: alerts interrupt the builder's flow

**Benefit:**
- Catches spatial problems at the 50-line stage, not the 500-line stage
- The ceiling experiment's void was already present at ~300 CSS lines but wasn't caught until the Mode 4 audit at ~850 lines and ~40 minutes of build time
- Reduces the probability of needing Iteration 1 by ~50%

**Playwright Contention Mitigation:**
The screenshot pre-capture pattern (used in Mode 4 PA) eliminates contention by having ONE agent control Playwright. For embedded auditing, the embedded auditor controls Playwright exclusively. The builder does NOT use Playwright -- it writes CSS and HTML directly. The embedded auditor periodically serves and screenshots the file. This requires sequential Playwright scheduling (embedded auditor takes a screenshot, then returns to waiting) but avoids concurrent Playwright access.

**Verdict: WORTH IT for Passes 1-2 (spatial and mechanism deployment). NOT NEEDED for Passes 3-4 (metaphor and intentionality add minimal CSS).**

### Implementation Detail: The Alert Protocol

The embedded auditor does NOT have authority to STOP the builder. It can only ALERT. This is deliberate:

1. **Hard alert (SendMessage, immediate):** Container width violation (SC-01 fail) or void ratio exceeding 30% at any checkpoint.
2. **Soft alert (log only, reviewed post-pass):** Void ratio trending upward, section density imbalance developing, designed moment distribution skewing.
3. **Summary message (sent at pass completion):** Full snapshot log with trend analysis. "Void ratio went from 15% at checkpoint 3 to 35% at checkpoint 7. The increase correlates with the addition of zone transition spacing in lines 250-300."

The builder is EXPECTED to acknowledge hard alerts with a brief response ("Intentional -- the metaphor requires breathing space here" or "Thank you -- adjusting padding"). Non-acknowledgment of a hard alert is a yellow flag for the team-lead.

### What This Looks Like In Practice

```
[EMBEDDED AUDITOR LOG]

Checkpoint 1 (after tokens): 0 CSS lines visual. No content to evaluate.
Checkpoint 2 (after header): SC-01 PASS (960px). Header height: 180px (23% viewport). PASS.
Checkpoint 3 (after section 1): Void ratio 12%. Content distribution: top 40% of scroll. ON TRACK.
Checkpoint 4 (+100 CSS): Void ratio 18%. Content distribution: top 55% of scroll. ON TRACK.
Checkpoint 5 (zone transition): Void ratio 25%. NOTE: Zone 2-3 transition adds 80px void.
  -> SOFT ALERT: Trend watch. Next checkpoint critical.
Checkpoint 6 (+100 CSS): Void ratio 32%.
  -> HARD ALERT to builder: "Void ratio 32%, exceeding 30% threshold. Primary source:
     zone transitions (80px x 3 = 240px). Consider reducing transition spacing or
     adding transition content."
Builder response: "Acknowledged. Reducing zone transitions to 48px."
Checkpoint 7 (+100 CSS): Void ratio 24%. RECOVERY confirmed. ON TRACK.
...
Checkpoint 12 (final): Void ratio 22%. SC-01 PASS. SC-02 PASS. SC-03 PASS.
  -> SUMMARY: Build maintained spatial confidence throughout. One alert at CP6 was
     acknowledged and corrected. No structural concerns.
```

---

## 5. THE CROWN JEWEL PIPELINE

### Reverse-Engineering CD-006 (39/40)

CD-006 was NOT produced by a pipeline. It was produced through a 24-exploration ITERATIVE DISCOVERY process. Here is what its actual pipeline looked like:

**Phase A: Foundation (Density explorations DD-001 through DD-006)**
- 6 progressively complex explorations
- Each built on findings from the previous
- DD-006 was the accumulation of 18 DD-F findings
- Total: approximately 900 minutes (15 hours)

**Phase B: Extension (Organization explorations OD-001 through OD-006)**
- 6 explorations extending density findings to organizational patterns
- Added 17 OD-F findings to the knowledge base
- Total: approximately 600 minutes (10 hours)

**Phase C: Integration (Axis explorations AD-001 through AD-006)**
- 6 explorations integrating density + organization into full-page axis patterns
- Added 28 AD-F findings
- Total: approximately 500 minutes (8 hours)

**Phase D: Synthesis (Combination explorations CD-001 through CD-006)**
- 6 explorations combining all previous findings
- CD-006 operated within 88 simultaneous constraints from 24 prior explorations
- Added 25 CD-F findings
- Total: approximately 500 minutes (8 hours)

**Actual CD-006 pipeline: ~2,500 minutes (41.7 hours) across 24 explorations.**

### Can We Replicate This With Agents?

**No, not in the same way.** The crown jewel pipeline's essential property is ITERATIVE DISCOVERY: each exploration produces new findings that inform the next. This requires 24 sequential experiments with learning between them. Even with unlimited budget, this takes weeks of calendar time.

**But we CAN simulate 80% of the effect by:**

1. **Compressing the finding accumulation:** CD-006 had access to 88 findings. Our mechanism catalog already encodes the 18 most important as named mechanisms with application rules. Our case studies document the 5 most important crown jewel patterns (fractal, combination, axis, density, organization). The builder does NOT need to re-discover these -- they are already in the system.

2. **Simulating iterative discovery through iteration cycles:** Instead of building 24 pages to discover what works, we build 1 page with 3 iteration cycles, using Mode 4 audit as the "discovery" mechanism. Each iteration cycle functions as ~6 explorations compressed: the audit reveals what works and what doesn't, the iteration applies those insights.

3. **Using the crown jewel pages as REFERENCE INPUTS:** The metaphor builder and intentionality builder receive screenshots and structural analysis of DD-006 and CD-006 as reference material. They don't copy specific techniques, but they develop taste for WHAT LEVEL OF QUALITY IS POSSIBLE.

### The Crown Jewel Simulation Protocol

**Step 1: Reference Analysis (20-30 min)**
An Opus agent reads DD-006 and CD-006 source code and produces a "Crown Jewel Technique Inventory":
- Every self-referential element and its mechanism
- Every cognitive transition and its bridge text
- Every structural bookend and its echo
- Every meta-annotation and its function
- Every reinforcing pair and its channel encoding

This inventory becomes an input to the Intentionality Builder (Pass 4).

**Step 2: Quality Calibration (10-15 min)**
The team-lead takes screenshots of DD-006 and CD-006 at 1440px. These screenshots become the REFERENCE STANDARD for the Post-Build Metacognitive Reviewer. The reviewer's question shifts from "does this look good?" to "does this match the quality bar set by the crown jewels?"

**Step 3: Technique Transfer (integrated into Pass 3-4)**
The Metaphor Builder and Intentionality Builder receive the Crown Jewel Technique Inventory alongside their other inputs. They are NOT told to copy techniques -- they are told: "These are examples of the quality level this pipeline should produce. Your page should feel like it BELONGS alongside these pages, not like it was produced by a different team."

### Was It the Pipeline or the Person?

**Answer: Both, but primarily the accumulation.**

CD-006 was built by a single Opus agent with access to 88 accumulated findings. The AGENT was competent but not extraordinary -- any Opus agent with the same 88 findings would have produced something comparable. The ACCUMULATION was the differentiator: 88 findings provided a constraint space so rich that the builder's decisions were heavily guided.

For the flagship pipeline, we simulate the accumulation through:
- The mechanism catalog (18 mechanisms with application rules)
- The case study index (5 crown jewel patterns)
- The Crown Jewel Technique Inventory (new, from Step 1)
- The perceptual auditing skill (48 PA questions)
- The restraint protocol (reject-to-deploy ratio)

This gives our builder access to approximately 60% of CD-006's constraint richness. The other 40% would require the actual iterative discovery process (24 explorations), which we cannot simulate.

**Estimated quality ceiling: 90-95% of CD-006, achieved in ~6 hours instead of ~42 hours.**

---

## 6. CALIBRATION RUN

### Should We Do a Dress Rehearsal?

**YES.** The evidence overwhelmingly supports a calibration run before committing to the flagship.

### What the Calibration Run Tests

| Dimension | What We Learn |
|-----------|---------------|
| Pipeline mechanics | Do the 5 passes and 7 gates work operationally? Do agents understand their prompts? Do handoffs function? |
| Spatial Confidence Gate | Does the 7-check gate actually catch the void problem? (We believe it will, but n=0 on this gate.) |
| Embedded Auditing | Does the embedded auditor protocol work without causing Playwright contention or builder disruption? |
| Competitive Build | Does having 2 skeleton builders produce meaningfully different outputs, or do they converge? |
| Messaging Protocol | Do agents actually use SendMessage? Does the "minimum 8 messages" threshold work? |
| Iteration Protocol | Does a rebuild of Pass 1 based on gate failure produce a BETTER skeleton, or the same one? |
| Timing | How long does the pipeline ACTUALLY take? (Our estimates are based on the ceiling and middle experiments, which used different pipelines.) |
| Crown Jewel Reference | Does the reference material improve intentionality, or does it cause the builder to COPY instead of CREATE? |

### Calibration Run Specification

**Target tier: Ceiling (not Flagship).** This keeps the stakes lower while testing the pipeline at a complexity level that is already challenging.

**Content: DIFFERENT content from the flagship attempt.** If the calibration uses the same content as the flagship, it contaminates the flagship's freshness (agents have continuation bias from the calibration). Use a different page from the same documentation set.

**Pipeline: IDENTICAL to the flagship pipeline** but with these simplifications:
- No competitive build (only 1 skeleton builder) -- saves 1 agent
- No mid-pipeline metacognitive review -- saves 1 agent
- Mode 2 audit instead of Mode 4 (2 PAs instead of 9) -- saves 7 agents
- No iteration cycles (build once, audit, stop) -- saves 30-60 min

**Calibration run team: 18 agents** (30 flagship - 1 competitive - 1 metacog - 7 auditors + some shared)

**Calibration run time: 150-220 min (2.5-3.7 hrs)**

### Cost/Benefit

**Cost:**
- 150-220 min of agent time
- Uses a different content page (this page could have been built at Middle tier in 70 min)
- Calendar time delay before flagship attempt

**Benefit:**
- Pipeline-level bugs caught before flagship commitment
- Gate calibration: if a gate is too strict (fails good output) or too loose (passes bad output), we adjust before flagship
- Timing model: actual timing data replaces estimates
- Team coordination validated: messaging, file handoffs, screenshot protocol all tested
- Reduces flagship failure probability by ~15-20% (from ~25-30% to ~10-15%)

**Verdict: STRONGLY WORTH IT.** The calibration run costs ~180 min and reduces flagship failure probability by ~20%. Given that a failed flagship costs ~380 min (the full pipeline wasted), the expected savings are 0.20 x 380 = 76 min. The calibration costs 180 min but saves 76 min in expected failure cost, AND produces a Ceiling-tier page as output. Net cost: 180 - 76 = 104 min extra, with a Ceiling-tier page as bonus output.

---

## 7. THE IDEAL FLAGSHIP SPECIFICATION

### Target Metrics

| # | Metric | Target | Gate/Score | Non-Negotiable? | Rationale |
|---|--------|--------|-----------|-----------------|-----------|
| 1 | PA-05 DESIGNED | 4/4 | GATE | YES | Below 4/4, the page is not flagship quality. The ceiling scored 1.5/4; this must be the primary gate. |
| 2 | PA-05c PROPORTIONATE | PASS | GATE | YES | This is the specific sub-criterion the ceiling failed. Spatial proportion is the foundation of everything. |
| 3 | Container width | 940-960px | GATE | YES | Confirmed by all evidence as THE primary specification constraint. |
| 4 | Void budget | < 15% contentless | GATE | YES | Prevents the ceiling experiment's catastrophic 70-80% void. |
| 5 | Soul compliance | 8/8 | GATE | YES | Binary rules achieving 100% compliance is a validated principle. |
| 6 | No Severity-1 findings | Zero WOULD-NOT-SHIP | GATE | YES | Any finding flagged as WOULD-NOT-SHIP blocks shipping. |
| 7 | Content distribution | 70%+ of scroll has content | GATE | YES | Prevents front-loading all content into the first 30% of the page. |
| 8 | Footer reachability | < 1 viewport void before footer | GATE | YES | Direct correction from the ceiling experiment. |
| 9 | Novelty | 8+/9 | SCORE | No (target) | Ceiling scored 9/9. Flagship should match or exceed. The perception threshold is at ~7/9. |
| 10 | Better than Ceiling | >= 3 improvements | SCORE | No (target) | Establishes that the pipeline produced meaningful improvement over the previous best. |
| 11 | Mechanism count | 12-14 deployed | SCORE | No (target) | Hard cap at 16. Soft recommendation 12-14. |
| 12 | Signal-to-silence ratio | 0.6-0.8:1 | SCORE | No (target) | For every unit of signal (mechanism deployment), 0.6-0.8 units of designed silence. |
| 13 | Content-to-void ratio | 70:30 or better | SCORE | No (target) | Already captured by metrics 4 and 7. This is a composite metric. |
| 14 | Restraint ratio | 1.5:1 reject-to-deploy | SCORE | No (target) | For every mechanism deployed, 1.5 mechanisms were considered and rejected. Ensures curation over accumulation. |
| 15 | Intentionality dimensions | 4+/6 | SCORE | No (target) | Self-reference, pedagogical sequencing, cognitive transitions, bookending, meta-annotation, provenance threading. 4 of 6 present. |
| 16 | Token compliance | >= 85% var() | SCORE | No (target) | Ceiling was 86%. Flagship should match or exceed. |
| 17 | CPL | 45-80 characters | GATE | YES | Binary rule, builder self-check. |
| 18 | Heading spacing ratio | >= 1.5:1 | GATE | YES | Binary rule, builder self-check. |
| 19 | Inter-agent messaging | >= 12 substantive | SCORE | No (target) | Higher than the 8-message minimum. In a 30-agent pipeline, 12 messages is conservative. |
| 20 | Reinforcing pairs | >= 3 documented | SCORE | No (target) | Ceiling target was 2. Flagship should demonstrate stronger multi-channel encoding. |
| 21 | 4-scale coherence | Nav + Page + Section + Component | GATE | YES | Each scale must have documented CSS evidence. |
| 22 | Transition variation | >= 4 different types | SCORE | No (target) | Ceiling target was 3. Flagship should demonstrate richer transition grammar. |
| 23 | Silence zones | >= 3 documented | SCORE | No (target) | Documented in the Restraint Map. Each is a deliberate design decision. |
| 24 | Metaphor structural score | >= 70% without labels | SCORE | No (target) | Ceiling scored 40% without labels. Flagship metaphor must be STRUCTURAL, not ANNOUNCED. |

### Which Are Gates vs Scores?

**GATES (must pass to ship): 1, 2, 3, 4, 5, 6, 7, 8, 17, 18, 21** -- 11 gates total, all binary-verifiable.

**SCORES (optimize, not blocking): 9, 10, 11, 12, 13, 14, 15, 16, 19, 20, 22, 23, 24** -- 13 scores total, judgment-based.

This follows the binary-vs-judgment principle: gates are binary (100% compliance expected), scores are judgment (optimize but don't block).

### The Minimum Viable Flagship

If the pipeline must produce SOMETHING shippable even under adverse conditions, what is the minimum that qualifies as "flagship"?

**Minimum Viable Flagship (MVF):**
- PA-05: 3/4 (PROPORTIONATE must pass, but DESIGNED can be partial)
- Novelty: 7+/9
- Soul: 8/8
- Container: 940-960px
- Void: < 20% (relaxed from 15%)
- Mechanism count: 10-12
- Intentionality: 2+/6 (bookending + one other)
- Metaphor: recognized by auditors, even if label-dependent

The MVF is essentially "Ceiling-plus" -- a ceiling-tier page with at least 2 intentionality elements. This is the fallback if the full flagship pipeline hits problems at Passes 3-4. The team-lead can declare MVF and ship the best page achieved, rather than burning time trying to reach full flagship.

### Quality Bands

| Band | PA-05 | Novelty | Intentionality | Verdict |
|------|-------|---------|----------------|---------|
| **Crown Jewel** | 4/4 | 9/9 | 5-6/6 | "Belongs alongside DD-006 and CD-006" |
| **Full Flagship** | 4/4 | 8+/9 | 4+/6 | "Clearly a different tier from Ceiling" |
| **Strong Flagship** | 3.5+/4 | 7+/9 | 3+/6 | "Strong page with flagship moments" |
| **Minimum Viable** | 3/4 | 7+/9 | 2+/6 | "Ceiling-plus -- better than Ceiling but not unambiguously Flagship" |
| **Failed Flagship** | < 3/4 | any | any | "Ship as Ceiling after downgrade" |

The GOAL is Full Flagship. Crown Jewel is aspirational. Strong Flagship is acceptable. MVF is the minimum for the "flagship" label. Failed Flagship triggers the kill criteria.

---

## 8. THE RECOMMENDED VERSION

### The Maximum Pipeline vs The Recommended Pipeline

The maximum pipeline (Section 1-7) is the "if money were no object" design. It uses 30 agents, 5 passes, 3 iteration cycles, competitive builds, embedded auditing, and Mode 4 final audit. Total time: ~380 min (6.3 hrs). Total agents: 30.

The recommended pipeline balances ambition with practicality, informed by the evidence about what actually drives quality.

### What to Keep from the Maximum (High ROI)

1. **5 passes (separate Content Architecture, Spatial Skeleton, Mechanism Deployment, Metaphor Integration, Intentionality).** The evidence overwhelmingly shows that combining concerns causes failure. The pass separation is the single most important architectural decision.

2. **7 gates between passes (all binary-verifiable).** Gates are the procedural prevention that the ceiling experiment lacked. Without gates, declarative knowledge ("container width is important") doesn't prevent failure.

3. **Competitive skeleton build.** Pass 1 is the highest-risk pass. Two builders doubles the probability of a good foundation. Cost: 1 additional Sonnet agent.

4. **Embedded auditing for Passes 1-2.** Catches spatial problems at 50 lines instead of 500. Cost: 1 Sonnet agent.

5. **Mode 4 final audit (9 auditors).** The ceiling experiment proved that lighter audits miss dominant failures. The 9-auditor Mode 4 is the only methodology with demonstrated ability to catch gestalt-level problems. Non-negotiable.

6. **Crown Jewel reference material.** Providing the intentionality builder with a technique inventory from DD-006 and CD-006. Cost: 1 Opus agent for 20 min.

7. **Calibration run.** Tests the pipeline before committing to the flagship. Cost: 150-220 min + a Ceiling-tier bonus page.

8. **1 iteration cycle (Iteration 1: spatial).** After the Mode 4 audit, rebuild Pass 1 if spatial problems are found. This is the highest-ROI iteration. Cost: 50-80 min.

### What to Defer from the Maximum (Lower ROI)

1. **Competitive metaphor derivation.** The current metaphor pipeline (single agent) has produced strong metaphors (ceiling scored 12/12). The problem was never the metaphor QUALITY but the metaphor SPATIAL COST. The Spatial Budget Gate (Pass 0) catches this more cheaply. Defer.

2. **Iterations 2-3.** Each additional iteration has diminishing returns. Iteration 1 (spatial) has EXTREME ROI. Iteration 2 (composition) has HIGH ROI but costs 40-60 min. Iteration 3 (intentionality) has MODERATE ROI. The recommended pipeline includes Iteration 1 only, with Iteration 2 as a contingency if Iteration 1 produces strong spatial results but weak composition.

3. **Pre-Mortem and Mid-Pipeline metacognitive reviewers.** These are valuable in theory but add 2 Opus agents and 20-30 min. The recommended pipeline keeps only the Post-Build Reviewer (the most impactful of the three).

4. **Competitive intentionality layer.** Already rejected in Section 2.

### The Recommended Pipeline

**Team: 22 agents (7 Opus + 15 Sonnet)**

| Role | Count | Model |
|------|-------|-------|
| Team Lead | 1 | Opus 4.6 |
| Metaphor Derivation Agent | 1 | Opus 4.6 |
| Content Architect (Pass 0) | 1 | Opus 4.6 |
| Planner | 1 | Opus 4.6 |
| Skeleton Builder A (Pass 1) | 1 | Sonnet 4.5 |
| Skeleton Builder B (Pass 1, competitive) | 1 | Sonnet 4.5 |
| Embedded Auditor (Passes 1-2) | 1 | Sonnet 4.5 |
| Mechanism Builder (Pass 2) | 1 | Sonnet 4.5 |
| Metaphor Builder (Pass 3) | 1 | Opus 4.6 |
| Intentionality Builder (Pass 4) | 1 | Opus 4.6 |
| Crown Jewel Analyst | 1 | Opus 4.6 |
| Post-Build Reviewer | 1 | Sonnet 4.5 |
| Spatial Confidence Auditor | 1 | Sonnet 4.5 |
| Mechanism Perception Auditor | 1 | Sonnet 4.5 |
| Metaphor Fresh-Eyes (2) | 2 | Sonnet 4.5 |
| Mode 4 Auditors (9) | 9 | Sonnet 4.5 (reuse Spatial, Mechanism, Fresh-Eyes agents for 4 of these) |
| Programmatic Auditor | 1 | Sonnet 4.5 |
| Comparative Auditor | 1 | Sonnet 4.5 |
| Novelty Evaluator | 1 | Sonnet 4.5 |
| Verdict Synthesizer | 1 | Opus 4.6 (team lead can do this) |

Note: With agent reuse, the actual number of concurrent agents peaks at ~12 (during Mode 4 audit phase).

**Timeline: ~280-360 min (4.7-6.0 hrs)**

| Phase | Duration | Notes |
|-------|----------|-------|
| Metaphor Derivation + Content Prep | 25-35 min | Parallel |
| Pass 0: Content Architecture | 15-25 min | Sequential |
| Pass 1: Competitive Skeleton Build | 35-50 min | 2 builders parallel + embedded auditor |
| Gates 1-2: Spatial Budget + Confidence | 15-20 min | Sequential |
| Pass 2: Mechanism Deployment | 25-40 min | Sequential + embedded auditor |
| Gate 3: Composition Verification | 10-15 min | Sequential |
| Pass 3: Metaphor Integration | 20-35 min | Sequential |
| Gate 4: Metaphor Coherence | 10-15 min | 2 fresh-eyes parallel |
| Pass 4: Intentionality Layer | 15-25 min | Sequential |
| Post-Build Review | 10-15 min | Sequential |
| Mode 4 Audit | 30-45 min | 9 auditors parallel (screenshot pre-capture) |
| Iteration 1 (if needed, ~50% chance) | 40-60 min | Spatial rebuild |
| Post-Iteration Audit (if needed) | 20-30 min | Targeted |
| Verdict Synthesis | 15-20 min | Sequential |
| **TOTAL** | **280-360 min** | **Expected: ~320 min (5.3 hrs)** |

**Probability of shipping at Full Flagship or better: 55-65%**
**Probability of shipping at Strong Flagship or better: 70-80%**
**Probability of shipping at MVF or better: 85-90%**

### Comparison: Maximum vs Recommended

| Dimension | Maximum | Recommended | Delta |
|-----------|---------|-------------|-------|
| Agents | 30 | 22 (12 concurrent peak) | -8 agents |
| Time | 380 min | 320 min | -60 min |
| Passes | 5 | 5 | Same |
| Iterations | 3 | 1 (+ 1 contingency) | -1 to -2 |
| Gates | 7 | 7 | Same |
| Competitive builds | 2 (skeleton + metaphor) | 1 (skeleton only) | -1 |
| Mode 4 auditors | 9 | 9 | Same |
| Metacognitive reviewers | 3 | 1 | -2 |
| Calibration run | Yes | Yes | Same |
| Crown jewel reference | Yes | Yes | Same |
| Embedded auditing | Passes 1-2 | Passes 1-2 | Same |
| P(Full Flagship) | 65-75% | 55-65% | -10% |
| P(MVF or better) | 90-95% | 85-90% | -5% |

The recommended version achieves ~90% of the maximum version's success probability at ~85% of the cost. The 10% probability gap comes from the removed iterations and metacognitive reviewers, which have diminishing ROI.

### The Decision Matrix

| If you have... | Use... | Expected result |
|----------------|--------|-----------------|
| Unlimited budget, unlimited time | Maximum pipeline (30 agents, 380 min) | 65-75% chance of Full Flagship |
| Large budget, some time concern | Recommended pipeline (22 agents, 320 min) | 55-65% Full Flagship, 85-90% MVF |
| Moderate budget | Flagship architecture (13 agents, 200-280 min) | 49-75% chance of better-than-Ceiling |
| Budget-conscious | "Ceiling + iterate twice" (10 agents, 235 min) | 70-80% chance of strong Ceiling |
| Minimum viable | Middle tier (8 agents, 70 min) | 85% chance of PA-05 4/4 |

---

## APPENDIX A: PIPELINE FLOW DIAGRAM

```
                         CONTENT
                           |
                    +------+------+
                    |             |
            Metaphor Deriv   Content Prep
            (Opus, 30min)    (reuse)
                    |             |
                    +------+------+
                           |
                    CONTENT ARCHITECT
                    (Opus, Pass 0)
                    Spatial Budget + Zone Plan
                           |
                    [SPATIAL BUDGET GATE]
                    Binary: content-to-void >= 60:40?
                           |
                    PRE-MORTEM REVIEW (max only)
                           |
              +------------+------------+
              |                         |
       SKELETON A              SKELETON B
       (Sonnet, Pass 1)       (Sonnet, Pass 1)
              |                         |
              +----> EMBEDDED <----+
              |      AUDITOR       |
              +------------+------------+
                           |
                    TEAM LEAD SELECTS BETTER SKELETON
                           |
                    [SPATIAL CONFIDENCE GATE]
                    7 binary checks + squint test
                           |
                    MECHANISM BUILDER
                    (Sonnet, Pass 2)
                    + EMBEDDED AUDITOR
                           |
                    [COMPOSITION VERIFICATION GATE]
                    Mechanism presence + perception check
                           |
                    MID-PIPELINE REVIEW (max only)
                           |
                    METAPHOR BUILDER
                    (Opus, Pass 3)
                           |
                    [METAPHOR COHERENCE GATE]
                    2 fresh-eyes: structural or announced?
                           |
                    INTENTIONALITY BUILDER
                    (Opus, Pass 4)
                    Input: Crown Jewel Technique Inventory
                           |
                    POST-BUILD REVIEW
                    (cold-eyes, emotional response)
                           |
                    [MODE 4 AUDIT]
                    9 auditors, 48 questions, 27 cold looks
                    Screenshot pre-capture pattern
                           |
                 +----SHIP DECISION----+
                 |         |           |
              SHIP    ITERATE 1    ITERATE 2-3
           (Full+)   (Spatial)    (Composition)
                       |               |
                  REBUILD PASS 1  REBUILD PASS 2-3
                       |               |
                  [RE-GATE]        [RE-GATE]
                       |               |
                  [TARGETED AUDIT]     |
                       |               |
                  VERDICT SYNTHESIS----+
                       |
                    SHIP or DOWNGRADE
```

## APPENDIX B: WHAT MAKES THIS PIPELINE DIFFERENT FROM THE 3-PASS ARCHITECTURE

| Dimension | 3-Pass Architecture (Section E) | No-Compromise (This Document) |
|-----------|--------------------------------|-------------------------------|
| Pass count | 3 (Skeleton, Elaboration, Intentionality) | 5 (Content Architecture, Skeleton, Mechanisms, Metaphor, Intentionality) |
| Key innovation | Spatial Confidence Gate between passes | SINGLE-CONCERN passes (each pass does ONE thing) |
| Builder count | 3 (skeleton, elaboration, intentionality) | 5-6 (content architect, 2 skeleton, mechanism, metaphor, intentionality) |
| Competitive build | No | Yes (skeleton) |
| Embedded auditing | No | Yes (Passes 1-2) |
| Crown jewel reference | No | Yes (technique inventory) |
| Calibration run | No | Yes (full pipeline test at Ceiling) |
| Metacognitive review | No | 1-3 reviewers at different stages |
| Iteration model | Implicit (fix cycles at gates) | Explicit (full pass rebuild based on Mode 4 findings) |
| Total agents | 13-16 | 22-30 |
| Total time | 200-280 min | 280-460 min |
| Key risk reduction | Spatial gate prevents void | Single-concern passes prevent attention fragmentation |

The fundamental philosophical difference: the 3-pass architecture TRUSTS the builder to manage multiple concerns within each pass (skeleton + mechanisms in Pass 1, metaphor + elaboration in Pass 2). The no-compromise architecture DOES NOT TRUST any single agent with multiple concerns. Every pass has ONE focus. The evidence from the ceiling experiment shows that multi-concern passes produce failures at the concern the builder deprioritizes.

## APPENDIX C: RISK REGISTER

| # | Risk | Probability | Impact | Mitigation |
|---|------|-------------|--------|------------|
| R1 | Metaphor creates spatial obligations that produce void | 30% | BLOCKING | Spatial Budget Gate (Pass 0) catches this before any build begins. Metaphor must be expressible within 960px + 15% void budget. |
| R2 | Competitive builders produce identical skeletons | 20% | WASTED effort | Builders receive different "approach hints" (one spatial-first, one content-first). If identical, pick either -- no harm done, just wasted one agent. |
| R3 | Embedded auditor causes Playwright contention | 15% | Delays build | Auditor uses exclusive Playwright access. Builder writes files directly, never uses Playwright. Sequential scheduling. |
| R4 | 5 passes create handoff degradation (each pass loses information from previous) | 25% | Quality loss | Each pass receives ALL previous pass outputs (not just the immediate predecessor). The intentionality builder sees the content architecture, skeleton, mechanism plan, AND metaphor derivation. |
| R5 | Mode 4 audit finds problems that require Pass 0-level restructuring (not fixable by iteration) | 15% | KILL trigger | If Mode 4 reveals architectural unfitness (content doesn't match structure), downgrade to MVF or abort. Pre-mortem review (in max pipeline) mitigates this. |
| R6 | Crown jewel reference causes copying instead of creation | 20% | Derivative output | Reference is technique inventory (abstracted), not screenshots (concrete). Builder prompt says: "match the QUALITY LEVEL, not the specific techniques." Novelty evaluator catches copying. |
| R7 | Pipeline takes >6 hours, hitting diminishing returns wall | 25% | Cost overrun | Kill criterion at 400 min (max) or 360 min (recommended). Ship at best current state. |
| R8 | Zero inter-agent messaging despite protocol | 30% | Quality defects | Team-lead checks at Pass 1 completion. If zero messages, PAUSE and explicitly prompt builders. Hard requirement: builder must acknowledge embedded auditor alerts. |
| R9 | Calibration run produces poor results, shaking confidence | 15% | Morale/decision | Calibration targets Ceiling (lower bar). Even if calibration produces Middle-quality output, the pipeline data is still valuable. Calibration success is PIPELINE validation, not page quality. |
| R10 | Too many gates cause builder frustration and corner-cutting | 10% | Process gaming | Gates are BINARY (pass/fail), not judgment. No room for gaming. Builders cannot bypass gates because handoffs are structurally enforced. |

---

**END NO-COMPROMISE FLAGSHIP PIPELINE DESIGN**

**Summary:**
- Maximum pipeline: 30 agents, 5 passes, 3 iterations, 7 gates, ~380 min, 65-75% P(Full Flagship)
- Recommended pipeline: 22 agents, 5 passes, 1 iteration, 7 gates, ~320 min, 55-65% P(Full Flagship), 85-90% P(MVF)
- Key innovations: single-concern passes, competitive skeleton build, embedded auditing, crown jewel reference, calibration run
- Philosophical core: DO NOT TRUST any single agent with multiple concerns. Decompose until each pass has ONE focus.
