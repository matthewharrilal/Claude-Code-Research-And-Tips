# Adversarial Review â€” Pipeline v3 Comprehensive Audit
## Challenging the Findings, Exposing the Gaps

**Reviewer:** Adversarial Auditor (Opus 4.6)
**Date:** 2026-02-24
**Sources:** All 7 core audit reports + 6 pipeline source artifacts
**Method:** Systematically challenge every "UPGRADED," "FULLY FIXED," and positive assessment. Surface cross-audit contradictions and shared blind spots. Paint the most plausible failure scenario.

---

## EXECUTIVE SUMMARY (Adversarial Verdict)

**The audit collectively presents a pipeline that is "ready for production." I dispute this conclusion.**

The 7 auditors produced thorough, technically competent reports. But they share a systematic bias: they evaluated whether the SPECIFICATION is correct, not whether the specification will be FOLLOWED. The pipeline's quality depends on an LLM orchestrator reading ~2,255 lines of instructions (MANIFEST.md + artifact-orchestrator.md) and executing every step perfectly. No auditor ran the pipeline. No auditor tested whether an Opus agent can actually hold the 40-50 discrete rules in working memory simultaneously. The "ready for production" conclusion is based entirely on spec analysis, not execution evidence.

**My verdict: READY FOR FIRST MONITORED RUN, NOT PRODUCTION.**

The difference matters. "Production" implies repeatable, reliable execution. This pipeline has never been executed in its v3 form. It should be run once, under observation, with the execution tracker fully populated, before anyone calls it production-ready.

**Key challenges:**
- 3 ratings I believe are overgenerous (Section 1)
- 2 ratings I believe are too harsh (Section 2)
- 4 cross-audit contradictions (Section 3)
- 5 questions nobody asked (Section 4)
- 1 detailed failure scenario (Section 5)
- Fresh-eyes rating of 3.5/5 is approximately correct but for the wrong reasons (Section 6)
- The audit methodology has a structural hole (Section 7)

---

## Section 1: Challenged Ratings (Overrated Items)

### 1.1 M-05 "Tiered Brief Structure" Should NOT Be UPGRADED

**What the anti-loss auditor said:** M-05 was UPGRADED from PARTIALLY EFFECTIVE because BV-01 (gate-runner-core.js lines 29-67) closes GAP-01 by enforcing tier line budgets.

**My challenge:** BV-01 checks LINE COUNT, not CONTENT QUALITY. The original failure (GAP-01) was that "Tier 4 at 62.5%" -- meaning the technique section was too thin. BV-01 would catch a brief with 25 lines of Tier 4 content (below the 25.6 threshold at 80% of 32). But it would PASS a brief with 32 lines of Tier 4 filler. For example:

```
## Tier 4: Disposition
D-01: Content-Register Reading. See vocabulary.
D-02: Room Perception. See vocabulary.
D-03: Signing Frame. See vocabulary.
[... repeated 29 more times with no CSS property:value pairs ...]
```

This would pass BV-01 (32 lines, meets 80% threshold) while being functionally equivalent to the original failure. The unfixed-items auditor correctly identified this gap: "the explicit 'each disposition must include at least one CSS property:value pair' guard is missing" (audit-unfixed-items.md, Item 1, Fix 1.2 assessment). Yet the anti-loss auditor rated it UPGRADED.

**Evidence of contradiction:** The anti-loss auditor (audit-anti-loss-mechanisms.md, M-05, line 102) claims "the original 25/40 failure (62.5%) would now trigger FAIL BV-01." True -- but a 32/40 brief of empty disposition labels would PASS. The mechanism catches the EXACT SAME failure but not the SAME CLASS of failure. That is "PARTIALLY IMPROVED," not "UPGRADED."

**Correct rating:** STILL PARTIAL (improved from original). Line count enforcement is a necessary but insufficient condition for brief quality.

### 1.2 GR-51 "Background Delta Distribution" Is Weaker Than Claimed

**What the anti-loss auditor said:** M-14 was UPGRADED because GR-51 (gate-runner-core.js lines 1170-1223) closes GAP-02 by enforcing distribution, not just minimum.

**My challenge:** GR-51 has a critical edge case in its zone detection. Lines 1178 show the section selector:

```javascript
const sections = document.querySelectorAll('section, [class*="zone"], [class*="section"]');
```

This selector relies on the builder using `<section>` elements OR class names containing "zone" or "section." If a builder uses `<div class="content-block">` or `<article>` elements, GR-51 finds zero sections and returns `{ pass: true, note: 'No zone boundaries found' }` (line 1190-1191). A page with all deltas at 15 RGB but no `section` tags would PASS GR-51.

**Why this matters:** The gate-quality auditor (audit-gate-quality.md, Section 5B) identified 9 gates with calibration concerns. GR-51 was listed at MEDIUM severity with the concern about the 50%/25 RGB threshold. But the gate-quality auditor did NOT catch the selector fragility. GR-11 uses a similar approach but with broader selectors. GR-51's selector is narrower.

The anti-loss propagation auditor (audit-anti-loss-propagation.md, Trace 1) validated the BV gates end-to-end but did not trace GR-51's DOM selection path. This is the gap between "does the code exist?" (yes) and "does the code work on realistic HTML?" (untested).

**Correct assessment:** GR-51 is a meaningful improvement but fragile. The "UPGRADED" rating for M-14 depends on the builder producing section-tagged HTML, which is probable but not guaranteed. The rating should carry a caveat: "UPGRADED, contingent on builder using semantic section elements."

### 1.3 The Unfixed Items Audit Is Too Generous With "FULLY FIXED" for Item 3

**What the unfixed-items auditor said:** Item 3 (Gate Runner Coverage, 19/42) is FULLY FIXED because "all 3 sub-fixes implemented and exceed specifications."

**My challenge:** The auditor verified that gate code EXISTS and that the manifest DOCUMENTS the execution order. But nobody verified that the code RUNS correctly. The fresh-eyes auditor (audit-buddy-fresh-eyes.md, Red Flag 4) correctly identified this: "I see no test file, no test HTML fixture, and no evidence that these functions have been run against known-good and known-bad HTML."

The unfixed-items auditor's "exceed specifications" claim is based on reading JavaScript code and confirming it matches the spec. That is spec-to-code traceability, not functional verification. Item 3 should be rated "FULLY SPECIFIED, UNTESTED" -- a meaningful distinction from "FULLY FIXED."

**Evidence:** gate-runner-core.js is 1,437 lines of JavaScript. Zero lines of test code exist anywhere in the artifact set. The gate-quality auditor evaluated thresholds and calibration but did not execute the code. This is the single most important untested component in the pipeline, and all 7 auditors treated it as verified.

---

## Section 2: Under-Appreciated Strengths

### 2.1 M-06 "Suppressor Management" Might Actually Be Good Enough

**What the anti-loss auditor said:** M-06 is STILL PARTIAL because the survival function S(x) has no executable implementation, and density stacking is not gated.

**My challenge from the other direction:** BV-04 (gate-runner-core.js lines 104-117) checks the brief for 4 suppressor patterns with ZERO tolerance. These 4 patterns (S-01 sample range, S-02 raw prohibition, S-11 compliance voice, S-08 count-gate) cover the 4 most damaging suppressors identified across the entire research history. The survival function S(x) = 1/(1+C(x)) was always a theoretical construct -- it has never been computed in any pipeline execution. Calling its absence a "gap" conflates theoretical completeness with operational necessity.

The 262 density violations cited in the catalog were false positives from table cells (the gate-quality auditor confirms this at audit-gate-quality.md, FIX 6, lines 176-181). After GR-17's table cell exemption fix, the density stacking issue is substantially addressed.

**Correct assessment:** M-06 should be UPGRADED (not STILL PARTIAL). The 4-pattern BV-04 scan plus GR-17's density fix address the operational failures. The survival function is a research artifact that has never impacted execution.

### 2.2 The Gate Count "Discrepancy" Is Actually Well-Documented

**What the fresh-eyes auditor said:** Gate count inconsistency between gate-runner-spec.md (35), gate-manifest.json (31/41), MANIFEST (37), and gate-runner-core.js header (31+4+2=37) "will confuse an orchestrator."

**My challenge:** The gate-manifest.json (lines 140-147) provides an unambiguous accounting:
- `inGateRunner: 31` (Playwright-executable)
- `inOrchestrator: 6` (text-based decision rules)
- `removed: 13` (historical, no longer active)
- `total: 37` (active gates)
- `briefVerification: 4`
- `grandTotal: 41` (all gates ever defined)

The "37 gates" figure used in the MANIFEST is the active gate count. The "31+4+2=37" in gate-runner-core.js breaks this into Playwright (31) + BV (4) + diagnostic (2). The fresh-eyes auditor was confused because they read multiple documents with different granularity, but the canonical source (gate-manifest.json) is unambiguous.

The gate-runner-spec.md "35" figure is the only genuine discrepancy. This is a documentation error in ONE file, not a systemic inconsistency. The fresh-eyes auditor elevated a single-file typo to a systemic concern.

---

## Section 3: Cross-Audit Contradictions

### 3.1 The Integrative Auditor PA-05 Contradiction: Is It Fixed or Not?

**Fresh-eyes auditor (Red Flag 1):** "The Integrative Auditor prompt template (Appendix E, line 1129) tells the agent to score PA-05 sub-criteria... This is a BLOCKING inconsistency."

**Actual MANIFEST.md lines 1126-1131:**
```
You are the Integrative Auditor. You have NO assigned questions...
NOTE: PA-05 scoring and Tier 5 synthesis are performed by the
**Weaver** (per FIX-083), NOT the Integrative Auditor. Do NOT
score PA-05 here.
```

**Verdict:** The fresh-eyes auditor was WRONG. The prompt template explicitly says "Do NOT score PA-05 here." This is one of the fresh-eyes agent's 5 red flags, and it is a false alarm. The fresh-eyes auditor misread the template or read a stale version. No other auditor caught this error because they all read the actual files. This undermines the fresh-eyes auditor's credibility on other claims.

However, this raises a meta-question: the fresh-eyes auditor cited "Appendix E, line 1129" which in the actual MANIFEST corresponds to the correct passage. Either the fresh-eyes agent misinterpreted "Rate each sub-criterion PASS/FAIL" as PA-05 scoring (it is the cross-validation instruction to secondary auditors, not the Integrative Auditor), or the fresh-eyes agent hallucinated content. Either way, this specific red flag is invalid.

### 3.2 GR-51 and M-14: Two Auditors, Two Different Assessments

**Anti-loss auditor (M-14):** "GR-51 closes GAP-02. UPGRADED."

**Gate-quality auditor (Section 5B, GR-51):** "MEDIUM severity calibration concern. The 50%/25 RGB threshold may be strict for content with intentionally subtle transitions."

These are not directly contradictory but they pull in opposite directions. The anti-loss auditor says GR-51 is strong enough to close a gap. The gate-quality auditor says GR-51 might be too strict and produce false positives. Which is it? If GR-51 is too strict, it might BLOCK builds that have valid subtle transitions. If it is too lenient (my Section 1.2 argument about selector fragility), it might MISS the exact failure it was designed to catch.

**Resolution:** Both are partially right. GR-51 is well-calibrated for the specific failure mode (all deltas clustered at 15-17 RGB) but fragile on selector coverage. The auditors were looking at different dimensions of the same gate.

### 3.3 PA-10 Removal: Contradictory Reporting

**PA-quality auditor (Part 1, Section 1.3):** Initially says "PA-10... NOT REMOVED" then corrects itself mid-paragraph: "Corrected assessment: PA-10 is ABSENT from both pa-questions.md and pa-deployment.md. The removal is COMPLETE."

This self-correction within the report is fine, but it reveals a process issue: the PA-quality auditor initially searched for PA-10 by its text in pa-questions.md, found it missing, but then checked the unfixed-items audit recommendation list and was confused by the wording. The final "All 3 removals: IMPLEMENTED (3/3)" is correct, but the mid-paragraph correction suggests the auditor was working through confusion in real-time rather than verifying systematically.

### 3.4 Value Tables "Optional" vs "Required": Three Auditors, Three Framings

**Anti-loss auditor (M-15):** "Value tables remain advisory. STILL PARTIAL."

**Propagation auditor (Trace 3):** "Value tables propagate. Loss probability 5%. But the 'optional' flag is dangerous."

**Fresh-eyes auditor (Section 4):** Does not flag value tables as a concern at all.

The propagation auditor identified the "optional" flag (MANIFEST line 1089: "optional, include if token budget allows") as a risk but rated propagation at only 5% loss probability. This seems inconsistent -- if the flag is "dangerous" (the auditor's word), why is loss probability only 5%? The anti-loss auditor rated value tables as STILL PARTIAL, implying a larger concern. These assessments are not contradictory but they reveal different risk appetites among auditors without a shared calibration.

---

## Section 4: Meta-Blindness (What Nobody Asked)

### 4.1 Nobody Tested Whether the Pipeline Can Actually Execute

This is the most significant shared blind spot. All 7 auditors performed STATIC ANALYSIS of artifacts. None executed the pipeline. None ran gate-runner-core.js against real HTML. None assembled a brief and checked BV gates. None spawned 9 auditors and verified they could answer 69 questions from screenshots.

The closest any auditor came was the fresh-eyes agent (Red Flag 4): "I see no test file... and no evidence that these functions have been run." But even this auditor did not RUN the code -- they noted its absence as a concern and moved on.

**What this means:** Every "FULLY FIXED" and "UPGRADED" rating in every audit is based on reading code, not running code. The pipeline has ZERO execution evidence in its v3 form. The Gas Town execution used v1/v2 artifacts. The current v3 artifacts are a theoretical improvement over a tested baseline.

### 4.2 Nobody Asked About Orchestrator Context Window Pressure

The MANIFEST.md is 1,197 lines. The artifact-orchestrator.md is 1,058 lines. Together, the orchestrator must absorb ~2,255 lines of instructions BEFORE spawning its first agent. The fresh-eyes auditor noted this ("40-50 discrete rules") but did not quantify the token cost.

At ~1,300 tokens per MANIFEST page, the orchestrator's initial context is ~18,000 tokens of instructions alone. After spawning Content Analyst and receiving a Content Map (~500 tokens), then spawning Brief Assembler and receiving a brief (~2,000 tokens), the orchestrator's context is ~20,500 tokens of pipeline state. This is well within Opus's 200K window, but the QUALITY of instruction-following degrades with context length even within limits. No auditor modeled this degradation.

**The question nobody asked:** "At what point in the pipeline does the orchestrator's instruction-following fidelity drop below acceptable levels?" The propagation auditor came closest with "orchestrator compliance is the weakest link" (audit-anti-loss-propagation.md, Finding 1), but did not quantify the degradation curve.

### 4.3 Nobody Asked About the Cost of a Failed Run

The pipeline costs approximately 15 agents x ~$2-5 per agent = $30-75 per run. A REBUILD adds another $30-75 plus human review time. The circuit breaker allows 1 REBUILD + 1 REFINE before terminating, meaning a worst-case run costs $90-225 plus 3-5 hours of human time.

No auditor assessed whether the pipeline's quality guarantees are proportionate to this cost. Is a 92/100 PA question quality score worth $75 per run? Is the marginal improvement from BV-01 through BV-04 (preventing brief defects) worth the 5-minute latency it adds? These are cost-effectiveness questions that no specification audit can answer.

### 4.4 Nobody Asked Whether 69 Questions Is Too Many

The PA-quality auditor evaluated question quality, auditor load balance, and failure mode coverage. But nobody asked the meta-question: does the pipeline NEED 69 questions?

Historical evidence: The Gas Town PA used 65 questions and achieved a 94/100 compliance score. The pipeline added 7 questions (PA-71 through PA-77) to cover "gaps." But if Gas Town scored 94/100 with 65 questions, the marginal value of 4 more questions is at most 6 points. The cost: 4 additional questions add ~20 minutes of auditor time across the 9-auditor pool and increase the weaver's synthesis burden.

The PA-quality auditor noted Auditor D's overload (11 questions) but did not question whether D's new questions (PA-71, PA-74, PA-75) could have been omitted entirely. PA-75 (completion quality) overlaps with PA-13 (clear visual ending) -- the auditor acknowledged this but did not recommend removal.

**The question nobody asked:** "What is the minimum question set that catches all historical failure modes?" The answer might be 45-50, not 69.

### 4.5 Nobody Asked About the Brief Assembler's Fidelity

The pipeline's quality depends on Phase 1: the Brief Assembler compresses ~2,000+ lines of artifact content into a ~100-165 line execution brief. BV-01 through BV-04 verify the brief's FORMAT (line counts, verb ratios, suppressor patterns, hex delta). But no gate or audit verified the brief's CONTENT FIDELITY.

Specifically: does the Brief Assembler correctly extract and synthesize content from the TC Brief Template, soul constraints, perception thresholds, recipe phases, and dispositions? The unfixed-items auditor identified Fix 1.2 (technique density per disposition) as PARTIALLY FIXED, but nobody traced whether the Brief Assembler actually preserves CSS property:value pairs during compression.

The propagation auditor (Trace 1, J4) noted the 20% risk that "the orchestrator... tries to modify the brief itself" during BV revision cycles. But nobody asked what happens when the Brief Assembler performs correct but lossy compression -- dropping nuance that passes BV gates but degrades builder output.

---

## Section 5: Most Plausible Failure Scenario

### The Scenario: "Compliant But Flat"

**Setup:** A first-time orchestrator executes Pipeline v3 with new content (not Gas Town). The orchestrator reads the MANIFEST quickstart, follows the 11 steps, and executes each phase.

**What goes right:**
1. Content Analyst produces a reasonable Content Map.
2. Brief Assembler produces a brief that passes all 4 BV gates (BV-01 through BV-04).
3. Builder produces an HTML file using the recipe format.
4. All identity gates (GR-01 through GR-10) PASS.
5. All perception gates (GR-11 through GR-15, GR-44) PASS.
6. GR-48 confirms all required gates ran.

**What goes wrong (and nobody catches it):**

7. The brief passes BV-01 but with minimal Tier 4 content -- 32 lines of disposition labels without CSS property:value pairs. The builder receives "D-01: Content-Register Reading. See vocabulary." instead of "D-01: Content-Register Reading. Deploy: zone padding 48px-to-64px progression, border-left 4px #E83025 on pull quotes."

8. The builder, receiving thin disposition instructions, exercises its 80% creative authority to produce a page that is technically compliant but ASSEMBLED rather than DESIGNED. All backgrounds use value-table colors (passing GR-51). All spacing is above 12px (passing GR-17). All letter-spacing is above 0.025em (passing GR-12). Container is 960px (passing GR-03). But the visual treatment is uniform across zones -- same padding, same typography, same border treatment -- because the builder had no disposition-specific guidance.

9. GR-20 (Structural Echo) checks bg + padding + borderBottom signatures across consecutive sections. The builder used different background colors (from value tables) so GR-20 PASSES even though the page feels monotonous.

10. GR-52 (Section Height Variation) finds 3 height bands because different zones have different content lengths. PASSES, but the variation is incidental, not designed.

11. The PA audit catches the monotony: Auditor F flags PA-41 (uniform rhythm), Auditor D flags PA-35 (front-loaded design), Auditor A rates PA-05 at 2.5/4 (ASSEMBLED -- designed elements present but not unified). The weaver synthesizes a REFINE verdict.

12. The pipeline terminates with "action items for a new manual run." The human must start over with a better brief, which requires debugging WHY the brief was thin.

**The root cause:** BV-01 checks line counts but not content density. The Brief Assembler produced a formally correct but informationally sparse brief. This is EXACTLY the failure mode the catalog identified (M-05 GAP-01, Tier 4 under-budgeting) in a different guise. The line count fix caught the EXTREME case (25/40 = 62.5%) but not the MODERATE case (32/40 = 80% with thin content).

**Why nobody caught this:** The anti-loss auditor rated M-05 as UPGRADED based on BV-01's line count enforcement. The unfixed-items auditor noted Fix 1.2 as PARTIALLY FIXED but rated the overall item as "good shape." No auditor connected the two findings into a coherent failure scenario.

**Probability:** 30-40% on the first run with new content. The Gas Town execution had good dispositions because the extraction was deep. A less curated extraction would produce thinner dispositions that BV-01 would not catch.

**Impact:** 3-5 hours wasted (full pipeline run + human review + second attempt). $75-150 in compute costs.

---

## Section 6: Fresh-Eyes Calibration

### 6.1 Is 3.5/5 the Right Rating?

The fresh-eyes agent rated the pipeline 3.5/5 ("FUNCTIONAL -- would work with competent orchestrator, but fragile"). I believe this is approximately correct, but I would rate it 3.25/5 for different reasons.

**Where the fresh-eyes agent was right:**
- The 73/165/3,600 line count confusion is real and will trip first-time orchestrators (Section 1, point 1).
- The missing test fixtures for gate-runner-core.js is a genuine risk (Red Flag 4).
- The over-documentation concern (provenance citations, evidence taxonomy, suppressor history) is valid and well-argued (Section 5).
- The missing cheat sheet, sample content, timing breakdown, and rollback format are real operational gaps (Section 6).

**Where the fresh-eyes agent was wrong:**
- **Red Flag 1 (Integrative Auditor PA-05 contradiction) is a false alarm.** The prompt template at MANIFEST line 1131 explicitly says "Do NOT score PA-05 here." The fresh-eyes agent either misread the template or hallucinated a contradiction that does not exist. This is the agent's most prominent red flag and it is wrong. See Section 3.1 above for full evidence.
- **The dual-route pattern repetition (Section 5, "over-engineered" point 1)** is actually necessary. The fresh-eyes agent says it is stated "at least 8 times" and "once would suffice." But the dual-route pattern is the pipeline's most critical isolation mechanism. Stating it once would create a single point of failure -- if the orchestrator misses that one mention, the builder receives gate thresholds (the #1 historical failure mode). Redundancy in life-safety instructions is a feature, not a bug.

**Where the fresh-eyes agent missed something because they lacked context:**
- The fresh-eyes agent did not know about the Flagship experiment (PA-05 1.5/4) or the Middle experiment (PA-05 4/4). Without this history, they could not assess whether the BV gates address PROVEN failure modes or THEORETICAL ones. Every "where is the evidence?" question from the fresh-eyes agent would have been answered by 5 minutes of research history context. The fresh-eyes principle prevents this by design, which means the fresh-eyes agent's quality assessment is necessarily superficial on the "why" dimension.
- The fresh-eyes agent flagged council-verdict.md's ambiguous role ("do I need it or not?"). With context, the answer is clear: council-verdict.md is the tiebreaker for unresolved contradictions, not required reading for normal execution. But the MANIFEST's hedging language ("all rulings are incorporated" + "read it to resolve ambiguities") is indeed confusing, so the fresh-eyes finding has merit even if the underlying concern is minor.

### 6.2 Corrected Fresh-Eyes Rating

I would rate the pipeline 3.25/5 with this breakdown:

| Dimension | Fresh-Eyes | My Rating | Reason for Difference |
|-----------|-----------|-----------|----------------------|
| Usability | 3/5 | 3/5 | Agree |
| Navigation | 4/5 | 3.5/5 | MANIFEST is thorough but too long; fresh-eyes agent was generous here |
| Consistency | 3.5/5 | 3.75/5 | Fresh-eyes Red Flag 1 is invalid, raising consistency score |
| Complexity | 3/5 | 3/5 | Agree |
| Design quality | 4/5 | 3.5/5 | Fresh-eyes agent lacks context to assess design quality of verification architecture |
| Completeness | 3/5 | 3/5 | Agree |

---

## Section 7: Methodology Critique

### 7.1 Were 7 Auditors the Right Decomposition?

The 7 auditors were:
1. Anti-loss mechanism verification (25 mechanisms)
2. Gate quality evaluation (37+4 gates)
3. PA question quality evaluation (69 questions)
4. Unfixed item tracing (10 items)
5. Logging and retrospection capability
6. Anti-loss propagation (5 end-to-end traces)
7. Fresh-eyes review (zero-context evaluation)

**What was missing:**

**An EXECUTION auditor.** No auditor was tasked with actually running gate-runner-core.js against test HTML, or assembling a brief and running BV-01 through BV-04, or spawning a PA auditor with screenshots. This is the single most important gap in the audit methodology. Every auditor performed document analysis. Nobody performed functional testing.

**A COST auditor.** No auditor evaluated whether the pipeline's complexity is proportionate to its value. Is 69 PA questions better than 50? Is 37 gates better than 25? Is the 5-file gate split worth the navigation cost? These are optimization questions that a cost auditor would ask.

**A REGRESSION auditor.** No auditor compared v3 against the Gas Town v1 execution to ask: "Would v3 have produced a BETTER result than v1 on the same content?" This is the most basic validation question and it went unasked.

### 7.2 Scope Issues

**Too narrow on anti-loss:** Auditors 1 and 6 both covered anti-loss mechanisms, creating overlap. Auditor 1 verified existence; Auditor 6 traced propagation. These could have been one auditor with a combined existence+propagation mandate, freeing a slot for the execution auditor.

**Too narrow on gates:** Auditor 2 evaluated gate meaningfulness and calibration but not gate executability. The gate quality audit produced excellent analysis of thresholds and false-positive risks, but it is an ANALYTICAL audit, not a FUNCTIONAL audit.

**Too narrow on PA questions:** Auditor 3 verified rewrites, additions, removals, and load balance. But the audit did not assess whether the WEAVER can reliably synthesize 69 answers from 9 auditors into a coherent verdict. The weaver protocol (pa-weaver.md) specifies 7 required outputs (Section 1), but nobody verified that an Opus agent can produce all 7 outputs in a single pass without degradation.

### 7.3 What the Methodology Got Right

The fresh-eyes auditor (Auditor 7) was the right call. Zero-context evaluation exposed real usability issues that context-loaded auditors would miss. The 73/165/3,600 confusion, the council-verdict.md ambiguity, and the missing cheat sheet are findings that only a fresh-eyes reviewer would surface.

The anti-loss propagation auditor (Auditor 6) was innovative. End-to-end junction tracing is a rigorous methodology that caught the conviction-statement-as-separate-file gap (35% loss probability) that no other auditor identified. This approach should be used more broadly.

---

## Section 8: The 5 Things I Would Change Before Production

### Change 1: RUN THE PIPELINE ONCE (Priority: BLOCKING)

Execute Pipeline v3 on Gas Town content (or equivalent test content) with full execution tracker population. Compare the result against the Gas Town v1 execution. This is not optional. No amount of specification analysis substitutes for a single execution. Time: 2-3 hours. Cost: $50-100.

**What this would test:** orchestrator context-window fidelity, BV gate behavior on real brief text, gate-runner-core.js against real HTML, 9-auditor parallel execution, weaver synthesis quality.

### Change 2: ADD A BV GATE FOR DISPOSITION CONTENT DENSITY (Priority: HIGH)

Create BV-05 that checks whether each disposition (D-01 through D-09) in the Tier 4 section contains at least one CSS property:value pair. This closes the "Compliant But Flat" failure scenario described in Section 5.

**Implementation:** In `runBriefVerification(briefText)`:
```javascript
// BV-05: Disposition Technique Density
const dispositionPattern = /D-0[1-9][^#]*/gs;
const dispositions = [...briefText.matchAll(dispositionPattern)];
const cssPropertyPattern = /[a-z-]+:\s*[\d.]+(?:px|em|rem|%|ch)/g;
const emptyDispositions = dispositions.filter(d =>
  !cssPropertyPattern.test(d[0])
);
```

**Effort:** 30 minutes. **Impact:** Directly prevents the most plausible failure scenario.

### Change 3: ADD VIEWPORT NAMESPACING TO GATE RESULTS (Priority: HIGH)

The gate-quality auditor (Section 6D, Finding 2) and the propagation auditor (Trace 2 notes) both identified the responsive re-run deduplication problem. When runGateRunner(page) runs at both 1440px and 768px, duplicate gate IDs are produced. GR-48 deduplicates via Set, but GR-49 would flag the duplicates. This is a conflict between two meta-gates that will cause one to produce a false alarm on every run.

**Fix:** Add a `viewport` field to every result object. Update GR-49 to allow duplicate IDs with different viewports. Update GR-48 to verify required gates exist at BOTH viewports.

**Effort:** 45 minutes. **Impact:** Prevents a false alarm from GR-49 on every single pipeline run.

### Change 4: MOVE PA-QUESTIONS.MD SECTION 4 TO PA-WEAVER.MD (Priority: HIGH)

The PA-quality auditor (Part 9, Section 9.2) flagged this as CRITICAL: pa-questions.md Section 4 contains quality tier definitions (FLOOR/MIDDLE/CEILING/FLAGSHIP with PA-05 score ranges) that would anchor auditor scoring if visible. The section now has a WARNING label (lines 312-316) and instructions for the orchestrator to send only Sections 1-3 + Appendices to auditors. But this relies on the orchestrator reading and following the warning -- the same orchestrator whose compliance is "the weakest link" per the propagation auditor.

**Fix:** Move Section 4 entirely to pa-weaver.md (which already contains identical information in Sections 2.4 and 9). Delete Section 4 from pa-questions.md. This structurally prevents the leak rather than relying on orchestrator compliance.

**Effort:** 15 minutes. **Impact:** Eliminates the single most important PA integrity risk.

### Change 5: ADD GATE-RUNNER TEST FIXTURES (Priority: MEDIUM-HIGH)

Create two HTML test fixtures:
- `test-pass.html`: A minimal page that passes all 17 REQUIRED gates
- `test-fail.html`: A minimal page that fails specific identity and perception gates

Add a test runner script that executes `runGateRunner(page)` against both fixtures and verifies expected PASS/FAIL results.

**Effort:** 2-3 hours. **Impact:** Provides the first functional verification of the pipeline's most critical automated component. Every gate threshold, every selector, every false-positive risk identified in the gate-quality audit can be validated in one test run.

---

## Appendix: Audit-by-Audit Scoring

For reference, my adversarial assessment of each audit's reliability:

| Audit | Auditor's Verdict | My Confidence | Key Concern |
|-------|-------------------|---------------|-------------|
| Anti-loss mechanisms | 12 confirmed, 6 upgraded, 6 partial | 85% | M-05 and M-14 UPGRADED ratings are overgenerous; M-06 STILL PARTIAL may be too harsh |
| Gate quality | GOOD WITH RESERVATIONS (76% well-calibrated) | 90% | Strongest audit. Thorough threshold analysis. Did not catch GR-51 selector fragility. |
| PA question quality | 92/100 (1 CRITICAL fix) | 88% | Correct identification of Section 4 leak risk. D overload finding is valid. Question quantity not challenged. |
| Unfixed items | 6 FULLY FIXED, 3 PARTIAL, 1 NOT FIXED | 80% | "FULLY FIXED" conflates "spec complete" with "verified working." Item 3 overstated. |
| Logging/retrospection | 80% coverage, 2 BLOCKING gaps | 92% | Most honest audit. Correctly identifies "why did the builder produce X?" as unanswerable. Builder prompt logging recommendation is the highest-leverage non-execution fix. |
| Anti-loss propagation | 4/5 propagate, M-16 at 35% loss | 90% | Strong methodology. Orchestrator compliance finding is the most important systemic insight across all 7 audits. |
| Fresh-eyes | 3.5/5 | 75% | Red Flag 1 is a false alarm (undermines credibility). Other findings are valid. Over-engineering critique is partially wrong (dual-route redundancy is intentional). |

---

*Adversarial review complete. 7 audit reports challenged across 8 sections. 3 overrated items, 2 underrated items, 4 contradictions, 5 blind spots, 1 failure scenario, and 5 production-blocking changes identified. The pipeline is well-specified but untested. Run it before calling it production-ready.*
