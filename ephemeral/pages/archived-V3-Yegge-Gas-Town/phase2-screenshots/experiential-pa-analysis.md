# Experiential PA Framing Analysis
## The Gorilla Experiment in Pipeline Verification

**Analyst:** Opus agent (experiential-analyst)
**Date:** 2026-02-24
**Inputs:** 9 PA auditor reports (A-I), integrative report, weaver diagnostic, pa-questions.md, pa-deployment.md, AUDIT-SYNTHESIS.md
**Subject:** The fundamental framing flaw in the pipeline's Perceptual Audit protocol

---

## 0. THE THESIS

The PA asks auditors to EVALUATE the page. It never asks them to USE the page. Every PA question is analytical ("Is any text uncomfortable to read?" -- evaluate a visual property). None are experiential ("Read all the text you see" -- attempt to use the page).

This is the Gorilla Experiment (Simons & Chabris, 1999). The 69 questions are the counting task. The illegible chart is the gorilla. The questions do not just guide auditors -- they create ATTENTIONAL TUNNELS. When Auditor D receives 11 questions about flow and pacing, the chart becomes a "rhythm beat" and a "dark block between light sections." It never becomes TEXT THAT SOMEONE NEEDS TO READ.

---

## 1. AUDITOR-BY-AUDITOR CHART LEGIBILITY ANALYSIS

### Auditor A (Impression + Emotion) -- 9 questions

**Questions assigned:** PA-01, PA-03, PA-04, PA-05, PA-45, PA-65, PA-67, PA-72, PA-76

**Did they mention chart legibility?** YES, in passing, once.

**Exact text:** PA-05d POLISHED assessment: "Even the visible portions have minor issues: the hierarchy chart text is barely readable at the image size used, and the role cards get cut off at the viewport edge in some captures."

**How questions created the attentional tunnel:** Auditor A's questions are about IMPRESSION (PA-01: what bothers you first), EMOTION (PA-45: show someone an example of good design), and GESTALT (PA-03: one designer or three). These questions frame the page as an aesthetic object to be FELT, not a document to be READ. The chart registers as a visual impression ("a large dark-background chart shows a horizontal bar diagram with a red 'YOU ARE LEARNING THIS' arrow") rather than as text to be consumed. The chart legibility appears only in a sub-clause of a sub-assessment, never as a primary finding.

**What they SAW the chart AS:** A visual rhythm device. An "anchoring" element. A "dark-background diagram." The chart is described 4 times across the report; all 4 descriptions are about its visual role in the page's composition, not about whether its content is readable.

---

### Auditor B (Readability + Typography) -- 8 questions

**Questions assigned:** PA-02, PA-06, PA-08, PA-29, PA-55, PA-56, PA-70, PA-77

**Did they mention chart legibility?** YES -- thoroughly. This is the ONE auditor who caught it.

**Exact text (PA-02):** "The chart labels inside the dark visualization (1440-scroll-00, the complexity ladder chart). The text within the chart bars is small and low-contrast (light gray on dark gray background). You must lean in slightly to read the level labels. The 'YOU ARE LEARNING THIS' callout is clear, but the individual bar labels are not. **Worst spot: the bar labels inside the complexity ladder chart at all viewports.**"

**Exact text (PA-08):** "The complexity ladder chart bar labels: Within the dark visualization background, the level labels and descriptive text on each bar are small, low-contrast, and partially obscured. At 1440px you can make out rough shapes but not read individual words without effort. At smaller viewports it gets worse."

**How questions ENABLED detection:** PA-02 ("Is any text uncomfortable to read? Point to the worst spot.") is the ONLY question in the entire 69-question battery that directly asks an auditor to evaluate text readability. PA-08 ("Is there any text you have to lean in or squint to read?") is its companion. These two questions, and ONLY these two questions, create the attentional frame "find text that is hard to read." Auditor B caught the chart legibility issue BECAUSE their questions directed them to look for exactly this class of failure.

**What they SAW the chart AS:** TEXT. Specifically, text with a readability problem. This is the ONLY auditor who saw the chart as a thing containing words that someone would need to read.

---

### Auditor C (Spatial + Proportion) -- 11 questions

**Questions assigned:** PA-09, PA-11, PA-30, PA-31, PA-32, PA-33, PA-50, PA-51, PA-53, PA-64, PA-66

**Did they mention chart legibility?** NO. Not once. Zero mentions.

**How questions created the attentional tunnel:** Every one of Auditor C's 11 questions is about SPACE: dead space (PA-09), margins (PA-11), layout width (PA-30, PA-53), visual weight distribution (PA-32), emptiness quality (PA-33, PA-66). The chart registers in C's report as: "the beginning of a 'Settlement Map' table of contents with a red left-border accent" and "Content ~85% of viewport. Well-filled." The chart is a SPATIAL ELEMENT -- something that fills space or doesn't. Whether its text is readable is entirely outside C's attentional frame.

**What they SAW the chart AS:** A spatial object with visual weight. A thing that fills approximately 85% of a viewport. A "dark diagram" in the rhythm of cream-then-dark-then-cream.

---

### Auditor D (Flow + Pacing) -- 11 questions

**Questions assigned:** PA-12, PA-13, PA-34, PA-35, PA-36, PA-52, PA-62, PA-69, PA-71, PA-74, PA-75

**Did they mention chart legibility?** NO. Not once. Zero mentions.

**How questions created the attentional tunnel:** This is the most dramatic example of attentional tunneling. Auditor D has 11 questions, all about FLOW: smooth section-to-section transitions (PA-12), visual ending (PA-13), designed transitions (PA-34), interest peaks and valleys (PA-35), dramatic visual moments (PA-36), transition intensity (PA-62, PA-69), transition coherence (PA-71, PA-74). The chart appears in D's report as: "dark complexity chart (horizontal bar chart with 'YOU ARE LEARNING THIS' callout in red)" -- a TRANSITION ELEMENT between the header zone and the content zone. D describes the chart as creating "a light-dark-light rhythm" and notes it as the first of three "dramatic visual moments." The chart is a BEAT in a rhythm. Whether its text is readable would require breaking out of the flow/pacing frame to ask "can I read this?" -- which none of D's 11 questions invite.

**What they SAW the chart AS:** A rhythmic beat. A dramatic visual moment. A "dark-background diagram" that creates "an immediate light-dark-light rhythm." The chart is purely a pacing device in D's perceptual frame.

---

### Auditor E (Grid + Layout) -- 6 questions

**Questions assigned:** PA-14, PA-15, PA-37, PA-38, PA-39, PA-63

**Did they mention chart legibility?** YES, in passing, once.

**Exact text:** "The horizontal chart loses legibility (labels overlap or become too small)." And later: "The chart diagram fills more of the horizontal space." And: "Chart fits width but labels are very hard to read."

**How questions created the attentional tunnel:** E's questions are about GRID: columns breathing (PA-14), alignment positions (PA-15), information density (PA-37), reading order (PA-38), header-to-content ratio (PA-39). The chart registers as a LAYOUT ELEMENT -- something with a width, a position, and a container. E notices the legibility issue but frames it as a grid observation ("labels overlap or become too small" at 768px, i.e., a responsive layout problem) rather than as a TEXT READABILITY crisis. E mentions chart legibility 3 times, but each time it is a subordinate observation within a grid analysis, not a flagged finding. It never appears in E's summary.

**What they SAW the chart AS:** A grid element with a container. Something that "fills more of the horizontal space." A width-consuming object within a column.

---

### Auditor F (Consistency + Rhythm) -- 6 questions

**Questions assigned:** PA-16, PA-17, PA-40, PA-41, PA-60, PA-61

**Did they mention chart legibility?** NO. Not once. Zero mentions.

**How questions created the attentional tunnel:** F's questions are about PATTERNS: identical elements (PA-16), visual rhythm (PA-17), spacing consistency (PA-40), pattern repetition (PA-41), design moments (PA-60), multi-voice composition (PA-61). The chart appears as: "a dark-background diagram (horizontal bar chart with 'YOU ARE LEARNING THIS' callout in red)" -- an element in a PATTERN of dark-on-light alternation. The chart is the first instance of a "dark container pattern" that F identifies as repeating at least 2 times visibly and 4-5 times in the full page. Whether the chart's text is readable is not a rhythm question, a consistency question, or a pattern question. It is invisible within F's frame.

**What they SAW the chart AS:** The first instance of a repeating "dark container pattern." A rhythm element that creates "audible-feeling pulse" through light/dark alternation. A pattern to be counted and compared, not a text to be read.

---

### Auditor G (Metaphor + Ideology) -- 7 questions

**Questions assigned:** PA-18, PA-19, PA-20, PA-42, PA-43, PA-44, PA-68

**Did they mention chart legibility?** YES -- one substantial mention, but framed as visual COST, not readability.

**Exact text (PA-42):** "The diagram is extremely small relative to its dark container. It occupies maybe 40% of the dark block's area, leaving large empty margins. The information density is low for the visual weight of the container."

**Exact text (PA-43):** "The Complexity Ladder bar chart: The full-width dark-background container for what is essentially a horizontal bar chart with 8 items could be half the height. The chart's visual footprint takes an entire viewport-height worth of space, but the information (8 levels, relative complexity bars, 'YOU ARE LEARNING THIS' marker) could be communicated in a compact card."

**How questions created the attentional tunnel:** G's questions are about METAPHOR: visual personality (PA-20), metaphor-reality mismatch (PA-42), visual cost (PA-43), metaphor persistence (PA-44, PA-68). PA-42 specifically asks "Any section where you understand WHY it looks this way but it still looks WRONG?" -- which DOES point G toward the chart. But the framing is about metaphor cost-benefit, not text legibility. G sees the chart as a metaphor that is TOO EXPENSIVE for what it communicates, not as text that is TOO SMALL to read. The diagnosis is "the dark background gives it the visual authority of a key diagram, but the content within does not justify that authority." This is a metaphor critique, not a readability critique.

**What they SAW the chart AS:** A metaphor with an unfavorable cost/benefit ratio. An element whose "visual footprint" exceeds its "information payoff." A container-vs-content proportion problem, not a readable-vs-illegible text problem.

---

### Auditor H (Responsiveness) -- 5 questions

**Questions assigned:** PA-22, PA-23, PA-46, PA-47, PA-73

**Did they mention chart legibility?** YES, briefly.

**Exact text:** "The complexity ladder chart has low-contrast labels that undermine the otherwise sharp typographic quality." (PA-22). And PA-73: "The complexity ladder chart is a raster image inside a dark container. If this image lacks alt text, a screen reader user skips the entire visual context."

**How questions created the attentional tunnel:** H's questions are about RESPONSIVENESS: viewport-specific experience (PA-22, PA-23), reorganization logic (PA-46), accessibility (PA-73). The chart registers as a RESPONSIVE ELEMENT -- something that scales (or doesn't) across viewports. H notes chart legibility as a viewport-scaling concern ("low-contrast labels") and as an accessibility concern ("raster image... lacks alt text"), but in both cases the chart is a VIEWPORT ADAPTATION problem, not a PRIMARY readability failure. H's PA-73 answer about the chart focuses on screen-reader access, not on whether sighted users can read the labels.

**What they SAW the chart AS:** A viewport-scaling challenge. An accessibility risk. An image that may lack alt text. A responsive element that "loses legibility" at narrow widths.

---

### Auditor I (Cross-Page + Adversarial) -- 6 questions

**Questions assigned:** PA-24, PA-25, PA-26, PA-27, PA-28, PA-48

**Did they mention chart legibility?** YES, once.

**Exact text (PA-27):** "The bar chart figure feels like a placeholder. The labels are barely readable at this size. A cleaner information hierarchy chart (perhaps with the levels labeled directly, not in a bar-chart format) would serve better."

**How questions created the attentional tunnel:** I's questions are about SYSTEM JUDGMENT: design system coherence (PA-24, PA-25), intentional wrongness (PA-26), design-from-scratch evaluation (PA-27), fragility analysis (PA-28), competitive ranking (PA-48). PA-27 ("Designing from scratch, would you design it this way?") is the question that surfaces the chart issue -- it invites I to imagine alternatives. But the framing is "would you design it differently" (a design choice question), not "can users read this" (a usability question). I calls the chart a "placeholder" and suggests a "cleaner information hierarchy chart" -- a design improvement, not a fix for a broken element.

**What they SAW the chart AS:** A design choice that could be improved. A "placeholder" that doesn't serve the page's information goals. Something to redesign, not something that is broken.

---

## 2. SUMMARY TABLE: CHART DETECTION ACROSS 9 AUDITORS

| Auditor | Questions | Chart Mention? | Severity Assigned | What They Saw It As |
|---------|-----------|---------------|-------------------|---------------------|
| **A** | PA-01,03,04,05,45,65,67,72,76 | Passing mention in PA-05d | MINOR (sub-clause) | Visual rhythm device |
| **B** | PA-02,06,08,29,55,56,70,77 | **THOROUGH -- "Worst spot"** | **SIGNIFICANT** | **TEXT with readability problem** |
| **C** | PA-09,11,30-33,50,51,53,64,66 | NO | -- | Spatial weight element |
| **D** | PA-12,13,34-36,52,62,69,71,74,75 | NO | -- | Rhythmic beat / pacing device |
| **E** | PA-14,15,37-39,63 | Passing mention (3x) | MINOR (responsive) | Grid layout element |
| **F** | PA-16,17,40,41,60,61 | NO | -- | Pattern instance (dark container #1) |
| **G** | PA-18-20,42-44,68 | Substantial (metaphor cost) | MODERATE (cost/benefit) | Metaphor with high visual cost |
| **H** | PA-22,23,46,47,73 | Brief (responsive + a11y) | MODERATE (accessibility) | Responsive/accessibility object |
| **I** | PA-24-28,48 | Once (PA-27) | MODERATE (design choice) | Placeholder to redesign |

**Detection pattern:**
- **Thorough detection:** 1/9 (Auditor B -- the ONLY one with text readability questions)
- **Passing/incidental mention:** 4/9 (A, E, H, I -- mentioned but not flagged as primary)
- **Reframed as different issue:** 1/9 (G -- metaphor cost, not readability)
- **Zero mention:** 3/9 (C, D, F -- entirely invisible in their attentional frame)

---

## 3. THE WEAVER'S HANDLING

The Weaver received all 9 reports plus the integrative report. In the Weaver's Top-5 Fixes, chart legibility appears as **Fix #5** (last priority):

> "Fix 5: Chart Label Legibility. TYPE: MECHANICAL. Auditors: B (PA-02, PA-08), G (PA-42), H (PA-73), I (PA-27). Evidence: Complexity ladder bar chart has low-contrast labels on dark background. Multiple auditors flagged squint-worthy text."

The Weaver classifies this as MECHANICAL (change a CSS value) and places it below font stack cleanup, callout tints, border widths, and HTML color. The fix description: "Increase text size and contrast within the ASCII/chart diagram containers. May require adjusting the chart's text elements or pre-block styling."

**What this reveals about the weaver's frame:** The Weaver is operating in a FIX-CLASSIFICATION frame: mechanical vs structural vs compositional. In this frame, illegible text is a CSS value change (increase contrast). It is not a usability crisis ("users cannot read the most prominent orientation device on the page"). The Weaver's frame converts a user-experience failure into a code maintenance task.

The fact that Fix #5 is LAST (below rounding border widths from 0.667px to 1px) demonstrates priority inversion caused by the analytical frame: technically measurable defects (non-integer borders) outrank perceptually obvious failures (can't read the chart) because the analytical frame has no mechanism to weigh USER IMPACT.

---

## 4. THE INTEGRATIVE AUDITOR

The Integrative Auditor, who has NO assigned questions, mentions chart legibility once:

> "The Complexity Ladder diagram is hard to read (all viewports). The dark-background bar chart with the 'YOU ARE LEARNING THIS' callout has labels that are barely legible at any viewport width."

This appears in the "What Feels Wrong" section, listed AFTER the catastrophic whitespace void, section spacing inconsistency, orphaned "8" numeral, and content density collapse. It is the sixth item in a list of six concerns.

**The integrative auditor has a MILD detection advantage** because they have no questions constraining their attention. But the integrative auditor's gestalt frame ("what FEELS right, what FEELS wrong") still produces an EVALUATIVE reaction, not a USAGE-BASED one. The chart "is hard to read" is an observation ABOUT the chart, not an experience OF trying to read it. The integrative auditor never says "I tried to read the complexity levels and couldn't make them out." They say "labels are barely legible" -- a judgment about a visual property.

---

## 5. COUNTERFACTUAL: THE EXPERIENTIAL PASS

### 5.1 The Proposed Instruction

> **EXPERIENTIAL PASS (before any PA questions):**
> "Look at these screenshots as a reader, not as an evaluator. Scroll through the page and try to READ every piece of text you see. Report:
> 1. Every place where you could not read the text.
> 2. Every place where you had to slow down or squint.
> 3. Every place where you skipped text because it was too small, too low-contrast, or too dense.
> Do this BEFORE answering any assigned questions."

### 5.2 Probability Model: What Percentage of Auditors Would Catch Chart Legibility?

**Under the current analytical framing (observed):**
- Thorough detection: 1/9 = 11%
- Any mention at all: 5/9 = 56%
- Primary finding: 1/9 = 11%
- Flagged at appropriate severity: 0/9 = 0% (even Auditor B listed it as "SIGNIFICANT" not "BLOCKING")

**Under an experiential pass instruction (projected):**

The experiential instruction directly inverts the attentional frame. Instead of "evaluate this visual property," the instruction is "attempt to use this content." Every auditor would need to look at the chart and attempt to read its text.

- **Auditors B, E, H, I** (who already noticed chart legibility incidentally): Near-certain to catch it (>95%). The experiential frame would elevate their incidental observations to primary findings.
- **Auditors A, G** (who noticed the chart but reframed it): Very likely to catch it (>85%). When forced to TRY to read the chart labels, they would encounter the failure directly instead of analyzing it from a distance.
- **Auditors C, D, F** (who never mentioned the chart): Likely to catch it (>70%). The experiential instruction says "try to READ every piece of text." The chart labels are text. C, D, and F would encounter them.

**Projected detection rate under experiential pass: 7-9/9 (78-100%).**

The gap between 11% (current) and 78-100% (projected) is not a small improvement. It is a categorical shift from "incidental detection by one specialist" to "routine detection by most auditors."

### 5.3 Why the Experiential Frame Works

The analytical frame asks: "Is this text uncomfortable to read?" This is a JUDGMENT about a visual property. The auditor scans for visual properties that match the concept "uncomfortable." The chart may or may not match this concept depending on the auditor's personal threshold.

The experiential frame asks: "Try to read this text." This is an ATTEMPT to use the content. The auditor encounters the chart labels and either can or cannot read them. There is no ambiguity. The failure is experienced, not evaluated.

The difference:
- **Analytical:** "The chart labels appear to be low-contrast." (Describes a property)
- **Experiential:** "I tried to read the level names on the complexity ladder and couldn't make out 'Advanced Vibe Coding' from 'Expert Vibe Coding' because they're the same color as the bar backgrounds." (Reports a failure)

The experiential frame converts perceptual evaluation into task failure. Task failures are salient. Perceptual evaluations are negotiable.

---

## 6. OTHER FAILURES THE ANALYTICAL-ONLY FRAMING MAY HAVE CAUSED

### 6.1 Density Label Faintness

The Settlement Map's density labels ("moderate," "dense," "reference") are extremely faint. Auditor B caught this in PA-02, calling them "rendered in a very light gray/tan that approaches sub-perceptual against the cream background." The integrative auditor also noted "density label faintness." But these labels are THE SETTLEMENT MAP'S DIFFERENTIATING FEATURE -- the thing that makes it more than an ordinary table of contents.

Under an experiential pass ("try to read all the text"), every auditor would encounter the density labels and report whether they could read them without effort. Under the analytical framing, only Auditor B (who has the text readability questions) consistently catches faint text.

### 6.2 Dark-Background Diagram Readability (Sections 03-07)

The corrected screenshots (visible in the Weaver's report) show multiple dark-background diagrams throughout the page: the role hierarchy diagram (corrected-scroll-05), the three-layer memory model (corrected-scroll-08), code blocks (corrected-scroll-13). The original PA auditors could not assess sections beyond scroll-04 due to the DPR screenshot bug. But in principle, the same analytical tunnel would apply: each diagram would be categorized by its assigned auditor's frame (spatial weight, rhythm beat, metaphor cost, pattern instance) rather than assessed for whether its text content is readable.

### 6.3 The Meta-Failure: No Auditor Tried to USE the Page

Not a single auditor in any of the 9 reports says anything like:
- "I tried to navigate using the Settlement Map and..."
- "I tried to read the Complexity Ladder to figure out where Level 7 falls and..."
- "I tried to follow the 8 Roles hierarchy from the diagram and..."
- "I tried to use the comparison table to understand the paradigm shift and..."

Every observation is ABOUT the page, not FROM the page. The auditors are critics standing in front of a painting, not readers sitting with a document. This is not a failure of the auditors -- it is a direct consequence of the protocol, which gives them 69 evaluative questions and zero experiential tasks.

---

## 7. THE SPECIFICATION CHANGE

### 7.1 What to Add

**Add to pa-deployment.md, Section 2.4 (Auditor Screenshot-Reading Protocol), as a new step 2.5 inserted BEFORE step 3 (question answering):**

```markdown
### 2.5 Experiential Pass (MANDATORY -- Before Questions)

**BEFORE answering any assigned questions, each auditor performs an experiential pass:**

Instruction to auditor:

> Look at these screenshots as a READER, not as an evaluator. Scroll through
> the page and try to READ every piece of text you see -- headings, body text,
> labels, chart annotations, captions, metadata, footer text, everything.
>
> Report THREE things:
> 1. **Illegible text:** Every place where you could not read the text at all.
>    Quote what you THINK it says and note that you're guessing.
> 2. **Effortful text:** Every place where you had to slow down, squint, or
>    look twice. Note what made it difficult (size, contrast, overlap, density).
> 3. **Skipped text:** Every place where you chose to skip text because it
>    looked too small, too dense, or too low-contrast to bother reading.
>
> Do this BEFORE reading your assigned questions. Your experiential pass
> observations are INDEPENDENT of your question answers. Report them in a
> separate section at the TOP of your audit report, labeled
> "## 0. Experiential Pass."
>
> NOTE: This is about TEXT READABILITY, not visual evaluation. Do not describe
> how elements LOOK. Report what you could and could not READ.

**Rationale:** PA questions create attentional tunnels that make certain failure
classes invisible. An auditor focused on spatial proportion (PA-09, PA-11) will
see a dark chart as a spatial element, not as text to read. An auditor focused
on flow (PA-12, PA-34) will see it as a rhythmic beat. Only auditors with
explicit text-readability questions (PA-02, PA-08) will attempt to read chart
labels. The experiential pass ensures ALL 9 auditors attempt to USE the page
before EVALUATING it. Historical evidence: Gas Town V3 PA detected chart
illegibility in 1/9 auditors under analytical-only framing.
```

### 7.2 What to Add to pa-questions.md

**Add to Appendix B (Quantitative Guardrails), a new subsection:**

```markdown
### Experiential Detection Guardrail

| Metric | Threshold | Meaning |
|--------|-----------|---------|
| Experiential Pass findings across 9 auditors | >= 3/9 flag same text element | CONFIRMED illegibility (priority escalation) |
| Experiential Pass findings | 1/9 flags a text element | POSSIBLE issue (Weaver investigates) |
| Experiential Pass findings | 0/9 across all auditors | All text readable at default viewing distance |
```

### 7.3 Why This Location

The experiential pass goes in pa-deployment.md (not pa-questions.md) because it is a PROTOCOL step, not a question. It modifies HOW auditors begin their work, not WHAT they answer. Placing it in the screenshot-reading protocol section means the orchestrator encounters it during setup, not during question assignment.

The threshold table goes in pa-questions.md Appendix B because it gives the Weaver a calibration framework for prioritizing experiential-pass findings across auditors.

---

## 8. SECOND-ORDER EFFECTS

### 8.1 Does an experiential pass change auditor behavior on analytical questions?

**YES -- and the effect is POSITIVE.**

When an auditor performs an experiential pass first, they enter their analytical questions with a GROUNDED mental model of the page. They have already attempted to read the text, already encountered the chart labels, already noticed the faint density labels. This grounding has two effects:

**Effect 1: Anchoring to experienced failures.** An auditor who struggled to read chart labels during the experiential pass will naturally reference this when answering analytical questions. Auditor D, answering PA-36 ("Is there a dramatic visual moment?"), might now say "the complexity ladder chart is dramatic visually but its text is illegible -- the visual drama doesn't translate to usable information." The experiential observation enriches the analytical answer.

**Effect 2: Breaking the pure-analytical frame.** Without the experiential pass, auditors are pure critics: they describe properties of the page from a distance. With the experiential pass, they are critics who have also been readers. This dual frame produces richer observations. An auditor who has tried and failed to read chart labels will see the chart differently than an auditor who has only evaluated its visual properties.

**Potential negative effect: Priming.** If the experiential pass surfaces a dramatic failure (e.g., illegible chart), auditors might over-weight that failure in their analytical answers, potentially drowning out other findings. Mitigation: the experiential pass is reported in a SEPARATE section ("Section 0") before the question answers. The structural separation encourages auditors to treat experiential and analytical observations as distinct.

### 8.2 Does the experiential pass add significant time?

**NO.** Auditors already review all screenshots sequentially during their cold-look and scroll-through. The experiential pass adds one instruction ("try to read the text as you scroll") to a process they already perform. Estimated time overhead: 2-5 minutes per auditor. For a 9-auditor PA that typically runs 30-60 minutes, this is a 3-8% overhead for a categorical improvement in failure detection.

### 8.3 Does the experiential pass violate the fresh-eyes principle?

**NO -- it strengthens it.** The fresh-eyes principle says auditors must not receive build context, mechanism counts, or prior results. The experiential pass gives auditors no additional context -- it gives them a TASK. "Try to read the text" is the ultimate fresh-eyes instruction: it asks the auditor to engage with the page as a naive reader, which is the fresh-eyes principle's intent.

---

## 9. CROSS-REFERENCE: AUDIT-SYNTHESIS.md

The AUDIT-SYNTHESIS.md metacognitive warning maps perfectly:

> **"Specification correctness != generative adequacy."** (AUDIT-SYNTHESIS.md, Section 5)

Rewritten for the PA: **"Analytical evaluation != experiential evaluation."**

The metacognitive auditor identified 7 shared assumptions across all pipeline auditors. Assumption #1: "Specification quality determines output quality." The PA equivalent: "Question quality determines audit quality." Both assume that better SPECIFICATIONS (questions, gates, rules) lead to better DETECTION (quality assessment, failure finding). Both miss the same thing: the specification creates an attentional frame that is simultaneously enabling (focuses analysis) and blinding (excludes usage).

The metacognitive auditor's central insight -- "The audit evaluates the pipeline as a specification system while the pipeline's actual challenge is as a generative system" -- has a PA parallel: **"The PA evaluates the page as an aesthetic object while the page's actual challenge is as a usable document."** Both the pipeline and the PA are ANALYTICAL systems trying to ensure EXPERIENTIAL quality. Neither has a mechanism for experiencing the thing it evaluates.

The metacognitive auditor also identified **validation circularity**: "the pipeline defines quality, measures quality using its own definitions, and declares success based on those measurements." The PA has the same circularity: the 69 questions define what "perceptual quality" means, auditors answer those questions, the Weaver synthesizes those answers into a verdict. If the questions miss a failure class (like "can users read the text?"), the verdict cannot detect it. The experiential pass breaks this loop by adding a question-independent observation channel.

---

## 10. THE DEEPER PATTERN

### 10.1 The Pipeline Has Never USED Its Own Output

The PA evaluates the page analytically. The gate runner evaluates the page programmatically. Neither USES the page.

What would "using" the page look like at the pipeline level?

**A navigation test:** Can an agent navigate from the Settlement Map to Section 07 and find the `gt` command syntax? If the Settlement Map links are broken, no gate catches it. If the Section 07 content is readable but requires 10 minutes of scrolling, no PA question catches the TIME dimension.

**A comprehension test:** After reading the page, can an agent answer questions ABOUT the content? "What are the 8 roles in Gas Town?" "What is the difference between Traditional Dev and Gas Town approaches?" If the page is beautifully designed but its content is unintelligible, neither gates nor PA questions detect this.

**A reference-use test:** Can an agent use the page as a REFERENCE -- looking up a specific fact, finding a specific command, confirming a specific comparison? Reference documents succeed or fail not on their first-read experience but on their lookup experience. The PA exclusively evaluates first-read experience.

### 10.2 The Evaluation Bias

Both the pipeline and the PA suffer from the same bias: they were designed by people thinking about HOW TO EVALUATE quality, not HOW TO USE the artifact. This is natural -- evaluation is the pipeline's job. But the evaluation frame systematically excludes usage-based failure modes:

| Failure Class | Caught by Gates? | Caught by PA Questions? | Caught by Usage? |
|---------------|-------------------|------------------------|-------------------|
| CSS violations | YES | NO | NO |
| Visual proportion | NO | YES | NO |
| Text illegibility | NO | PARTIAL (1/9) | **YES (all)** |
| Navigation breakage | NO | NO | **YES** |
| Comprehension failure | NO | NO | **YES** |
| Reference-lookup failure | NO | NO | **YES** |

The experiential pass is the minimum viable intervention: it adds usage-based observation to the PA without restructuring the entire protocol. But the deeper insight is that the pipeline's verification stack has a MISSING LAYER -- the layer where someone tries to USE the thing.

### 10.3 Parallel to Software Testing

In software testing, there is a well-known distinction between:
- **Unit tests** (gates): Do individual components meet specifications?
- **Integration tests** (PA questions): Do components work together visually?
- **User acceptance tests** (MISSING): Can a user accomplish their task?

The pipeline has unit tests (37+ gates) and integration tests (69 PA questions) but no user acceptance tests. The experiential pass is the closest thing to a UAT that can be added without restructuring the pipeline.

---

## 11. CONCLUSION

### What Happened

The Complexity Ladder chart on the Gas Town page has illegible labels. It is the most prominent orientation device on the page (occupying nearly a full viewport) and the first content a reader encounters after the header. Its text is cream/light gray on dark bars at small size -- objectively illegible at normal reading distance.

In a 9-auditor, 69-question Mode 4 PA:
- 1 auditor (B) caught it thoroughly, because PA-02 and PA-08 specifically ask about text readability
- 4 auditors (A, E, H, I) mentioned it in passing, as a subordinate observation within their analytical frame
- 1 auditor (G) reframed it as a metaphor cost-benefit problem
- 3 auditors (C, D, F) never mentioned it at all
- The Weaver classified it as Fix #5 (lowest priority, below border-width rounding)

### Why It Happened

The 69 PA questions are all analytical: they ask auditors to EVALUATE visual properties. None ask auditors to USE the page. Each question creates an attentional tunnel that determines what the auditor sees the chart AS -- a rhythm device (D), a spatial weight (C), a pattern instance (F), a metaphor vehicle (G). Only PA-02 and PA-08, assigned exclusively to Auditor B, create the frame "text that needs to be read." The questions do not just guide attention; they constitute the auditor's perceptual reality.

### What to Do About It

Add a mandatory Experiential Pass (Section 7 above) to pa-deployment.md. Before answering any questions, each auditor scrolls through the page and attempts to read all text, reporting illegible, effortful, and skipped text in a separate section.

Projected impact: chart legibility detection rate rises from 11% (1/9) to 78-100% (7-9/9). Time overhead: 2-5 minutes per auditor. No conflict with fresh-eyes principle. Positive second-order effects on analytical question quality.

### The Deeper Lesson

The PA protocol is an analytical evaluation system trying to ensure experiential quality. This is like testing a car by measuring its paint quality, door alignment, and engine specifications, but never driving it. The experiential pass is the equivalent of "start the car and drive it around the block." It is not a replacement for the analytical tests -- it is the MISSING layer that catches the failures the analytical tests cannot see.

**"Specification correctness != generative adequacy"** is the pipeline's metacognitive warning.
**"Analytical evaluation != experiential evaluation"** is the PA's metacognitive warning.

Both point to the same structural blind spot: systems designed to EVALUATE quality are not the same as systems designed to EXPERIENCE quality. The gorilla walks through the frame, and the counters keep counting.

---

*End of Experiential PA Framing Analysis. 9 auditor reports analyzed, 69 questions mapped, 1 specification change proposed, 1 structural insight articulated.*
