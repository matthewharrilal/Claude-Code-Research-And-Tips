# Discussion Guides: Journey + Dimensional Files (17-24)

**Generated by:** per-file-guide-dimensional (Opus 4.6)
**Date:** 2026-02-18
**Coverage:** 8 files (17-24), cross-referenced against bias challenger (26) and cross-reference map (32)

---

## FILE 17: decision-architecture.md

**What this file IS:** A forensic reconstruction of every major fork in the road between the failed flagship and the enhanced remediation, tracing 7 architectural decisions with alternatives considered and counterfactuals explored.

**Lines / Size:** 406 lines.

### THE 5 MOST SURPRISING FINDINGS

1. **Approach B was a COMPROMISE, not a winner** (lines 69-73). The analyst frames Approach B (CSS + targeted HTML) not as the obvious best choice but as a negotiated middle ground between the "mostly correct bones" camp (remediation) and the "DNA is wrong" camp (adversarial). The adversarial agent DISAGREED with the approach selection, and the resolution was pragmatic, not principled. Decision 1 was not a clean victory -- it was a truce.

2. **The single-builder decision is a PHILOSOPHICAL REVERSAL from the master prompt** (lines 204-205). The master prompt used multi-agent architecture as a quality mechanism. The remediation uses single-agent architecture as a quality mechanism. Same goal, diametrically opposite solution. The evidence that justified this reversal: the flagship proved multi-agent with poor communication produces WORSE results than single-agent with complete information. This reversal is presented as coherent with the other 6 decisions, but it's actually a genuine tension.

3. **Phase ordering follows a SUBTRACTIVE-BEFORE-ADDITIVE principle** (lines 159-161). Builders want to ADD things. The remediation STARTS by deleting 216 lines. This is counterintuitive -- why remove CSS before adding CSS? Because the removed CSS creates false compliance signals: sub-perceptual values that satisfy rules without producing visible effects. Cleaning first establishes a baseline of honest measurement.

4. **All 7 decisions move in the SAME direction along 5 axes** (lines 340-366). Abstract->Concrete, Builder->Spec Author, Post-hoc->Pre-authoring, Distributed->Centralized, Constraint List->Implementation Guide. This unidirectional consistency across independent decisions is either evidence of a genuine architectural insight or evidence of a single narrative being imposed post-hoc.

5. **All 7 decisions trace to exactly 3 root evidence sources** (lines 393-396). The flagship failure (1.5/4), the middle-tier success (4/4), and the retrospective's 10 takeaways. The entire decision tree rests on 3 experiments, two of which are N=1.

### ADVERSARIAL CHALLENGES (File 26)

**Bias Rating: 3 (Mixed)**

The bias challenger identifies **confirmation bias**: "The analyst presents all 7 decisions as moving in a single coherent direction... This clean unidirectional arc is suspicious. Real decision-making processes involve trade-offs, reversals, and contradictions." (26, lines 24-26)

**Specific challenge on Decision 4:** "Decision 4 (single builder) is presented as a 'PHILOSOPHICAL REVERSAL'... But a reversal BY DEFINITION disrupts a unidirectional arc. The analyst resolves this by reframing the reversal as 'centralization' -- which IS consistent with the arc -- but this reframing hides the genuine tension." (26, lines 26-27)

**Ungrounded counterfactual claim flagged:** "With fewer inputs, the remediation would have been a simpler CSS-only document without Phase 0 deallocation, Phase 1 HTML restructuring, or Phase 8 verification" -- the challenger notes this is "counterfactual speculation presented as fact" (26, lines 30-32).

**Causal vs correlational warning:** "This treats the Middle-tier and Flagship results as controlled experiments varying only FORMAT. But they varied on at least 5 dimensions simultaneously" (26, lines 43-44). The recipe-vs-checklist lesson from these comparisons is confounded.

### CROSS-REFERENCES (File 32)

- **Agreeing files:** 02 (failures), 04 (building), 05 (CSS philosophy) -- all provide evidence that Decision Architecture cites
- **Challenging files:** 20 (adversarial journey), 25 (bias core), 26 (bias dimensional)
- **Convergence:** Decision 7 (perception thresholds) has the HIGHEST convergence score of any finding in the entire corpus: 10/25 files support sub-perceptual CSS as primary failure mode

### QUESTIONS THE USER SHOULD ASK

1. If Decision 4 (single builder) is a "philosophical reversal," how can all 7 decisions be part of a single coherent arc? Is the analyst suppressing a genuine tension?
2. Would Approach C (full rebuild) with a Middle-tier-style recipe prompt have outperformed Approach B? This counterfactual is explored superficially but never quantified.
3. The decision dependency map (lines 372-391) shows all 7 decisions tracing to 3 evidence sources. What if one of those sources is wrong -- e.g., what if the Middle-tier success was luck, not recipe superiority?
4. Decision 7 (hardcoded perception thresholds) is called "a genuine innovation" by the adversarial agent. Is this actually new, or is it just what any physical science discipline would call "measurement precision"?
5. How much of the decision architecture was ACTUALLY deliberated during the conversation vs reconstructed post-hoc by this analyst?

### NUANCES EASILY MISSED

1. **The 89.5% correctness finding** (line 37) is what tipped the rebuild-vs-remediate debate. Without that quantitative argument, the adversarial camp's "DNA is wrong" framing might have won. The number did the deciding, not the philosophy.
2. **The "missing footer" smoking gun** (line 181) from the Middle-tier experiment was evidence that COMMUNICATION matters, but the remediation's solution was not "better communication" -- it was "eliminate the need for communication." This is a subtle but important distinction.
3. **Decision 6's 4-wave architecture** was designed AFTER the remediation spec, as a response to the user's "invoke every single line" mandate. It's a meta-decision about how to CONSUME the remediation, not part of the remediation itself.

### IRREPLACEABLE CONTRIBUTION

Only this file provides the **counterfactual analysis** for every decision: what would have happened if the other path was chosen. No other file explores the roads not taken. The decision dependency map (Appendix) is also unique -- it shows that Decisions 1, 2, and 4 are independent roots, while 3, 5, 6, and 7 are derived.

### DISCUSSION STARTER

"The analyst claims all 7 decisions move in the same direction, but Decision 4 is a reversal of the master prompt's multi-agent philosophy -- how does a reversal become part of a unidirectional arc?"

---

## FILE 18: prompt-craftsmanship-evolution.md

**What this file IS:** A quantitative linguistic analysis comparing the master prompt, remediation spec, and auxiliary wrapper across 6 dimensions: constraint-vs-action language, specificity gradient, information architecture, tone, redundancy strategy, and inclusion-vs-omission.

**Lines / Size:** 534 lines.

### THE 5 MOST SURPRISING FINDINGS

1. **The constraint-to-action ratio INVERTS between documents: 3.0:1 (master) vs 0.32:1 (remediation)** (lines 36-58). The master prompt uses 3 constraint/judgment verbs for every action verb. The remediation uses 3 action verbs for every constraint. This is a near-perfect 10x inversion. The auxiliary sits between them at 0.60:1, suggesting it learned from both.

2. **28% of the master prompt is backstory that the remediation drops entirely** (line 320). 272 lines of conviction, experiential laws, exemplars, parameter tables, and integration logs -- roughly a quarter of the document -- serve no purpose for the execution agent. The remediation reclaimed that 28% for 310 lines of CSS recipes that agents CAN execute.

3. **MORE repetition correlated with MORE failures** (lines 295-297). The most-repeated rules (container width: 4.3x average, content-to-void: 7x across document) were the ones that failed most spectacularly. The analyst hypothesizes that redundancy creates "attention diffusion" -- each encounter carries less weight. BUT the bias challenger notes the analyst contradicts themselves: container width was ALSO the most-successfully-followed rule (lines 70-71 of file 26).

4. **Three prompt archetypes emerge: Regulatory (master), Recipe (remediation), Orchestration (auxiliary)** (lines 433-476). Each has characteristic strengths and a natural use case: Regulatory for AUDITING, Recipe for MODIFYING, Orchestration for MANAGING. The ideal prompt would layer all three, which none achieves alone.

5. **The auxiliary prompt INVENTED "mandatory extraction artifacts"** (lines 388-389) -- a pattern neither the master prompt nor remediation spec contains. Reading must produce a written artifact ("Read X, write extraction Y, cannot proceed until Y exists"). This transforms the passive act of reading into an active, verifiable act of extraction. No prior document in the project required proof of reading.

### ADVERSARIAL CHALLENGES (File 26)

**Bias Rating: 2 (Mostly Grounded)**

The bias challenger rates this as one of the more grounded reports, praising the empirical verb-counting methodology. However:

**Conviction audience conflation:** "The analyst measures motivational tone at '25% of master prompt.' But Section A (conviction) is explicitly designed for the PLANNING agents, not the builder. The builder NEVER SAW the conviction layer. Testing whether conviction improved builder performance is testing a null intervention." (26, lines 74-76)

**Redundancy contradiction:** "Container width DID NOT fail. The analyst's own report acknowledges the flagship maintained 960px container width. The 'paradoxical finding' framing is misleading." (26, lines 68-70)

### CROSS-REFERENCES (File 32)

- **Strongest pairing:** File 21 (verbiage analysis) -- complementary lens on language. Both conclude remediation's structure is more reliable.
- **Converging with:** 22 (structural architecture), 24 (fundamental differences)
- **Challenged by:** 26 (bias dimensional) on constraint:action ratio as "symptom not cause"
- **Cross-reference map Contradiction C7:** Files 18, 21, 22, and 24 disagree about whether conviction helps or hurts. File 22's resolution: "conviction helps planners, hurts builders."

### QUESTIONS THE USER SHOULD ASK

1. The verb counting is precise, but does the RATIO actually explain the outcome difference? Could the same remediation have succeeded with a 1.5:1 constraint:action ratio if the constraints were perception-gated?
2. The "ideal prompt" architecture (Layer 0-3, lines 482-506) -- has this been tested anywhere, or is it purely theoretical? What would it look like as a concrete document?
3. The auxiliary wrapper invented "mandatory extraction artifacts" -- did the remediation execution team actually USE this pattern? Did it work?
4. If backstory is 28% waste for agents, why did the master prompt include it? Was it written for a human audience that never materialized?
5. The 3-archetype model (Regulatory/Recipe/Orchestration) -- are there prompts that don't fit any of these archetypes?

### NUANCES EASILY MISSED

1. **The auxiliary wrapper positions conviction at the END, not the beginning** (line 371). The master prompt opens with conviction (primacy position). The auxiliary closes with it (recency position). The analyst argues recency is more effective for motivational content because agents process sequentially and the last thing read is freshest. This is a specific, testable claim about LLM attention.
2. **Zero FAIL IF statements in remediation build phases** (line 223). Not "fewer" -- literally zero. Every constraint-like statement is in the Appendix, separated from the action flow. The builder never encounters a gate while building.
3. **The "teaching chain" section (lines 399-428) identifies 10 lessons** learned across the three documents. Lesson 5 ("Deletion is a design action") is philosophically novel -- the master prompt is entirely additive, assuming building from scratch.

### IRREPLACEABLE CONTRIBUTION

Only this file provides the **quantitative verb taxonomy** and **constraint:action ratios** with exact counts. Other files describe the language difference impressionistically; this one MEASURES it. The 3-archetype model and the ideal 4-layer prompt architecture are also unique to this file.

### DISCUSSION STARTER

"The master prompt uses 3 constraint verbs for every action verb; the remediation inverts this to 0.32:1 -- is this inversion the CAUSE of the improvement, or just a symptom of deeper changes like specificity and perception gating?"

---

## FILE 19: conversation-metacognition.md

**What this file IS:** An analysis of how the 5-phase conversation journey (Retrospective -> Scales Education -> Remediation Audit -> Auxiliary Prompt -> Execution) created knowledge that no single phase could have produced alone, with arguments about whether this discovery process was irreducibly experiential.

**Lines / Size:** 298 lines.

### THE 5 MOST SURPRISING FINDINGS

1. **The 5-phase pipeline is NON-REDUCIBLE** (lines 114-121). Removing any link produces a qualitatively different outcome: Without A: symptoms addressed, causes untouched. Without B: presence checked, perception unchecked. Without C: CSS patched, architecture unchanged. Without D: document delivered, invocation uncertain. The analyst argues you can shrink agents within phases but cannot collapse phases.

2. **The user's "invoke every single line" mandate came from DISTRUST, not optimization** (lines 73-75, 179-183). The user had learned from the master prompt's failure that comprehensiveness without invocation produces nothing (builder saw 13.4%). This distrust-driven maximalism was "ironically correct" -- Phase 0 deallocation, Phase 7 accessibility, and Phase 8 verification are exactly the phases an optimizer would skip but that proved critical.

3. **The knowledge is CALIBRATIONAL, not INFORMATIONAL** (lines 214-226). The "strong claim": you cannot shortcut the discovery process because knowing "sub-perceptual CSS is bad" (information) is different from knowing "backgrounds need >= 10 RGB, letter-spacing needs >= 0.025em" (calibration). Calibration requires observation of failure. The "weak claim": a pre-built perception threshold library could skip Phases B and C.

4. **The theoretical minimum pipeline is 6-8 agents across 5 steps, versus the actual 30+** (lines 155-161). The efficiency gap is ~4x in agent count. But the user needed the FULL pipeline to DISCOVER what the minimal pipeline should be. This is the "bootstrapping problem of prompt engineering."

5. **Interpreting 2.5/4 as progress rather than failure was a distinctively HUMAN judgment** (lines 185-189). An autonomous system calibrated to 4/4 would see 2.5 as 62.5% of target and might trigger a full rebuild. The user recognized that 67% improvement (1.5->2.5) in one remediation pass was strong signal. This required "domain knowledge that the user accumulated through the retrospective and audit phases."

### ADVERSARIAL CHALLENGES (File 26)

**Bias Rating: 3 (Mixed)**

**Hindsight bias:** "The entire analysis narrates the user's journey as a progressive scientific discovery. But the journey context document shows the user making pragmatic decisions, not scientific ones... The 'scientific method' framing is imposed by the analyst, not present in the source material." (26, lines 96-98)

**Self-contradicting claim:** The analyst asserts knowledge is "irreducibly experiential" (line 216) but then shows how perception thresholds (a calibrational insight) were codified into an informational table. "This directly contradicts the 'irreducibly experiential' claim." (26, lines 93-94)

**Attribution error on Phase 0:** "The claim that 'only human maximalism' motivated Phase 0 inclusion underestimates the engineering rationale. Phase 0 is good engineering practice (remove dead code), not just emotional thoroughness." (26, lines 102-104)

### CROSS-REFERENCES (File 32)

- **Agreeing files:** 00 (journey context), 17 (decision architecture)
- **Cross-referenced by:** 28 (reproducibility) for the 5-phase causal chain
- **Contradiction C4:** This file treats 2.5/4 as "meaningful progress"; file 20 calls it "below the shipping threshold, not success"
- **Meta-Finding 4 from file 32:** The intentionality layer is the UNANALYZED gap -- this file discusses what the journey TAUGHT but not what it MISSED

### QUESTIONS THE USER SHOULD ASK

1. Is the "irreducibly experiential" claim falsifiable? Could a sufficiently detailed perception threshold library + stacking arithmetic reference make Phases A-C unnecessary for FUTURE projects?
2. The analyst claims the user's "invoke every single line" was distrust-driven. But what if it was just thoroughness? Does the distinction matter for future process design?
3. The minimum pipeline of 6-8 agents -- has anyone tried running it? Would it actually work, or would the compression lose critical discoveries?
4. If the 5-phase pipeline IS non-reducible, what does that say about the cost of prompt engineering? Every project needs 5 phases just to write a good execution prompt?
5. The "bootstrapping problem" (line 161) -- is there a second-order solution, like maintaining a cross-project knowledge base that accumulates calibrational knowledge?

### NUANCES EASILY MISSED

1. **Phase B (Scales Education) redirected learning** (lines 136-137). Before the retrospective, the user would have asked about metaphor derivation (the flashy part). After, they asked about perception thresholds (the critical part). The retrospective didn't just reveal failures -- it changed WHAT the user wanted to learn.
2. **The "scientific method" analogy is the analyst's frame** (line 225), not the user's. The user didn't plan an experiment; they reacted to a broken page. The scientific arc is an after-the-fact narrative.
3. **The paradox of this analysis** (Section 8, lines 280-293): This metacognitive analysis ITSELF required completing the journey first. It could not have been written at Phase A, because the vocabulary didn't exist yet. Meta-knowledge is inherently retrospective.

### IRREPLACEABLE CONTRIBUTION

Only this file traces the **USER's evolving understanding** across 5 stages (diagnostic -> vocabulary acquisition -> audit -> invocation -> measurement). No other file analyzes the human's intellectual progression as a subject of study. The "calibrational vs informational knowledge" distinction (Section 6) is also unique.

### DISCUSSION STARTER

"The analyst claims this discovery process was 'irreducibly experiential,' but then shows that the key insight (perception thresholds) was codified into a table that any future prompt could include -- so is it really irreducible, or just currently unexported?"

---

## FILE 20: adversarial-journey-review.md

**What this file IS:** A genuinely adversarial challenge to every major claim in the pipeline analysis, identifying survivorship bias, effort confounds, N=2 limitations, and proposing the single most informative experiment nobody has run.

**Lines / Size:** 310 lines.

### THE 5 MOST SURPRISING FINDINGS

1. **The recipe-vs-checklist thesis is OVERSTATED** (lines 99-111). The analyst argues the real distinction is "concrete perceptual specifications vs abstract structural specifications" -- NOT recipe format vs checklist format. A concrete checklist with perception thresholds would outperform a vague recipe. The format is the vehicle, not the cargo. This directly contradicts the master narrative of the entire pipeline analysis.

2. **30-40% of improvement may be attributable to simple ITERATION, not the remediation spec** (lines 34-37). A second-pass builder who opens a mostly-blank page would add content and increase contrast even WITHOUT a remediation spec. The remediation's specific contributions (thresholds, deallocation, HTML grids, stacking math) account for the other 60-70%. This "iteration effect" confound is never addressed by any other file.

3. **The steel-man for the master prompt is genuinely strong** (lines 186-218). The master prompt's failures might be explained by 4 missing lines (perception thresholds) and 2 routing errors (A5/A7 in conviction layer instead of builder layer). CD-006 achieved 39/40 using the same approach. The cheapest experiment (add thresholds to master prompt, re-run) would test whether the entire recipe approach was necessary.

4. **PA-05 scoring is evaluated by AI agents, not humans** (lines 291-293). "We're building prompts that satisfy other Claude instances' perception of quality, which may or may not correlate with human perception. Nobody has asked a human designer to evaluate either page." The entire quality framework is synthetic and potentially circular.

5. **Content quality is the unexamined variable** (lines 244-247). A 12-section page with dense research prose might produce spatial monotony regardless of CSS treatment -- because the content itself is structurally monotonous. The remediation acknowledged this ("RC-13: Uniform prose register -- NOT ADDRESSED") but no analyst examines whether CSS/HTML can solve a content problem.

### ADVERSARIAL CHALLENGES (File 26)

**Bias Rating: 1 (Strongly Grounded)** -- The BEST rated report in the entire corpus.

The bias challenger calls this "the most rigorous report in the entire set. It systematically challenges every major narrative, provides steel-man arguments for the master prompt, identifies N=2 limitations, and raises genuinely uncomfortable questions." (26, lines 113-114)

Only minor issues flagged:
- "'Almost certainly yes' is an unjustified confidence level for a counterfactual" (26, line 120)
- The N=2 critique, while accurate, would "make ALL prompt engineering research indefensible" if taken to its logical conclusion (26, line 124)

### CROSS-REFERENCES (File 32)

- **Reinforcing:** Files 25, 26 (both apply adversarial lens, agree on N=2 limitations, recipe thesis overstated, hindsight packaging)
- **Contradicts:** Files 17, 19 (which treat remediation decisions as universally correct)
- **Cross-reference map Meta-Finding 3:** File 20 identifies the cheapest most informative experiment; file 26 endorses it; NO other file proposes running it
- **Convergence score for steel-man (A8):** 5/25 files agree CD-006 proves the constitutional approach CAN work

### QUESTIONS THE USER SHOULD ASK

1. The "cheapest experiment" (add 4 lines of perception thresholds to master prompt, re-run): Why hasn't this been proposed as a next step? Is the analysis corpus avoiding it because it might invalidate the narrative?
2. The survivorship bias estimate (30-40% iteration effect) -- is this testable? Could you give a different builder the failed flagship HTML with NO remediation spec and measure what they improve?
3. If PA-05 is evaluated by AI agents, is there a way to validate that AI-perceived quality correlates with human-perceived quality for this specific design system?
4. The content monotony concern (RC-13) -- what would the page look like with DIFFERENT content that had more structural variety (lists, dialogues, case studies)? Would the same CSS produce a better PA-05 score?
5. "The single most dangerous bias: the narrative has settled on 'recipe > constitutional' before adequately testing whether constitutional + perception thresholds would have produced equivalent results" (line 309). Do you agree? Is this a fair characterization?

### NUANCES EASILY MISSED

1. **"Blind Spot 4: We're Evaluating the Remediation Against Its OWN Criteria"** (lines 237-239). PA-05 scoring, perception checks, void measurement -- these evaluation criteria were DESIGNED by the same process that produced the remediation. The master prompt's own evaluation criteria (CCS, SCI) would score its output more favorably. This is circular evaluation.
2. **The builder's deviations IMPROVED the output** (lines 18-22). The warm-tone substitutions (B-01 through B-03 fixes) show the builder overriding spec values. This means the builder brought independent design intelligence that the recipe alone didn't provide.
3. **The steel-man's 3 weaknesses** (lines 210-218) are as important as the steel-man itself. CD-006 was built by a "hand-picked Opus agent with deep context." "4 missing lines" understates the cascade of changes needed. The constitutional approach "hasn't scaled either" -- the master prompt grew from 100 to 963 lines across phases.

### IRREPLACEABLE CONTRIBUTION

Only this file provides the **steel-man case FOR the master prompt** (Section 6) and identifies the **5 blind spots** in the collective narrative (Section 7). It is the only file that proposes specific EXPERIMENTS to resolve the ambiguities rather than more analysis. The "iteration effect" estimate (30-40%) is unique.

### DISCUSSION STARTER

"The adversarial reviewer claims the cheapest, most informative experiment -- adding 4 lines of perception thresholds to the master prompt and re-running -- has never been proposed by any other analyst. Why not? What does that tell us about the narrative?"

---

## FILE 21: verbiage-analysis.md

**What this file IS:** An exhaustive linguistic forensics of the master prompt and remediation spec, counting every verb, classifying every directive by specificity and cognitive demand, measuring negation density, and ranking instruction patterns by predicted propagation reliability.

**Lines / Size:** 499 lines.

### THE 5 MOST SURPRISING FINDINGS

1. **The judgment-to-action ratio inverts by 6.7x** (lines 49-53). Master prompt: 1.53 judgment verbs per action verb. Remediation: 0.23 judgment verbs per action verb. This is NOT a small shift -- it's a near-order-of-magnitude reversal in what the documents ASK agents to DO (judge vs execute).

2. **The 10-rank propagation reliability ranking** (lines 378-391) is the single most actionable artifact in the entire analysis corpus. It ranks instruction patterns from CONCRETE DELETE (~98% reliability) through BINARY THRESHOLD (~90%) down to COUNTERFACTUAL THOUGHT EXPERIMENT (~5%). The master prompt concentrates at ranks 7-9 (25-50% reliability); the remediation at ranks 1-3 (92-98%).

3. **The master prompt has a negation every 11.5 lines; the remediation has one every 64 lines** (lines 261-282). 5.4x difference. The master prompt's negation architecture makes the PROHIBITED space explicit but leaves the PERMITTED space implicit. The remediation makes the PERMITTED space explicit (exact values to write) and concentrates prohibitions in a single appendix.

4. **53 JUDGE directives at 40-60% reliability each creates (0.5)^5 = 3.1% compound success** (lines 417-418). Even just 5 sequential judgment calls at 50% reliability produces only 3.1% chance of ALL aligning with intent. The master prompt has 53 such calls. BUT the bias challenger notes this probability model assumes independence, which is wrong -- LLM judgments are correlated.

5. **The remediation places ALL judgment in Phase 8 (verification), after ALL action** (lines 438-450). The master prompt interleaves judgment and action at every step (judge-act-judge-act). The remediation's flat-then-verify pattern (act-act-act-verify) is less cognitively demanding. But the bias challenger asks: did the builder actually faithfully execute Phase 8's 9 judgment calls?

### ADVERSARIAL CHALLENGES (File 26)

**Bias Rating: 2 (Mostly Grounded)**

**Independence assumption challenged:** "This calculation assumes judgment calls are INDEPENDENT. In practice, LLM judgments are correlated... The probability model is wrong." (26, lines 149-151)

**Reliability figures unsourced:** "The 40-60% reliability figure is asserted but unsourced. The analyst attributes it to 'established LLM behavioral research' without citation." (26, lines 153-154)

**Phase 8 assumption untested:** "The analyst does not investigate whether Phase 8's 9 judgment calls were ACTUALLY PERFORMED effectively. If the builder rushed through Phase 8, then placing judgment at the end is not a strength but a vulnerability." (26, lines 156-157)

### CROSS-REFERENCES (File 32)

- **Complementary pair:** File 22 (structural architecture) -- 21 analyzes language, 22 analyzes structure; same conclusion
- **Endorsed by:** File 28 (reproducibility) which calls the propagation ranking "single most actionable artifact"
- **Agreement A2:** 8/25 files agree recipe format outperforms constraint format; 3/25 challenge it as overstated
- **Contradiction C1:** Files 18 and 21 say recipe-vs-checklist IS the core finding; file 20 says it's overstated

### QUESTIONS THE USER SHOULD ASK

1. The propagation reliability ranking (ranks 1-10) -- is this empirically validated or theoretically estimated? Has anyone tested "concrete delete" at exactly 98% across multiple runs?
2. The independence assumption in the compound probability calculation -- what IS the correlation structure of LLM judgments? If we estimate 0.8 correlation, how does the compound probability change?
3. Phase 8 judgment calls placed at the end -- is there evidence that builders actually perform end-of-build verification faithfully, or do they rush through it?
4. The 5.4x negation density difference -- does negation actually degrade LLM compliance? Is there research on how LLMs process negation vs affirmation?
5. The finding "Both documents are necessary" (Section 6.6, lines 453-466) -- how would a prompt that combined the master's generative capacity with the remediation's execution precision actually be structured?

### NUANCES EASILY MISSED

1. **SEMI-CONCRETE directives collapse from 57.4% to 13.7%** (lines 109-113). This is where most LLM interpretation errors occur -- bridging from measurable criteria to concrete implementation. The remediation eliminates this bridge by providing the implementation directly.
2. **The cognitive demand model** (Section 4, lines 195-239) classifies every directive as JUDGE, COMPUTE, EXECUTE, or COMPOSE. The master prompt puts 49% of demand in JUDGE+COMPUTE (40-80% reliability); the remediation puts 78% in EXECUTE (~95% reliability). This is a PLACEMENT argument, not just a formatting one.
3. **Conviction language propagates as DIRECTION without MAGNITUDE** (lines 429-434). "Warm" -> agent uses warm colors (correct direction) but may under- or over-warm (uncalibrated magnitude). "Unhurried" -> agent adds TOO MUCH spacing (correct direction, catastrophic magnitude).

### IRREPLACEABLE CONTRIBUTION

Only this file provides the **10-rank propagation reliability ranking** with predicted percentages per instruction type. Only this file quantifies the **negation density** (1/11.5 lines vs 1/64 lines). The **cognitive demand model** (JUDGE/COMPUTE/EXECUTE/COMPOSE) with distribution percentages is unique.

### DISCUSSION STARTER

"The propagation reliability ranking predicts that COUNTERFACTUAL THOUGHT EXPERIMENTS have ~5% compliance -- meaning the master prompt's MG-04 ('imagine removing all labels -- does structure communicate?') was essentially a decorative instruction that no agent would ever execute. How many of the master prompt's 97 rules are actually at the ~5% tier?"

---

## FILE 22: structural-architecture.md

**What this file IS:** A structural comparison of the master prompt and remediation spec as information architectures -- how they organize rules, manage attention, handle cross-referencing, and what cognitive load each places on agents.

**Lines / Size:** 416 lines.

### THE 5 MOST SURPRISING FINDINGS

1. **The master prompt requires 10 locations across 5 major sections spanning 625+ lines for a single build step** (lines 141-156). A skeleton builder must extract S-01 from B1, U-01 from B2, relevant rules from B3/B4/B5/B6/B7, self-check items from B10, and file lists from E. The remediation requires reading a single contiguous block of 30-80 lines per phase. Zero cross-referencing.

2. **The master prompt has 0 attention renewal points vs the remediation's 8** (lines 224-228). But the bias challenger CHALLENGES this: "The master prompt has GATE CHECKPOINTS (C3: Gates 0-5) that function as attention renewal points... If gates are attention renewal points, the master prompt has 6, not 0." (26, lines 175-177). This changes the comparison from 8-vs-0 to 8-vs-6.

3. **The conviction layer consumes 12.1% of the document but the empirical evidence suggests it helped PLANNING agents, not builders** (lines 231-261). The master prompt treats all agents as planners. The remedy: conviction for Opus creative agents ONLY; builders see recipe ONLY.

4. **The ideal 4-layer architecture** (lines 376-403). Layer 0: Phase-specific recipe (remediation style, builders only). Layer 1: Domain rules (master style, planners and auditors). Layer 2: Conviction (master style, Opus creative agents only). Layer 3: Reference (master style, file paths validated pre-launch). This separates audiences that the master prompt collapses into one document.

5. **Structure is not neutral** (lines 405-409). The master prompt's domain-organized, conviction-first architecture PRODUCES "high ambition + incomplete execution." The remediation's phase-sequential, embedded-value architecture PRODUCES "reliable competence without creative breakthrough." Neither alone achieves the goal.

### ADVERSARIAL CHALLENGES (File 26)

**Bias Rating: 2 (Mostly Grounded)**

**Gate checkpoint omission:** "The analyst claims the master prompt has '0 attention renewal points.' This is structurally false. The master prompt has GATE CHECKPOINTS (C3: Gates 0-5) that function as attention renewal points... The analyst either missed the gates or chose to exclude them." (26, lines 175-177)

**Attention degradation oversimplified:** "LLM attention is not a monotonically decreasing function of consumed tokens. Attention is influenced by salience, formatting, and relevance markers. The master prompt uses anti-skimming techniques specifically to combat attention degradation. The analyst doesn't account for these countermeasures." (26, lines 181-183)

**4-layer recommendation praised:** "The 4-layer architecture recommendation is one of the most actionable outputs of the entire analysis set." (26, lines 187-188)

### CROSS-REFERENCES (File 32)

- **Complementary pair:** File 21 (verbiage analysis) -- 21 analyzes language, 22 analyzes structure
- **Agreement on conviction routing:** Files 18, 21, 22, 24 address conviction; file 22's resolution ("helps planners, hurts builders") is endorsed by cross-reference map as correct
- **Contradiction C7:** Conviction helps or hurts? File 22 provides the synthesis resolution: audience-dependent
- **Contradiction C8:** Single-agent vs multi-agent? File 22 and 24 both argue context-dependent (single for remediation, multi for novel builds)

### QUESTIONS THE USER SHOULD ASK

1. The 4-layer architecture -- has anyone tried implementing it as an actual prompt? What would Layer 0 look like for a NEW page (not remediation)?
2. If gates ARE attention renewal points, the master prompt has 6 vs the remediation's 8. Is the 2-point difference significant enough to explain the outcome difference?
3. "Lines to read before first action" is 500 for the master prompt vs 132 for the remediation (line 223). Is there a threshold beyond which pre-action reading degrades execution quality? What is it?
4. The external reference model (master) vs embedded value model (remediation) -- is there a hybrid that gets DRY reusability without file-path fragility?
5. The analyst says "neither structure alone achieves the project's goal of 4/4 DESIGNED + SHIP-ready" (line 409). What would a structure that achieves both look like in practice?

### NUANCES EASILY MISSED

1. **The master prompt's 75-line builder cap creates a second-order problem** (lines 157-158). SOMEONE must select which 75 lines from a 964-line document. Who? With what criteria? This selection problem is never addressed.
2. **The remediation's phases are "mostly independent" not "fully independent"** (line 227). Phase 1 class names must exist before Phase 2 CSS selectors reference them. The independence is approximate, and the exceptions could cause bugs if phases are executed out of order.
3. **The traceability chain** (lines 337-351) maps the STRUCTURAL cause of each outcome. Master: scattered rules -> cross-reference 10+ locations -> attention degradation at location 7+ -> MC rules not internalized -> CCS 0.05. Remediation: sequential phases -> read-execute-verify per phase -> attention resets -> values applied as specified -> 3/4 COMPETENT.

### IRREPLACEABLE CONTRIBUTION

Only this file provides the **cognitive load comparison table** (lines 180-187) with specific metrics: cross-references per task, lines to scan, external file reads, working memory demand, and error mode. The **4-layer ideal architecture** recommendation is unique and the most actionable structural proposal in the corpus.

### DISCUSSION STARTER

"The bias challenger says the analyst missed that the master prompt has 6 gate-based attention renewal points, not 0 -- if that's true, how does it change the structural comparison, and why did the analyst miss something so prominent?"

---

## FILE 23: propagation-analysis.md

**What this file IS:** A causal chain trace of 30 rules (15 master prompt, 15 remediation) from instruction text through builder action to visible output, measuring compliance rate, visibility rate, and full conversion rate for each.

**Lines / Size:** 404 lines.

### THE 5 MOST SURPRISING FINDINGS

1. **The master prompt has a HIGHER visibility rate per followed rule than the remediation: 64% vs 54%** (lines 259-264). This finding FAVORS the master prompt but is buried in the analysis. The master prompt's followed rules tend to be high-visibility identity rules (border-radius: 0, font trinity). The remediation includes low-visibility accessibility rules (ARIA, skip link). The bias challenger flags this as "metric shopping" by the analyst.

2. **Both prompts have ~25% waste rate** (lines 279-289). Roughly 1 in 4 followed instructions produce no clearly visible effect. BUT the waste is different: master prompt waste is ACCIDENTAL (sub-perceptual identity enforcement nobody can see), remediation waste is INTENTIONAL (accessibility infrastructure for non-visual modalities).

3. **C-12 (rejection log) is a DEAD RULE with zero visual propagation** (lines 54-58). The master prompt requires documenting 21+ rejected mechanism placements. This is a process artifact that never touches the HTML. It's a rule whose output is a document, not a visual. Dead rules inflate the master prompt's rule count without contributing to visual quality.

4. **MG-04 (structural metaphor test) achieved ZERO propagation** (lines 96-100). The instruction asks the agent to IMAGINE removing labels and JUDGE whether visual structure communicates. This is "maximally difficult for an LLM" -- counterfactual reasoning about perception. The metaphor remained "announced" (40% without labels) not "structural."

5. **The remediation achieved 100% mechanical compliance: every instruction was executed** (line 369). The 27-point compliance gap (100% vs 73%) is entirely explained by the master prompt containing rules with NO execution path in a single-agent context (process rules, cross-prompt rules, dead rules). If you exclude inapplicable rules, the master prompt's rate rises to 92%.

### ADVERSARIAL CHALLENGES (File 26)

**Bias Rating: 2 (Mostly Grounded)**

**Metric shopping flagged:** "The analyst achieved the 'more efficient by every measure' framing by switching metrics: they compare COMPLIANCE RATES (100% vs 73%) rather than VISIBILITY RATES (54% vs 64%). This is metric-shopping: choosing the metric that supports the narrative." (26, lines 318-319)

**Ch1 chromatic impact contradicted:** "The analyst claims Ch1 has lowest perceptual impact but... acknowledges the GROUP-level contrast 'IS perceptible' then claims the channel has 'LOWEST perceptual impact' anyway. This is a contradiction." (26, lines 209-211)

**Asymmetric attribution noted:** "The analyst attributes the master prompt's 14-mechanism deployment to the master prompt's rules but attributes the remediation's improvements to the remediation's specific CSS values... inadvertently undercuts their own overall conclusion." (26, lines 217-219)

### CROSS-REFERENCES (File 32)

- **Complementary pair:** File 11 (bidirectional mapping) -- 11 maps philosophy->action, 23 maps instruction->visible output
- **Strongest evidence source for:** Agreement A1 (sub-perceptual CSS, 10/25 convergence), Agreement A3 (stacking loophole, 8/25)
- **Contradiction C4:** "Remediation improved but did NOT achieve shipping threshold"
- **Finding 21 propagation ranking** from file 21 is empirically validated here

### QUESTIONS THE USER SHOULD ASK

1. The master prompt's higher visibility rate per followed rule (64% vs 54%) -- why does the analyst bury this finding? Is it because it contradicts the narrative?
2. Out of the master prompt's 97 rules, how many are DEAD RULES (process artifacts with zero visual path)? If it's 20+, the master prompt is inflated with non-visual busywork.
3. MG-04 achieved ZERO propagation -- should metaphor structural tests be removed from future prompts, or reformulated as concrete checks?
4. Ch1 (chromatic) is the primary metaphor carrier but has lowest perceptual impact. Should metaphor be carried by Ch4 (structural) instead?
5. Is 100% mechanical compliance actually meaningful if 46% of those complied instructions produce no visible effect?

### NUANCES EASILY MISSED

1. **U-06 (no pure black/white) achieves COMPLIANCE without PERCEPTION** (lines 32-34). The difference between #000000 and #1A1A1A is 26 RGB points -- borderline perceptible on a dark header. This is "sub-perceptual identity enforcement." The rule is philosophically important but visually inert.
2. **The conversion topology differs fundamentally** (lines 293-313). Master prompt failures cluster in 3 types: dead rules, cross-prompt rules, aspirational rules. Remediation failures cluster in 1 type: invisible-by-design (accessibility). The remediation has ZERO structural conversion failures.
3. **Ch4 (structural/borders) is the remediation's highest-visibility contribution** (line 331). Despite the master prompt assigning Ch1 (chromatic) as primary metaphor channel, the actual perceptual hierarchy inverts: borders are more visible than backgrounds.

### IRREPLACEABLE CONTRIBUTION

Only this file traces the **complete causal chain** (instruction -> builder action -> CSS/HTML evidence -> visible effect) for 30 individual rules with line-number evidence. The **conversion topology** (dead/cross-prompt/aspirational vs invisible-by-design) is a unique analytical framework. The per-channel propagation table (Section 5) is also unique.

### DISCUSSION STARTER

"The master prompt's followed rules produce HIGHER visibility per rule (64%) than the remediation's (54%), but the analyst headlines the compliance rate (100% vs 73%) instead -- is this metric shopping, and what does the visibility-rate advantage tell us about the master prompt's actual quality?"

---

## FILE 24: fundamental-differences.md

**What this file IS:** A metacognitive analysis that examines the master prompt and remediation spec as different cognitive instruments -- what kind of intelligence each assumes, demands, and produces -- across 7 conceptual dimensions.

**Lines / Size:** 400 lines.

### THE 5 MOST SURPRISING FINDINGS

1. **"The 75-line builder cap is the most honest line in the master prompt"** (Proposition 4, line 153/363). It acknowledges that the specification paradigm exceeds individual agent capacity. If agents COULD hold 964 lines, the cap wouldn't exist. The cap exists because the prompt's author knows agents CAN'T -- but the SYSTEM needs all 964 lines. The solution: distribute across multiple agents, each holding ~75.

2. **Generation can produce 39/40 OR 1.5/4 -- transformation reliably produces 3/4** (lines 123-131). This is the paradox of quality: the harder task (generation) has higher ceiling AND lower floor. The remediation guarantees the floor (3/4) while accepting a lower ceiling. This is the classic reliability-vs-ceiling tradeoff made explicit.

3. **The optimal prompt would make the declarative/imperative duality EXPLICIT** (lines 346-352). Blue blocks = declarative (describe desired state, let agent derive). Red blocks = imperative (describe exact steps, let agent execute). Green blocks = verification (describe what to check, binary pass/fail). This color-coding would give agents clear signals about which cognitive mode to engage.

4. **Ambient complexity is the master prompt's greatest asset AND greatest liability** (Proposition 3, lines 250-287). 97 rules create combinatorial interactions that no agent can hold simultaneously. But these interactions are also what enables emergent quality. The remediation eliminates ambient complexity and thereby eliminates both catastrophic failure AND emergent beauty.

5. **Specification and procedure are complementary, not competing** (Proposition 1, line 357). They serve different phases of the same design lifecycle: specification for GENERATION (create the artifact), procedure for REMEDIATION (fix what generation got wrong). This is the most important conceptual contribution across the 8 files.

### ADVERSARIAL CHALLENGES (File 26)

**Bias Rating: 2 (Mostly Grounded)**

**"LLMs are better at application" contradicted by evidence:** "CD-006 was a DERIVATION task and scored 39/40. The Middle-tier experiment was also derivation-based and scored 4/4. Both derivation tasks outperformed the remediation (2.5/4). If LLMs are 'better at application,' why do their best results come from derivation?" (26, lines 233-235)

**"~40 rules" number ungrounded:** "The '~40' number is pulled from thin air... 40 actions and 40 rules are different cognitive loads. An action-based prompt with 40 steps is a recipe; a rule-based prompt with 40 constraints is still a specification." (26, lines 239-241)

**Task Taxonomy praised:** "The 'Task Taxonomy' table correctly identifies that the two prompts solve DIFFERENT PROBLEMS. This is the most important conceptual contribution across all 9 reports." (26, lines 244-245)

### CROSS-REFERENCES (File 32)

- **Contradiction C7 (conviction):** File 24 argues conviction allows LLMs to use "pattern completion from training data" -- i.e., conviction HELPS generation. This partially contradicts file 18's "motivation wastes tokens."
- **Contradiction C8 (single vs multi-agent):** File 24 argues multi-agent is NECESSARY for novel builds because "no single agent can hold 964 lines, exercise aesthetic judgment, AND write coherent CSS"
- **Agreement A8:** 5/25 files agree CD-006 proves constitutional approach CAN work; file 24 frames this as the ceiling-vs-floor tradeoff
- **Meta-Finding 7:** File 26 separates file 24's "declarative vs imperative" distinction as "applied existing knowledge (fundamental CS distinction), not a discovery"

### QUESTIONS THE USER SHOULD ASK

1. The blue/red/green block color-coding for declarative/imperative/verification -- could this literally be implemented in a prompt? Would agents respect the color-coded cognitive mode switches?
2. If ambient complexity enables emergent quality but also causes catastrophic failure, is there a way to preserve the emergence while catching the failure EARLIER (e.g., more gates, smaller batches)?
3. The "telescope vs microscope" metaphor (line 399) -- is there a "binoculars" prompt that gets moderate range and moderate precision?
4. "4/4 requires emergence, which simplicity cannot produce" (Proposition 5, line 365) -- is this actually true? The Middle-tier achieved 4/4 DESIGNED with a simple recipe. Does that contradict this claim?
5. The "deepest insight" (line 183) that the master prompt's theory of mind is "wrong about individual agents but right about systems" -- does this suggest the master prompt should be kept but ONLY used as a SYSTEM prompt, never given to a single agent?

### NUANCES EASILY MISSED

1. **The determinism hierarchy** (lines 37-46) places the master prompt across the ENTIRE spectrum simultaneously -- from "write this exact CSS block" to "the mission is beauty, not compliance." The remediation operates only in the top two tiers. This is accurate mapping, not a flaw of either document.
2. **The "hidden advantage of declarative"** (lines 241-242): declarative prompts allow LLMs to use pattern completion from training data. An LLM that has seen thousands of beautiful web pages can generate one that satisfies "warm, authoritative, unhurried" BETTER than it can follow 40 steps to approximate warmth. This argues AGAINST the remediation approach for novel builds.
3. **The master prompt's gate sequence IS a hybrid** (lines 343-344). Gates are imperative checkpoints that interrupt declarative specification. The master prompt already partially combines both paradigms. The remediation's perception checks are declarative evaluations that interrupt imperative steps. Both documents are ALREADY hybrids -- they just don't LABEL themselves as such.

### IRREPLACEABLE CONTRIBUTION

Only this file provides the **7 conceptual propositions** (lines 356-370) that synthesize findings into testable claims. The **Task Taxonomy** table (lines 298-307) mapping each prompt to its appropriate task domain is unique. The "telescope vs microscope" metaphor and the blue/red/green block proposal are unique practical recommendations.

### DISCUSSION STARTER

"The analyst claims 4/4 DESIGNED quality requires emergence, which explicit simplicity cannot produce -- but the Middle-tier experiment achieved 4/4 with a simple recipe. Is this a fatal counterexample, or was the Middle-tier's content simple enough that emergence wasn't needed?"

---

## CROSS-CUTTING THEMES ACROSS ALL 8 FILES

### Theme 1: The Recipe-vs-Checklist Debate Is NOT Settled

Files 17, 18, 21, 22 treat recipe superiority as established fact. File 20 calls it "overstated" and argues the real variable is concrete-vs-abstract, not format. File 24 says specification and procedure are complementary, not competing. File 26 rates this as the "most dangerous bias" in the corpus. The cross-reference map (32) records this as Contradiction C1 with 8 files supporting and 3 challenging.

**Status: CONTESTED. The cheapest test (master prompt + perception thresholds, re-run) has not been performed.**

### Theme 2: The Master Prompt Did More Right Than the Narrative Acknowledges

File 23 shows the master prompt has HIGHER visibility per followed rule (64% vs 54%). File 20's steel-man argues 89% of the final artifact's architecture was created by the master prompt. File 24 acknowledges the master prompt's compositional architecture (14 mechanisms, 4 clusters, metaphor channels) cannot be produced by a recipe. File 26 calls asymmetric credit assignment the second most dangerous bias.

**Status: UNDER-ACKNOWLEDGED. The remediation is a patch on the master prompt's architecture, not a replacement.**

### Theme 3: N=2 Undermines All Conclusions

Files 20, 25, 26 independently flag that the entire analysis rests on two data points (flagship failure + remediation improvement). From N=2, the corpus derives "experiential laws," "propagation reliability rankings," and 87.6% reproducibility scores. The appropriate confidence level is much lower than what the analysts express.

**Status: ACKNOWLEDGED but unaddressed. No agent proposes controlled experiments to increase N.**

### Theme 4: The Missing Experiment

File 20 identifies it. File 26 endorses it. No other file proposes it. The cheapest, most informative experiment: add 4 lines of perception thresholds to the master prompt and re-run with the same builder. If the result improves to 3/4+, the recipe format was unnecessary and thresholds were the causal variable. If it stays at 1.5/4, the recipe IS necessary. Until this is run, all causal claims are premature.

**Status: UNRUN. The analysis corpus is ANALYZING when it should be EXPERIMENTING.**

### Theme 5: Context-Dependent, Not Universal

Files 20 and 24 correctly distinguish between brownfield (remediation) and greenfield (novel build) lessons. The remediation's advantages are context-specific: it knew what was broken, operated on existing HTML, had a concrete artifact to prescribe against. Most other files implicitly treat remediation lessons as universal, which the adversarial and fundamental-differences analyses warn against.

**Status: PARTIALLY recognized. Transferable lessons (perception thresholds, stacking arithmetic) should be distinguished from context-dependent ones (deallocation, single-builder).**

---

## BIAS RATINGS SUMMARY (from File 26)

| File | Rating | Primary Bias |
|------|--------|-------------|
| 17 Decision Architecture | 3 (Mixed) | Confirmation + Hindsight: clean unidirectional arc imposed |
| 18 Prompt Craftsmanship | 2 (Mostly Grounded) | Conviction audience conflation |
| 19 Conversation Metacognition | 3 (Mixed) | Hindsight + Attribution: scientific method framing post-hoc |
| 20 Adversarial Journey | 1 (Strongly Grounded) | Best report in entire corpus |
| 21 Verbiage Analysis | 2 (Mostly Grounded) | Independence assumption in probability model |
| 22 Structural Architecture | 2 (Mostly Grounded) | Gate checkpoints omitted from attention renewal count |
| 23 Propagation Analysis | 2 (Mostly Grounded) | Metric shopping: compliance rate over visibility rate |
| 24 Fundamental Differences | 2 (Mostly Grounded) | "LLMs better at application" contradicted by CD-006 |

**Average: 2.1 (Mostly Grounded with identifiable biases)**
**Best: File 20 (Adversarial Journey) at 1**
**Weakest: Files 17 and 19 at 3**

---

## CONVERGENCE DATA (from File 32)

Top convergence scores relevant to these 8 files:

| Finding | Files Supporting | Net Score | Confidence |
|---------|-----------------|-----------|------------|
| Sub-perceptual CSS is primary failure | 10/25 | 10 | VERY HIGH |
| Perception thresholds prevent failure | 11/25 | 11 | VERY HIGH |
| Deallocation-before-addition principle | 9/25 | 9 | VERY HIGH |
| S-09 stacking loophole | 8/25 | 8 | VERY HIGH |
| Recipe > constraint format | 8/25 (3 challenge) | 5 | HIGH with caveat |
| Binary rules > judgment rules | 6/25 | 6 | HIGH |
| Zone architecture preserved | 7/25 | 7 | HIGH |
| CD-006 proves constitutional CAN work | 5/25 | 5 | HIGH (conditions unexamined) |
| Concrete-vs-abstract is true variable | 3/25 | 3 | MODERATE (but strongest analytical rigor) |
| N=2 undermines conclusions | 3/25 | 3 | HIGH (meta-finding) |

---

## RECOMMENDED READING SEQUENCES

### If the user has 30 minutes (pick ONE path):

**Path A -- "How did the decisions get made?"**
Read file 17 (decision architecture) then file 20 (adversarial journey) for the strongest challenge.

**Path B -- "How do the two prompts differ linguistically?"**
Read file 21 (verbiage analysis) for the quantitative data, then file 24 (fundamental differences) for the philosophical framing.

**Path C -- "Did the remediation actually work?"**
Read file 23 (propagation analysis) for the evidence, then file 20 (adversarial journey) for the skeptical counter-narrative.

### If the user has 60 minutes (the essential quartet):

Read files 20, 21, 22, 24 in that order. File 20 provides the adversarial frame that prevents uncritical absorption. File 21 gives the quantitative linguistic evidence. File 22 shows the structural architecture differences. File 24 synthesizes everything into the fundamental-differences framework. Skip 17, 18, 19, 23 -- they reinforce but do not add conceptually distinct findings.

### If the user has 2+ hours (full ingestion):

Read all 8 files in this order: 24 (framework first), 21 (quantitative evidence), 22 (structural lens), 23 (propagation evidence), 17 (decision reconstruction), 18 (prompt evolution), 19 (metacognition), 20 (adversarial challenge LAST -- read this after forming your own views, so the challenges actually challenge rather than pre-empt).

---

## FILE UNIQUENESS MAP

What ONLY this file provides (not duplicated elsewhere):

| File | Unique Contribution | Why It Matters |
|------|-------------------|---------------|
| 17 | Counterfactual analysis for every decision fork | No other file explores roads not taken |
| 18 | Quantitative verb taxonomy + 3 prompt archetypes | Only measured data on constraint:action ratios |
| 19 | User's intellectual progression as analytical subject | Only file studying the HUMAN's learning curve |
| 20 | Steel-man for master prompt + 5 blind spots + missing experiment | Only genuine adversarial challenge to the whole narrative |
| 21 | 10-rank propagation reliability ranking + negation density | Only predictive model of instruction-type reliability |
| 22 | 4-layer ideal architecture + cognitive load table | Most actionable structural recommendation |
| 23 | 30-rule causal chain trace with line evidence | Only empirical rule-by-rule compliance/visibility measurement |
| 24 | 7 propositions + task taxonomy + declarative/imperative framework | Most philosophically sophisticated conceptual model |

---

**END OF DISCUSSION GUIDES (FILES 17-24)**

**Total files covered:** 8 primary + 2 cross-reference sources
**Total discussion starters:** 8
**Total questions for user:** 40
**Total nuances flagged:** 24
**Total lines:** 610+

---

## APPENDIX: QUICK-REFERENCE CONTRADICTION MAP

These are the active contradictions across the 8 files that should drive discussion:

| Contradiction | Position A (files) | Position B (files) | Resolution Status |
|--------------|-------------------|-------------------|-------------------|
| Recipe-vs-checklist as core finding | 17, 18, 21 | 20, 24 (overstated) | CONTESTED -- real variable may be concrete-vs-abstract |
| Does conviction language help? | 22, 24 (helps planners) | 18, 21 (wastes tokens) | RESOLVED by audience separation: helps creative agents, hurts builders |
| Single-agent vs multi-agent | 17 (single for remediation) | 22, 24 (multi for novel) | RESOLVED as task-dependent, not universal preference |
| Is 2.5/4 success or failure? | 19 (meaningful progress) | 20 (below shipping threshold) | UNRESOLVED -- depends on definition of "success" |
| Master prompt visibility rate | 23 (64% per followed rule) | 23 also (headlines 100% compliance instead) | METRIC TENSION -- both numbers are correct, emphasis is editorial |
| Can remediation lessons transfer to greenfield? | 17 (implicitly yes) | 20, 24 (no, context-dependent) | PARTIALLY RESOLVED -- perception thresholds transfer, deallocation does not |
