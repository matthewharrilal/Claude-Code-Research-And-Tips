# Adversarial Review â€” yegge-gas-town-2026-02-25

**Reviewer:** Opus adversarial agent
**Scope:** Challenge all 9 analysis reports + MASTER-CHECKLIST.md

---

## 1. Challenges to Specific Reports

### 01-pipeline-mechanics: Routing Table Confidence is Inflated

The routing table claims 12/18 CORRECT but 4 artifacts are UNCERTAIN. "UNCERTAIN" is not the same as "not a problem" -- it means we literally don't know. The summary should read "12/18 CORRECT, 2/18 PARTIAL, 4/18 UNKNOWN (may include additional losses)." The optimistic framing ("critical path is solid") lets the unknowns pass unchallenged. Specifically: artifact-worked-examples.md is a calibration file designed to show "what good looks like." If neither the Content Analyst nor Brief Assembler read it, the quality of their output is UNCALIBRATED -- they may have produced good work by luck, not by design.

### 02-info-loss: "COMPLETE LOSS" Claim for Step 3.4c Is Speculative

Report 02 says the initial dark zone invisibility "was likely caused by scroll-triggered animations with no fallback." The word "likely" carries enormous weight here. There is NO evidence in the HTML that scroll-triggered animations existed. The dark zone invisibility was caused by low rgba opacity values on a dark background at DPR 0.667 -- report 04 (REFINE analysis) confirms this with 12 specific CSS edits. The signal declaration loss is real (Step 3.4c never reached builder), but attributing the dark zone catastrophe to it is an unfounded causal chain. The ACTUAL cause was opacity calibration, not missing signal declarations.

### 03-pa-quality: Calibration Concern Is Too Timid

Report 03 notes "3.5/4 feels slightly generous" then immediately retreats to "defensible." This is a missed opportunity. The 3.5/4 score has 6/10 auditors flagging dark zone fatigue, 5 outstanding fixes (3 MECHANICAL + 2 STRUCTURAL), and DELIGHT at 4/10 (weakest register). Compare this to the tier definitions: CEILING tier expects PA-05 >= 3.5 AND zero soul violations AND all sub-criteria PASS. The score IS defensible under the protocol, but the report should explicitly flag the score-quality tension rather than dismiss it.

### 04-refine-analysis: Attribution of Delta Sources Lacks Evidence

The 60/25/15 split (visibility unlocking / transition architecture / fresh-eyes generative) is presented as fact but is entirely the analyst's judgment. There is no methodology for attributing PA-05 improvement to specific changes. Did visibility account for 60% or 40%? The report provides no way to verify. A more honest framing: "The improvement has multiple sources that cannot be cleanly decomposed."

### 05-agent-ops: "Zero SendMessage = No Quality Cost" Conclusion Is Premature

Report 05 says this build "appears to refute" the established finding that zero inter-agent messaging degrades quality. This is a single run. The previous finding was derived from multiple builds. One counter-example does not refute a pattern -- it could be that the TC Brief pre-computation mitigated the messaging absence THIS TIME, or that the content was particularly well-suited for file-bus communication. The conclusion should be: "TC Brief may mitigate the messaging cost, but n=1 is insufficient to declare the pattern refuted."

### 06-tracker-audit: Automation Recommendations Are Too Optimistic

The tracker audit proposes a `tracker-verify.sh` script with "2-3 hours to write." This underestimates: the script must parse markdown tables, handle variable formatting, validate file paths across operating systems, and integrate with the orchestrator's workflow. More realistically: 8-12 hours for a robust implementation. The cheaper fix is structural: reduce the tracker's field count by 40% (eliminate redundant L0 that Report 06 correctly identifies as derivable from L2).

### 07-gate-analysis: The "Result Recording Error" Finding Is Explosive

Report 07 identifies that at least 3 gate results (GR-05, GR-08, GR-15) have JSON results that DON'T MATCH what the code would produce. The report calls these "result-recording errors" -- but this means the gate results JSON is partly HAND-WRITTEN by the orchestrator, not generated by the gate runner. This undermines the entire gate system's reliability. If gate results are editorially constructed, they are not programmatic verification -- they are the orchestrator's opinion labeled as automated output. This finding deserves P0-BLOCKING treatment, not the P2-MEDIUM it received.

### 08-comparison: Cross-Run Methodology Is Compromised

The comparison table shows 4 runs with different pipeline versions, different tracker formats, and different PA protocols. Run C used a different PA scale entirely. Run A had screenshot corruption. Comparing PA-05 scores across these runs is comparing unlike things. The "3.5/4 is the new high-water mark" claim is only valid within the current protocol -- it cannot be compared to Run C's 3.5/4 (different evaluation rigor) or Run B's 174/200 (different scale). The honest conclusion: we can only compare current run to Run A, and even that comparison is weakened by A's screenshot corruption.

### 09-instrumentation: Scope Creep Risk

Report 09 proposes 11 additions across 4 existing files + 2 new templates. This is substantial new infrastructure. The pipeline already has a meta-to-output ratio concern (2.6:1 documented in MEMORY.md, with Flagship inflating to 660:1). Every instrumentation addition increases meta-output ratio. The report should have a section asking: "Which of these 11 additions have the highest ROI?" and proposing a minimal viable instrumentation set (3-4 items) rather than all 11.

---

## 2. Cross-Report Contradictions

**C1: Cause of dark zone failure.** Report 02 attributes it to missing signal declarations (Step 3.4c). Report 04 attributes it to DPR opacity calibration. Report 07 notes GR-60 caught 48 WCAG violations. These are three different causal stories. The ACTUAL chain: builder set low rgba opacity -> DPR 0.667 made it invisible -> GR-60 caught contrast violations -> signal declarations are unrelated. Report 02's IL-2 (P0-BLOCKING for signal declarations) is over-prioritized based on a false causal link.

**C2: Zero SendMessage value.** Report 01 says file-bus "compensated" for missing Integrative synthesis. Report 05 says zero SendMessage had no quality cost. Report 03 says the Integrative ran in parallel (not sequentially after PAs) and its designed synthesis role was unfulfilled. These cannot all be true simultaneously. The Integrative's demotion to "10th independent auditor" IS a quality cost -- it just wasn't measured because we have no baseline for what sequential Integrative synthesis would have produced.

**C3: Component adoption severity.** Report 02 calls it a "COMPLIANCE GAP" at P2-MEDIUM. Report 08 notes Run C had 18 component types vs current run's 5-6. That is a 3x regression, not a moderate gap. If component variety genuinely correlates with quality (and the comparison data suggests it does), this should be P1-HIGH.

---

## 3. Blind Spots No Agent Covered

**B1: Source content quality.** All 9 reports treat the source content as a given. Nobody asked: is this content inherently suited to Flagship-tier output? The yegge-gas-town extraction is a technical blog post summary. Technical blog summaries may have an inherent quality ceiling -- they lack the narrative arc, emotional register variation, and visual opportunity that other content types provide. The pipeline cannot produce Flagship output from Floor-tier content regardless of pipeline quality.

**B2: The orchestrator's own performance.** Every report analyzes agents the orchestrator spawned. Nobody analyzed the orchestrator. The orchestrator made manual gate result entries, estimated timestamps, left 33% of tracker fields unfilled, and ran the Integrative in parallel instead of sequentially. The orchestrator IS an agent -- the most consequential one -- and it received zero retrospective scrutiny.

**B3: Reader experience.** No agent asked: would a human actually want to read this page? PA auditors evaluate design quality (DESIGNED, COHERENT, PROPORTIONATE, POLISHED). Nobody evaluated information architecture, reading flow, or whether the content is usefully organized. A page can score PA-05 3.5/4 while being confusing or unhelpful to its target reader.

**B4: Error recovery.** What happens when a builder crashes mid-output (2,931-line HTML partially written)? When the gate runner fails on gate 15 of 42? When 3/9 PA auditors need respawns? The pipeline has no documented error recovery protocol -- the orchestrator improvises (as evidenced by the ad-hoc respawn handling). For a 28-agent pipeline, error recovery should be designed, not improvised.

**B5: Diminishing returns on PA rounds.** The initial PA found 1 dominant issue (dark zone invisibility). The REFINE PA found 5 issues. If a second REFINE cycle ran, would the third PA find 5 more issues of decreasing severity, or would it find the same issues? Nobody modeled the expected information value of additional PA rounds vs their cost (9 agents + weaver each time).

---

## 4. Checklist Challenges

**Over-prioritized items:**
- IL-2 (signal declarations) is P0-BLOCKING based on a speculative causal claim (see C1 above). Should be P1-HIGH.
- RF-01 (DPR opacity floor) addresses a DPR-specific rendering issue. At DPR 1.0 (which is the standard target), rgba 0.9 is perfectly visible. The fix should be conditional on detected DPR, not a blanket "never use rgba below 0.95." P1-HIGH is correct; P0-BLOCKING would be wrong.

**Under-prioritized items:**
- TR-1 (derived L0) is P0-BLOCKING but solves a symptom. The root cause is that L2 takes too long to fill manually. Without also reducing L2 field count or automating L2 fills, TR-1 just makes the tracker show "INCOMPLETE" instead of "misleadingly COMPLETE" -- no improvement in actual tracking quality.

**Potentially conflicting items:**
- IL-5 (route value tables to builder) adds ~262 lines to builder input. RF-01 (DPR opacity floor) adds recipe steps. RF-02 (threshold component) adds recipe steps. RF-03 (navigation requirement) adds recipe steps. Collectively these add ~400 lines to builder input. The builder already received ~2,850 lines. At ~3,250, we approach the MANIFEST's ~3,600 target. Three more items and the builder hits input ceiling, requiring compression elsewhere.

**Missing from checklist:**
- No item addresses the orchestrator's performance (blind spot B2)
- No item addresses content quality assessment (blind spot B1)
- No item addresses error recovery protocols (blind spot B4)
- No item addresses the hand-constructed gate results (Report 07's most dangerous finding)

---

## 5. Summary Verdict

The retrospective is strong on WHAT happened and WHAT to fix. It is weak on WHY things happened (causal chains are speculative) and WHO is accountable (the orchestrator escapes scrutiny). The checklist has good coverage of pipeline artifact fixes but misses systemic issues: gate result integrity, orchestrator accountability, content quality assessment, and error recovery. Priority calibration needs adjustment: IL-2 should drop from P0 to P1, component adoption should rise from P2 to P1, and a new P0 item should address gate result integrity (the finding that JSON results don't match gate runner code output).
