{
  "items": [
    {
      "id": "inv-1",
      "type": "inversion",
      "linkedSection": "essence",
      "title": "What if LLMs HAD built-in memory?",
      "content": "<strong>You'd design:</strong> Single persistent session per project. No files needed. Context just accumulates.<br>\n          <strong>Why this fails:</strong> Context rot at 80K tokens. Quality degrades with size. No cross-instance sharing.<br>\n          <strong>Hidden constraint revealed:</strong> Memory layers exist BECAUSE persistence is external to the model."
    },
    {
      "id": "min-2",
      "type": "minimal",
      "linkedSection": "essence",
      "title": "The irreducible memory stack",
      "content": "<code style=\"background: #f4f4f5; padding: 2px 6px; border-radius: 4px; font-size: 12px;\">CLAUDE.md + progress.txt</code><br>\n          <strong>Essential:</strong> File-based state that persists across sessions.<br>\n          <strong>Everything else:</strong> Optimization for scale, search, and coordination."
    },
    {
      "id": "war-3",
      "type": "warstory",
      "linkedSection": "essence",
      "title": "Molly Cantillon: Jmail with 18M users",
      "content": "Built overnight using Ralph loops with file-based memory (progress.txt + prd.json). Each agent had its own directory. Proof that simple memory patterns scale to production systems."
    },
    {
      "id": "ana-4",
      "type": "analogy",
      "linkedSection": "core",
      "title": "Memory Pyramid = CPU Cache Hierarchy",
      "content": "L1 cache (fast, small) -&gt; Hot memory (current session)<br>\n          L2 cache (medium) -&gt; Warm memory (recent summaries)<br>\n          L3 cache (slower, larger) -&gt; Cold memory (vector archive)<br>\n          RAM/Disk -&gt; Full historical archive<br><br>\n          <em>If you understand cache hierarchies, you understand memory layers.</em>"
    },
    {
      "id": "con-5",
      "type": "constraint",
      "linkedSection": "core",
      "title": "One constraint -> Five memory decisions",
      "content": "<strong>ROOT:</strong> LLMs have no built-in persistence<br>\n          -&gt; Memory must be external (files/DBs)<br>\n          -&gt; Context injection required at session start<br>\n          -&gt; Compression needed to fit limited context<br>\n          -&gt; Progressive disclosure to minimize token cost<br>\n          -&gt; Retrieval must be relevance-weighted"
    },
    {
      "id": "grad-6",
      "type": "gradient",
      "linkedSection": "core",
      "title": "How memory injection degrades",
      "content": "<strong>0-5KB injected:</strong> Pure benefit, relevant context<br>\n          <strong>5-15KB injected:</strong> Diminishing returns, some noise<br>\n          <strong>15-30KB injected:</strong> Neutral, reading time = benefit<br>\n          <strong>30KB+ injected:</strong> Net negative, context pollution<br><br>\n          <em>Critical: More memory isn't always better.</em>"
    },
    {
      "id": "inv-7",
      "type": "invariant",
      "linkedSection": "decisions",
      "title": "INV-003: External state > internal memory",
      "content": "Ralph, Gas Town, CC Mirror, and all memory tools share this invariant. If state must persist, it must be external. This isn't a preference - it's a constraint of the architecture."
    },
    {
      "id": "vio-8",
      "type": "violation",
      "linkedSection": "decisions",
      "title": "If you rely on \"Claude remembering\"",
      "content": "<strong>IF:</strong> You rely on session memory instead of files<br>\n          <strong>THEN:</strong> Next iteration starts with zero knowledge<br>\n          <strong>THEN:</strong> Same discoveries repeated<br>\n          <strong>THEN:</strong> Same mistakes made<br>\n          <strong>FINALLY:</strong> No compound learning across sessions<br><br>\n          <em>The fix: If it's not in a file, it doesn't exist.</em>"
    },
    {
      "id": "trade-9",
      "type": "tradeoff",
      "linkedSection": "decisions",
      "title": "Compression Level Dilemma",
      "content": "- <strong>No compression:</strong> Full fidelity, but context explodes<br>\n          - <strong>Light compression:</strong> Most details, but still expensive<br>\n          - <strong>Heavy compression:</strong> Token efficient, but lossy<br><br>\n          <em>Heuristic: Start with heavy compression. Fetch full details only when summaries indicate relevance.</em>"
    },
    {
      "id": "exp-10",
      "type": "expertise",
      "linkedSection": "pillars",
      "title": "Memory tool understanding depth",
      "content": "<strong>Beginner:</strong> \"What tools exist?\" -&gt; Claude-Mem, Beads, HUD<br>\n          <strong>Intermediate:</strong> \"When to use which?\" -&gt; See decision tree<br>\n          <strong>Advanced:</strong> \"How do they combine?\" -&gt; Mem + HUD for visibility<br>\n          <strong>Staff:</strong> \"What's the minimal stack?\" -&gt; CLAUDE.md + progress.txt<br>\n          <strong>Expert:</strong> \"Cross-pattern integration?\" -&gt; Beads = Gas Town data plane"
    },
    {
      "id": "comp-11",
      "type": "composition",
      "linkedSection": "pillars",
      "title": "Claude-Mem + Claude HUD",
      "content": "<strong>Works:</strong> Excellent for solo developer visibility. See context consumption while memory handles persistence.<br>\n          <strong>Danger:</strong> Both use system resources. On low-memory machines, prefer one or the other.<br>\n          <strong>Recommendation:</strong> This is the default single-agent stack."
    },
    {
      "id": "comp-12",
      "type": "composition",
      "linkedSection": "pillars",
      "title": "Beads + Claude-Mem",
      "content": "<strong>Works:</strong> Shared work tracking (Beads) + per-agent observations (Claude-Mem). Each agent has personal memory plus shared state.<br>\n          <strong>Danger:</strong> Two persistence systems to maintain. Potential consistency issues.<br>\n          <strong>Recommendation:</strong> Use for Gas Town deployments where coordination matters."
    },
    {
      "id": "eff-13",
      "type": "effect",
      "linkedSection": "pillars",
      "title": "Token savings compound exponentially",
      "content": "Claude-Mem's 90% token savings means:<br>\n          - 10 observations costs 1K tokens (not 10K)<br>\n          - 100 observations costs 10K tokens (not 100K)<br>\n          - At scale, the difference is entire features vs. context overflow<br><br>\n          <strong>Threshold:</strong> Savings become critical after ~50 observations."
    },
    {
      "id": "hor-14",
      "type": "horizon",
      "linkedSection": "path",
      "title": "Memory ROI over iterations",
      "content": "<strong>Iteration 1:</strong> \"Memory setup feels like overhead\"<br>\n          <strong>Iteration 5:</strong> \"Wait, Claude knew about that bug from iteration 2!\"<br>\n          <strong>Iteration 15:</strong> \"Memory is why late iterations are FASTER\"<br>\n          <strong>Iteration 30:</strong> \"This is institutional knowledge\"<br><br>\n          <em>Don't judge memory tools by setup time. Judge by iteration 20.</em>"
    },
    {
      "id": "vio-15",
      "type": "violation",
      "linkedSection": "path",
      "title": "If you skip observation capture",
      "content": "<strong>IF:</strong> Disable claude-mem hooks to \"speed up\" sessions<br>\n          <strong>THEN:</strong> No observations stored<br>\n          <strong>THEN:</strong> Next session has no prior context<br>\n          <strong>THEN:</strong> Claude repeats same discoveries<br>\n          <strong>FINALLY:</strong> No compound learning - every session is groundhog day<br><br>\n          <em>The \"speedup\" is a false economy.</em>"
    },
    {
      "id": "war-16",
      "type": "warstory",
      "linkedSection": "path",
      "title": "@thedotmack: 95% token savings",
      "content": "Creator of Claude-Mem reports 95% fewer tokens per session after implementing progressive disclosure. ~20x more tool calls before hitting limits. Real production numbers, not theory."
    },
    {
      "id": "inf-17",
      "type": "inflection",
      "linkedSection": "gotchas",
      "title": "When context injection flips from help to hurt",
      "content": "<strong>0-5KB:</strong> Pure benefit (relevant context)<br>\n          <strong>5-15KB:</strong> Diminishing returns (some noise)<br>\n          <strong>15-30KB:</strong> Neutral (reading cost = benefit)<br>\n          <strong>30KB+:</strong> Net negative (context pollution)<br><br>\n          <strong>THE INFLECTION:</strong> ~15KB injected context<br>\n          <em>Detection: Late iterations worse than early? Check injection size.</em>"
    },
    {
      "id": "inv-18",
      "type": "inversion",
      "linkedSection": "gotchas",
      "title": "What if SQLite didn't lock?",
      "content": "<strong>You'd design:</strong> Multiple agents writing simultaneously, no daemon needed.<br>\n          <strong>Why this fails:</strong> SQLite is single-writer by design. Concurrent writes corrupt data.<br>\n          <strong>Hidden constraint revealed:</strong> Beads uses daemon pattern BECAUSE of SQLite's write locking."
    },
    {
      "id": "fron-19",
      "type": "frontier",
      "linkedSection": "gotchas",
      "title": "UNSOLVED: Optimal observation decay",
      "content": "When should old observations be archived? After N days? After M observations? Based on access patterns? Optimal varies by project. No automation exists yet. Current best practice: manual archival when retrieval quality degrades."
    },
    {
      "id": "fron-20",
      "type": "frontier",
      "linkedSection": "gotchas",
      "title": "UNSOLVED: Cross-project memory",
      "content": "Should learnings from project A transfer to project B? Some patterns are universal, some are project-specific. No good heuristics exist for filtering. Current practice: separate memory per project."
    },
    {
      "id": "trade-21",
      "type": "tradeoff",
      "linkedSection": "hard",
      "title": "Compression vs. Fidelity",
      "content": "<strong>THE DILEMMA:</strong><br>\n          - Full fidelity: Everything preserved, but context explodes<br>\n          - Semantic compression: Token efficient, but nuance lost<br>\n          - Hybrid: Complex to implement and debug<br><br>\n          <strong>WHY NO PERFECT ANSWER:</strong><br>\n          Information theory: compression is inherently lossy. You're choosing WHICH information to lose.<br><br>\n          <em>Heuristic: Preserve decisions and gotchas. Compress implementation details.</em>"
    },
    {
      "id": "exp-22",
      "type": "expertise",
      "linkedSection": "hard",
      "title": "Multi-agent memory understanding",
      "content": "<strong>Beginner:</strong> \"Can agents share memory?\" -&gt; Not directly<br>\n          <strong>Intermediate:</strong> \"How to coordinate?\" -&gt; Shared files or Beads<br>\n          <strong>Advanced:</strong> \"Race conditions?\" -&gt; Use explicit handoff protocols<br>\n          <strong>Staff:</strong> \"Message passing vs shared state?\" -&gt; MCP Agent Mail exists<br>\n          <strong>Expert:</strong> \"Distributed consensus?\" -&gt; Too complex, use file locking"
    },
    {
      "id": "ana-23",
      "type": "analogy",
      "linkedSection": "hard",
      "title": "Memory staleness = Cache invalidation",
      "content": "Old observations polluting search -&gt; Stale cache entries<br>\n          TTL-based expiry -&gt; Time-decay scoring<br>\n          Cache busting -&gt; Manual archival<br>\n          LRU eviction -&gt; Access-pattern based cleanup<br><br>\n          <em>\"There are only two hard things: cache invalidation and naming things.\" Memory staleness is cache invalidation.</em>"
    },
    {
      "id": "inv-24",
      "type": "inversion",
      "linkedSection": "when",
      "title": "What if you always added memory?",
      "content": "- One-off question with 30 min Claude-Mem setup -&gt; Massive overhead<br>\n          - Simple hotfix with full Beads stack -&gt; Coordination complexity &gt; fix complexity<br>\n          - Debugging session with memory injection -&gt; Old patterns pollute fresh perspective<br><br>\n          <strong>Hidden constraint:</strong> Memory has setup cost. Only amortizes over many sessions."
    },
    {
      "id": "alt-25",
      "type": "alternative",
      "linkedSection": "when",
      "title": "If memory integration isn't right",
      "content": "<strong>One-off tasks?</strong><br>\n          -&gt; Just use CLAUDE.md (zero setup)<br><br>\n          <strong>Single session sufficient?</strong><br>\n          -&gt; progress.txt only (manual append)<br><br>\n          <strong>Team needs shared context?</strong><br>\n          -&gt; Git-based AGENTS.md (no tools needed)<br><br>\n          <strong>Debugging, need fresh perspective?</strong><br>\n          -&gt; Disable memory injection temporarily"
    },
    {
      "id": "inv-26",
      "type": "invariant",
      "linkedSection": "when",
      "title": "Memory tools implement the same invariants",
      "content": "Claude-Mem, Beads, progress.txt all implement:<br>\n          - INV-003: External state &gt; internal memory<br>\n          - INV-005: Verification enables automation<br>\n          - INV-007: Context is finite, manage it<br><br>\n          <em>Different implementations, same underlying principles.</em>"
    },
    {
      "id": "war-27",
      "type": "warstory",
      "linkedSection": "when",
      "title": "Steve Yegge: Gas Town + Beads",
      "content": "~225K lines of Go implementing the full memory stack for multi-agent orchestration. Beads provides git-backed universal data plane. \"Stage 7+ infrastructure for factory-scale operations.\" Proof that memory can scale, but requires significant investment."
    },
    {
      "id": "hor-28",
      "type": "horizon",
      "linkedSection": "when",
      "title": "When memory setup is worth it",
      "content": "<strong>1 session:</strong> Never worth it (setup &gt; benefit)<br>\n          <strong>3-5 sessions:</strong> Marginal (break-even zone)<br>\n          <strong>10+ sessions:</strong> Clearly worth it (compound returns)<br>\n          <strong>50+ sessions:</strong> Essential (institutional memory)<br><br>\n          <em>Rule of thumb: If you expect 10+ sessions, invest in memory.</em>"
    }
  ]
}