{
  "items": [
    {
      "id": "inv-1",
      "type": "inversion",
      "linkedSection": "essence",
      "title": "What if you DIDN'T archive?",
      "content": "<strong>You'd design:</strong> Start fresh every time. No files. Pure context.<br>\n          <strong>Why this fails:</strong> Run 10 is identical to Run 1. No velocity increase.<br>\n          <strong>Hidden constraint revealed:</strong> Agents are stateless. Memory must be external."
    },
    {
      "id": "min-2",
      "type": "minimal",
      "linkedSection": "essence",
      "title": "The irreducible core of learning loops",
      "content": "<strong>Minimum viable:</strong> Archive prd.json + progress.txt after each run.<br>\n          <strong>Essential:</strong> Instruct agent to READ archives before starting.<br>\n          <strong>Everything else:</strong> Refinement for reliability (learnings.md, Claude-Mem, etc)."
    },
    {
      "id": "war-3",
      "type": "warstory",
      "linkedSection": "essence",
      "title": "Ryan Carson: \"Every run gets better\"",
      "content": "\"I archive each PRD and user story JSON. So every time I run Ralph, it gets better and better.\" By story 10, Ralph knew all the project patterns without being told."
    },
    {
      "id": "con-4",
      "type": "constraint",
      "linkedSection": "core",
      "title": "Fresh context as root constraint",
      "content": "<strong>ROOT:</strong> Each iteration spawns fresh Claude (no memory)<br>\n          → State must persist externally<br>\n          → Files become the memory system<br>\n          → Archives enable cross-run learning<br>\n          → Compounding requires explicit reference<br>\n          → Verification ensures learning quality"
    },
    {
      "id": "ana-5",
      "type": "analogy",
      "linkedSection": "core",
      "title": "Learning Loops = Compound Interest",
      "content": "Principal → Initial capability<br>\n          Interest → Each run's learnings<br>\n          Compounding → Archives feeding future runs<br>\n          Time → Number of runs<br><br>\n          <em>If you understand compound interest, you understand learning loops.</em>"
    },
    {
      "id": "inv-6",
      "type": "invariant",
      "linkedSection": "core",
      "title": "INV-003: External state > internal memory",
      "content": "Learning Loops, Ralph, Gas Town all share this: files persist, context doesn't. Understanding this invariant unlocks all three patterns."
    },
    {
      "id": "eff-7",
      "type": "effect",
      "linkedSection": "core",
      "title": "Compounding accelerates over time",
      "content": "<strong>Run 1-5:</strong> Linear improvement (each run adds ~1 pattern)<br>\n          <strong>Run 6-15:</strong> Acceleration (patterns combine into shortcuts)<br>\n          <strong>Run 16+:</strong> Exponential (agent \"knows\" the codebase)<br><br>\n          <em>The benefit curve is not linear. Patience pays.</em>"
    },
    {
      "id": "vio-8",
      "type": "violation",
      "linkedSection": "decisions",
      "title": "If you skip archiving",
      "content": "<strong>IF:</strong> You skip archiving \"just this once\"<br>\n          <strong>THEN:</strong> That run's learnings are lost<br>\n          <strong>THEN:</strong> Next run repeats those mistakes<br>\n          <strong>THEN:</strong> No velocity increase over time<br>\n          <strong>FINALLY:</strong> Run 20 is no better than Run 1<br><br>\n          <em>The fix: Archive is mandatory. No exceptions.</em>"
    },
    {
      "id": "trade-9",
      "type": "tradeoff",
      "linkedSection": "decisions",
      "title": "The Archive Depth Dilemma",
      "content": "<strong>Shallow:</strong> Quick to archive, but misses nuance<br>\n          <strong>Medium:</strong> Balanced, but requires judgment<br>\n          <strong>Deep:</strong> Complete, but overhead dominates<br><br>\n          <em>Heuristic: Archive prd.json + progress.txt always. learnings.md for significant runs only.</em>"
    },
    {
      "id": "exp-10",
      "type": "expertise",
      "linkedSection": "decisions",
      "title": "How deep is your learning loop understanding?",
      "content": "<strong>Beginner:</strong> \"Why archive?\" → Agents are stateless<br>\n          <strong>Intermediate:</strong> \"What to archive?\" → prd.json + progress.txt<br>\n          <strong>Advanced:</strong> \"How to reference?\" → Explicit prompt instructions<br>\n          <strong>Staff:</strong> \"When to prune?\" → 20KB threshold, 30-day retention<br>\n          <strong>Expert:</strong> \"Cross-project?\" → Claude-Mem semantic memory"
    },
    {
      "id": "inv-11",
      "type": "inversion",
      "linkedSection": "decisions",
      "title": "What if archives were auto-referenced?",
      "content": "<strong>You'd design:</strong> Agent automatically reads all archives.<br>\n          <strong>Why this fails:</strong> 50 archives = context overflow. Noise &gt; signal.<br>\n          <strong>Hidden constraint revealed:</strong> Explicit reference forces curation."
    },
    {
      "id": "grad-12",
      "type": "gradient",
      "linkedSection": "loops",
      "title": "How progress.txt degrades",
      "content": "<strong>0-10KB:</strong> Pure benefit (learnings compound)<br>\n          <strong>10-20KB:</strong> Diminishing returns (more to read)<br>\n          <strong>20-30KB:</strong> Neutral (reading cost = thinking benefit)<br>\n          <strong>30KB+:</strong> Net negative (context wasted on history)<br><br>\n          <em>Critical: The cliff is at ~20KB. Truncate before you hit it.</em>"
    },
    {
      "id": "inf-13",
      "type": "inflection",
      "linkedSection": "loops",
      "title": "When progress.txt flips from help to hurt",
      "content": "<strong>THE INFLECTION:</strong> ~20KB or ~50 iterations<br><br>\n          <strong>BEFORE:</strong> \"Always append. Never delete.\"<br>\n          <strong>AFTER:</strong> \"Promote to CLAUDE.md. Start fresh progress.txt.\"<br><br>\n          <em>Detection: Late iterations worse than early ones? Check file size.</em>"
    },
    {
      "id": "hor-14",
      "type": "horizon",
      "linkedSection": "loops",
      "title": "How judgment evolves with runs",
      "content": "<strong>Run 2:</strong> \"Archiving feels like overhead\"<br>\n          <strong>Run 5:</strong> \"Wait, the agent is repeating mistakes\"<br>\n          <strong>Run 10:</strong> \"Archives are why later runs are faster\"<br>\n          <strong>Run 20:</strong> \"This is institutional memory\"<br><br>\n          <em>Don't judge learning loops by run 2.</em>"
    },
    {
      "id": "comp-15",
      "type": "composition",
      "linkedSection": "loops",
      "title": "Learning Loops + Claude-Mem",
      "content": "<strong>Works?</strong> Yes, but with overlap.<br>\n          <strong>The benefit:</strong> Claude-Mem handles cross-project; archives handle cross-run.<br>\n          <strong>The danger:</strong> Redundant storage if both capture same patterns.<br>\n          <strong>Recommendation:</strong> Use Claude-Mem for cross-project, archives for project-specific."
    },
    {
      "id": "ana-16",
      "type": "analogy",
      "linkedSection": "loops",
      "title": "The 4 Loops = Memory Hierarchy",
      "content": "<strong>Within-session</strong> = CPU cache (fast, ephemeral)<br>\n          <strong>Cross-session</strong> = RAM (persistent while running)<br>\n          <strong>Cross-run</strong> = Disk (survives restart)<br>\n          <strong>Cross-project</strong> = Network storage (shared across machines)<br><br>\n          <em>If you understand computer memory hierarchy, you understand learning loops.</em>"
    },
    {
      "id": "war-17",
      "type": "warstory",
      "linkedSection": "implementation",
      "title": "Boris Cherny: Verification = 2-3x quality",
      "content": "Claude Code creator's insight: \"Verification feedback loop yields 2-3x quality improvement.\" Without verification, learnings are unreliable. With it, each archive is trustworthy."
    },
    {
      "id": "vio-18",
      "type": "violation",
      "linkedSection": "implementation",
      "title": "If you edit progress.txt (instead of append)",
      "content": "<strong>IF:</strong> You edit previous entries to \"clean up\"<br>\n          <strong>THEN:</strong> History is lost<br>\n          <strong>THEN:</strong> Can't trace what happened when<br>\n          <strong>THEN:</strong> Debugging becomes impossible<br>\n          <strong>FINALLY:</strong> You lose the \"why\" behind patterns<br><br>\n          <em>The fix: APPEND ONLY. The log is sacred.</em>"
    },
    {
      "id": "con-19",
      "type": "constraint",
      "linkedSection": "implementation",
      "title": "Why Codebase Patterns lives at TOP",
      "content": "<strong>ROOT:</strong> Agents read files top-to-bottom<br>\n          → Most important content first<br>\n          → Patterns at top = read first<br>\n          → Session details at bottom = read if relevant<br>\n          → Structure encodes priority<br>\n          → No explicit \"read this first\" instruction needed"
    },
    {
      "id": "fron-20",
      "type": "frontier",
      "linkedSection": "implementation",
      "title": "UNSOLVED: Optimal archive retention",
      "content": "<strong>The question:</strong> How many archives to keep? 10? 30? All?<br>\n          <strong>Why it's hard:</strong> Old patterns may resurface. But old archives add noise.<br>\n          <strong>Current best practice:</strong> Keep 30 days, synthesize monthly, delete old runs."
    },
    {
      "id": "inv-21",
      "type": "inversion",
      "linkedSection": "gotchas",
      "title": "Why CAPS for \"APPEND\"?",
      "content": "<strong>What if \"update\" meant \"append\"?</strong><br>\n          Then you wouldn't need CAPS. But models interpret \"update\" as \"replace\" by default.<br><br>\n          <strong>Why CAPS matters:</strong> Formatting emphasis changes model behavior. \"append\" gets ignored. \"APPEND\" gets followed."
    },
    {
      "id": "trade-22",
      "type": "tradeoff",
      "linkedSection": "gotchas",
      "title": "CLAUDE.md Size Dilemma",
      "content": "<strong>Too small:</strong> Patterns missing. Agent re-learns.<br>\n          <strong>Just right:</strong> 300-500 tokens. Core patterns only.<br>\n          <strong>Too large:</strong> Context wasted. Irrelevant patterns dilute signal.<br><br>\n          <em>Heuristic: If CLAUDE.md is longer than this page, it's too long.</em>"
    },
    {
      "id": "grad-23",
      "type": "gradient",
      "linkedSection": "gotchas",
      "title": "How false completions corrupt learning",
      "content": "<strong>1 false completion:</strong> Minor pollution, barely noticeable<br>\n          <strong>5 false completions:</strong> Patterns include bad practices<br>\n          <strong>15 false completions:</strong> Archives are more wrong than right<br>\n          <strong>50+ false completions:</strong> Compounding works AGAINST you<br><br>\n          <em>Critical: Verification is not optional. It's the foundation.</em>"
    },
    {
      "id": "eff-24",
      "type": "effect",
      "linkedSection": "gotchas",
      "title": "Session memory substitution at scale",
      "content": "<strong>At run 1:</strong> Session memory seems fine<br>\n          <strong>At run 5:</strong> \"Wait, it forgot what we did yesterday\"<br>\n          <strong>At run 10:</strong> Repeated context setup costs hours<br><br>\n          <em>Threshold: If you're explaining the same thing twice, externalize it.</em>"
    },
    {
      "id": "trade-25",
      "type": "tradeoff",
      "linkedSection": "hard",
      "title": "Context Rot vs. Memory Loss",
      "content": "<strong>THE DILEMMA:</strong><br>\n          - Extended session: Rich context, but quality degrades<br>\n          - Fresh context: Peak quality, but must reconstruct state<br>\n          - Compounding: Best of both, but requires discipline<br><br>\n          <strong>WHY NO PERFECT ANSWER:</strong> progress.txt is lossy. You choose what to write."
    },
    {
      "id": "hor-26",
      "type": "horizon",
      "linkedSection": "hard",
      "title": "The discipline investment curve",
      "content": "<strong>Day 1:</strong> \"Archiving is overhead\"<br>\n          <strong>Week 1:</strong> \"The pattern is paying off\"<br>\n          <strong>Month 1:</strong> \"I can't imagine working without it\"<br>\n          <strong>Month 3:</strong> \"This IS how I work\"<br><br>\n          <em>The habit takes ~2 weeks to feel natural.</em>"
    },
    {
      "id": "exp-27",
      "type": "expertise",
      "linkedSection": "hard",
      "title": "Archive retrieval mastery",
      "content": "<strong>Beginner:</strong> \"Just read all archives\" → Context overflow<br>\n          <strong>Intermediate:</strong> \"Read most recent 3\" → Better<br>\n          <strong>Advanced:</strong> \"Read learnings.md only\" → Synthesized signal<br>\n          <strong>Staff:</strong> \"Semantic search archives\" → Claude-Mem<br>\n          <strong>Expert:</strong> \"Tiered retrieval\" → Recent full, old summarized"
    },
    {
      "id": "war-28",
      "type": "warstory",
      "linkedSection": "hard",
      "title": "Matt Pocock: Context rot awareness",
      "content": "\"There's a point where the AI starts producing worse output. It's not obvious when it happens.\" Fresh context + compounding archives = the solution to context rot without losing history."
    },
    {
      "id": "inv-29",
      "type": "inversion",
      "linkedSection": "when",
      "title": "What if you used this for one-off tasks?",
      "content": "<strong>You'd design:</strong> Archive even single-use work.<br>\n          <strong>Why this fails:</strong> Archive overhead exceeds benefit. No second run to compound.<br>\n          <strong>Hidden constraint revealed:</strong> Compounding requires REPETITION to pay off."
    },
    {
      "id": "ana-30",
      "type": "analogy",
      "linkedSection": "when",
      "title": "Learning Loops = Unit Tests",
      "content": "<strong>Write once, benefit forever</strong> = Archive once, reference forever<br>\n          <strong>Catches regressions</strong> = Prevents repeated mistakes<br>\n          <strong>Overhead upfront</strong> = Archiving takes time<br>\n          <strong>Pays off at scale</strong> = Compounding kicks in at run 5+<br><br>\n          <em>If you understand why unit tests matter, you understand learning loops.</em>"
    },
    {
      "id": "alt-31",
      "type": "alternative",
      "linkedSection": "when",
      "title": "If learning loops aren't right",
      "content": "<strong>One-off tasks?</strong> → Skip archiving, use single session<br>\n          <strong>Highly varied work?</strong> → Claude-Mem for semantic cross-domain<br>\n          <strong>No verification?</strong> → Add tests first, then compounding<br>\n          <strong>Already using Claude-Mem?</strong> → May be redundant"
    },
    {
      "id": "inv-32",
      "type": "invariant",
      "linkedSection": "when",
      "title": "INV-007: Verification before persistence",
      "content": "Learning Loops, Ralph verification, CI/CD all share this: only persist what's verified. False positives compound negatively. This invariant is why verification gates exist at every level."
    },
    {
      "id": "comp-33",
      "type": "composition",
      "linkedSection": "when",
      "title": "Learning Loops + Team Sharing",
      "content": "<strong>Works?</strong> Yes, with multiplied benefit.<br>\n          <strong>The benefit:</strong> One person's learnings benefit entire team.<br>\n          <strong>The danger:</strong> Conflicting patterns from different team members.<br>\n          <strong>Recommendation:</strong> Weekly archive review meeting. Curate shared CLAUDE.md."
    }
  ]
}